[["index.html", "EconometrÃ­a II InformaciÃ³n general DescripciÃ³n del curso Material bibliogrÃ¡fico EvaluaciÃ³n Programa semanal Recursos adicionales InclusiÃ³n Integridad acadÃ©mica", " EconometrÃ­a II Ana MarÃ­a DÃ­az 2025-07-16 InformaciÃ³n general Campo Detalle Curso EconometrÃ­a Avanzada (CodÂ 1420) Docente AnaÂ MarÃ­aÂ DÃ­az Oficina / AtenciÃ³n SÃ©ptimo piso EdificioÂ 20Â | LunesÂ 9â€“11Â a.m. (o por Teams) Sitio web http://adiazescobar.com Correo a.diaze@javeriana.edu.co Prerequisito EconometrÃ­aÂ I Horario de clase MartesÂ yÂ JuevesÂ 11:00â€“13:00Â | SalonesÂ 67â€‘208 yÂ 67â€‘314 Monitor MiguelÂ ÃngelÂ CortÃ©sÂ â€” horarios y oficina por definir DescripciÃ³n del curso El objetivo principal es proporcionar herramientas para el anÃ¡lisis economÃ©trico de datos de corte transversal, series de tiempo y panel. Se revisa el modelo clÃ¡sico de regresiÃ³n lineal, las consecuencias de violar sus supuestos, modelos para variables dependientes discretas o limitadas y tÃ©cnicas bÃ¡sicas de series de tiempo y panel. Al finalizar, el estudiante podrÃ¡ ejecutar regresiones mÃºltiples, diagnosticar problemas comunes y aplicar soluciones apropiadas. Material bibliogrÃ¡fico Libro obligatorio Verbeek, MarnoÂ (2004). A Guide to Modern Econometrics. Wiley. Libros recomendados Greene, WilliamÂ (2003). Econometric Analysis. Prentice Hall. Wooldridge, JeffreyÂ (2003). Introductory Econometrics: A Modern Approach. Thomson. Wooldridge, JeffreyÂ (2002). Econometric Analysis of Cross Section and Panel Data. MITÂ Press. Montenegro, ÃlvaroÂ (2009). Series de Tiempo. Javegraf, PUJ. Stock, J.Â &amp;Â Watson, M.Â (2006). Introduction to Econometrics. Addisonâ€‘Wesley. Hayashi, FumioÂ (2000). Econometrics. Princeton UP. Angrist, J.D.Â &amp;Â Pischke, J.S.Â (2009). Mostly Harmless Econometrics. Princeton UP. Cameron, A.C.Â &amp;Â Trivedi, P.K.Â (2009). Microeconometrics Using Stata. StataÂ Press. StataÂ 11 Time Series Reference Manual. StataÂ Press. RosalesÂ R.Â etÂ al.Â (2010). Fundamentos de EconometrÃ­a Intermedia. CEDE. EvaluaciÃ³n Porcentaje Actividad 25Â % ParcialÂ 1 teÃ³rico 25Â % ParcialÂ 2 teÃ³rico 3Â % Talleres en clase 7Â % MonitorÃ­as 15Â % Trabajo final 25Â % Examen final +0.5Â (para el mejor) Videoâ€‘bono examen final Los exÃ¡menes son con libro cerrado y sin dispositivos electrÃ³nicos. El incumplimiento se sanciona segÃºn el reglamento de integridad acadÃ©mica. Programa semanal Semana Tema principal Lecturas clave 1 Supuestos del MCRL VerbeekÂ cap.Â 1â€‘2 (oblig.) Â | HayashiÂ cap.Â 1; WooldridgeÂ cap.Â 1â€‘2 (opc.) 2 RegresiÃ³n simple vs mÃºltiple; Teorema FWL VerbeekÂ cap.Â 1â€‘2 Â | The Stata JournalÂ (2013)Â 13(1):Â 92â€‘106 3 Propiedades de MCO en muestras finitas; Teorema Gaussâ€‘Markov VerbeekÂ cap.Â 2 4 Inferencia y predicciÃ³n; Propiedades asintÃ³ticas de MCO VerbeekÂ cap.Â 3 5 Primer parcial â€” 6 No linealidad; Multicolinealidad VerbeekÂ cap.Â 3â€‘4 7 Heterocedasticidad VerbeekÂ cap.Â 4 8 Endogeneidad: simultaneidad, omitidas, mediciÃ³n VerbeekÂ cap.Â 5 9 Variables instrumentales, MCO2E, GMM VerbeekÂ cap.Â 5 10 Modelos LPM, logit y probit VerbeekÂ cap.Â 7 11 MÃ¡ximo verosimilitud; DID, RD, duraciÃ³n, cuantÃ­lica (opc.) â€” 12 Semana Santa / Receso â€” 13 Segundo parcial â€” 14 Series de tiempo: conceptos bÃ¡sicos VerbeekÂ cap.Â 8 15 AR, MA y VAR estacionarios VerbeekÂ cap.Â 9 16 Datos de panel: pooled, between, FE, RE VerbeekÂ cap.Â 10 17 Examen final â€” Recursos adicionales BenÂ Lambert â€“ Econometrics on YouTube Mastering Econometrics (MRU) AEA Journal of Economic Perspectives â€“ Classroom Google Dataset Search Stata Cheat Sheets Seeing Theory â€“ Visual Probability InclusiÃ³n Este curso da la bienvenida a personas de todas las edades, orÃ­genes, creencias, etnias, gÃ©neros, identidades, orientaciones sexuales y capacidades. Se espera un ambiente respetuoso e inclusivo. Integridad acadÃ©mica La Universidad Javeriana fomenta la honestidad y establece sanciones por fraude o plagio segÃºn el reglamento de estudiantes. Cualquier uso no autorizado de materiales durante evaluaciones se considera falta grave. "],["repaso.html", "1 Repaso Construimos una poblaciÃ³n de juguete Â¿Y si mi muestra es mala? Â¿Y si mantengo fija la muestra? ğŸ“˜ Preguntas de repaso", " 1 Repaso Â¿QuÃ© estudia la econometrÃ­a? La econometrÃ­a es la herramienta que usamos para entender el mundo usando datos. Nos ayuda a responder preguntas como: Â¿cuÃ¡nto gana una persona segÃºn su nivel educativo? Â¿CÃ³mo influye la experiencia laboral en el salario? Â¿CuÃ¡l es el impacto de una polÃ­tica pÃºblica sobre el empleo? Pero aquÃ­ hay un reto importante: casi nunca podemos observar a toda la poblaciÃ³n. En vez de eso, trabajamos con una muestra. Usamos esta muestra para hacer inferencias sobre cÃ³mo funciona el mundo real, ese que no podemos ver completamente. En este capÃ­tulo vamos a entender, paso a paso, por quÃ© eso genera incertidumbre â€”y por quÃ© esa incertidumbre es una parte inevitable (Â¡y valiosa!) del anÃ¡lisis economÃ©trico. El proceso generador de datos Supongamos que el salario de un individuo, \\(y_i\\), depende de forma lineal de su nivel educativo, \\(x_i\\): \\[ y_i \\;=\\; \\beta_0 \\;+\\; \\beta_1\\,x_i \\;+\\; u_i, \\] donde \\(u_i\\) recoge todo lo que no observamos (habilidad, contactos, suerteâ€¦). A esta ecuaciÃ³n la llamaremos Proceso Generador de Datos (PGD) o modelo poblacional. El problema es que no podemos observar \\(u_i\\) porque es un tÃ©rmino de error. Ni tenemos acceso a todos los individuos de la poblaciÃ³n. Â¿QuÃ© hacemos entonces? En la prÃ¡ctica, tomamos una muestra aleatoria de individuos y observamos sus salarios y aÃ±os de educaciÃ³n \\((y_i,\\,x_i)\\), el termino de error \\(u_i\\) permanece oculto. Con una muestra aleatoria de tamaÃ±o \\(n\\), estimamos los parÃ¡metros \\(\\beta_0\\) y \\(\\beta_1\\) de la siguiente manera: \\[ y_i \\;=\\; \\hat{\\beta}_0 + \\hat{\\beta}_1\\,x_i + e_i, \\qquad \\hat{y}_i \\;=\\; \\hat{\\beta}_0 + \\hat{\\beta}_1\\,x_i, \\] Donde \\(\\hat{\\beta}_0\\) y \\(\\hat{\\beta}_1\\) son los estimadores de los parÃ¡metros poblacionales \\(\\beta_0\\) y \\(\\beta_1\\), y \\(e_i\\) es el tÃ©rmino de error muestral. A la recta que obtenemos se le llama modelo muestral. La direfencia entre este modelo y el PGD es precisamente lo que genera incertidumbre en nuestras estimaciones. Es decir que tenemos dos fuentes de incertidumbre, la muestra que compone nuestros datos y el tÃ©rmino de error \\(u_i\\) que no podemos observar. Para entender todo esto mejor, vamos primero a enfocarnos en la muestra que tenemos y cÃ³mo podemos usarla para estimar el PGD. Luego veremos cÃ³mo la incertidumbre afecta nuestras estimaciones y por quÃ© es importante. Construimos una poblaciÃ³n de juguete Para ilustrar el proceso generador de datos y la incertidumbre, vamos a crear una poblaciÃ³n de juguete. Esta poblaciÃ³n serÃ¡ un conjunto de 100 individuos con caracterÃ­sticas especÃ­ficas. Luego tomaremos muestras aleatorias de esta poblaciÃ³n y realizaremos regresiones para ver cÃ³mo se comportan nuestras estimaciones en comparaciÃ³n con el PGD real. Vamos a crear un mundo ficticio con 100 individuos. A cada uno le asignamos: \\(x\\) (aÃ±os de educaciÃ³n) sigue una normal con mediaÂ 5 y desviaciÃ³nÂ 1.5. \\(y\\) depende linealmente de \\(x\\) con pendienteÂ 0.5 y un tÃ©rmino aleatorio \\(u\\sim N(0,1)\\). La relaciÃ³n verdadera en la poblaciÃ³n El modelo poblacional que usamos, es decir el PGD, es: \\(y = 3 + 0.5x\\): AsÃ­ que en promedio los salarios de los individuos aumentan en 0.5 por cada aÃ±o adicional de educaciÃ³n. Esta es la verdad de nuestra poblaciÃ³n simulada. Obtenemos que los coeficientes son muy similares a los que usamos para generar la poblaciÃ³n: \\[ y_i = 2.53 + 0.57 x_i + u_i \\] Esto significa que el modelo poblacional es: \\[ y_i = \\beta_0 + \\beta_1 x_i + u_i \\] Sin embargo, esa linea estÃ¡ fuera de nuestro alcance porque requerirÃ­a encuestar a todos los egresados. Podemos estimar la relaciÃ³n entre \\(y\\) y \\(x\\) en una muestra aleatoria de individuos. Comencemos tomando 30 graduados al azar de nuestro grupo de 100 individuos: Estimemos la relaciÃ³n que existe entre \\(y\\) y \\(x\\) en esta muestra de 30 individuos. En la siguiente grÃ¡fica, la lÃ­nea roja es el modelo poblacional y la lÃ­nea negra discontinua es el modelo muestral. Ahora encontramos unos coeficientes estimados que son diferentes a los del modelo poblacional: PGD Modelo Poblacional \\(y_i = 2.53 + 0.57 x_i + u_i\\) Modelo muestral \\(\\hat{y}_i = 2.36 + 0.61 x_i\\) Tomemos otros 30 individuos al azar de la poblaciÃ³n y veamos cÃ³mo se comporta la regresiÃ³n. Ahora encontramos los siguientes coeficientes estimados: PGD Modelo Poblacional \\(y_i = 2.53 + 0.57 x_i + u_i\\) Modelo muestral \\(\\hat{y}_i = 2.79 + 0.56 x_i\\) Podemos ver que los coeficientes estimados son diferentes a los del modelo poblacional y tambiÃ©n diferentes entre sÃ­. Esto es normal, porque cada muestra aleatoria puede dar lugar a diferentes estimaciones. Tomemos una tercera muestra aleatoria de 30 individuos y veamos cÃ³mo se comporta la regresiÃ³n. Ahora encontramos los siguientes coeficientes estimados: PGD Modelo Poblacional \\(y_i = 2.53 + 0.57 x_i + u_i\\) Modelo muestral \\(\\hat{y}_i = 3.21 + 0.45 x_i\\) Siguen siendo diferentes a los del modelo poblacional y tambiÃ©n diferentes entre sÃ­. A veces se parece mucho, a veces no tanto. La razÃ³n es simple; cada muestra incluye un conjunto diferente de personas y eso cambia los resultados. Ahora repitamos esto 10,000 veces. Este ejercicio se conoce como Ejercicio de Monte Carlo. Vamos a tomar 10,000 muestras aleatorias de 30 individuos de nuestra poblaciÃ³n y estimar los coeficientes de regresiÃ³n para cada muestra. Luego, graficaremos todas las lÃ­neas de regresiÃ³n obtenidas para ver cÃ³mo se distribuyen en relaciÃ³n con la lÃ­nea poblacional. Â¿Lo interesante? Aunque cada recta individual es distinta, en promedio todas convergen hacia la recta verdadera (la primera que estimamos). En resumen, en promedio las lÃ­neas de regresiÃ³n se ajustan muy bien a la lÃ­nea de la poblaciÃ³n. Sin embargo, las lÃ­neas individuales (muestras) pueden desviarse significativamente. Las diferencias entre las muestras individuales y la poblaciÃ³n generan incertidumbre para el econometrista. ğŸ‘‰ Este resultado es tranquilizador: aunque nuestras estimaciones varÃ­an de muestra a muestra, en promedio nos acercamos a la verdad. Esto es lo que se conoce como insesgamiento del estimador MCO. Eso implica que cuando estimamos los coeficientes de regresiÃ³n, no podemos estar seguros de que nuestros estimadores sean exactamente iguales a los parÃ¡metros poblacionales. En cambio, obtenemos estimaciones que son variables aleatorias. En otras palabras, \\(\\hat{\\beta}\\) en sÃ­ mismo es una variable aleatoria, dependiente de la muestra aleatoria. Cuando tomamos una muestra y realizamos una regresiÃ³n, no sabemos si es una muestra â€˜buenaâ€™ ( \\(\\hat{\\beta}\\) estÃ¡ cerca de \\(\\beta\\)) o una muestra â€˜malaâ€™ (nuestra muestra difiere significativamente de la poblaciÃ³n). Mantener un registro de esta incertidumbre es clave para el anÃ¡lisis economÃ©trico. Nos permite entender la precisiÃ³n de nuestras estimaciones y cÃ³mo podemos mejorar nuestro modelo. Â¿Y si mi muestra es mala? â“ Pregunta del lector: Â¿QuÃ© pasa si me toca una muestra mala? Â¿CÃ³mo lo sÃ©? Â¿Se puede hacer algo para reducir esa incertidumbre? Una muestra mala es una muestra que, por puro azar, no representa bien a la poblaciÃ³n. Esto puede pasar incluso si tomamos la muestra correctamente. En esos casos, los estimadores como \\(\\hat{\\beta}_1\\) pueden estar lejos de su valor verdadero \\(\\beta_1\\), y nuestras conclusiones podrÃ­an ser engaÃ±osas. Â¿CÃ³mo saber si me tocÃ³ una muestra mala? No podemos saberlo con certeza, porque no conocemos la verdad poblacional. Pero hay seÃ±ales que nos pueden alertar: Errores estÃ¡ndar grandes: indican mucha variabilidad en la estimaciÃ³n. Intervalos de confianza anchos: reflejan gran incertidumbre. Signos o tamaÃ±os inesperados en los coeficientes: pueden deberse a una muestra no representativa. Pruebas de diagnÃ³stico del modelo: pueden revelar si los supuestos no se cumplen (residuos no normales, heterocedasticidad, etc.). Â¿Se puede reducir la incertidumbre muestral? Â¡SÃ­! Estas son algunas estrategias comunes: Aumentar el tamaÃ±o de la muestra (\\(n\\)) Entre mÃ¡s observaciones, mÃ¡s cerca estarÃ¡ \\(\\hat{\\beta}\\) de \\(\\beta\\) (por la ley de los grandes nÃºmeros). Mejorar el diseÃ±o muestral Muestreos estratificados, por conglomerados o con pesos pueden hacer las estimaciones mÃ¡s precisas. Controlar por variables relevantes Incluir mÃ¡s covariables reduce la varianza al explicar mejor el comportamiento de \\(y\\). Usar estimadores eficientes o robustos Si hay heterocedasticidad, los errores estÃ¡ndar robustos o el uso de mÃ©todos como MCO ponderado pueden mejorar la precisiÃ³n. âœ… La importancia de la fuente de los datos Una forma muy eficaz de minimizar el riesgo de una muestra sesgada es usar datos de fuentes confiables y con buen diseÃ±o muestral. Por ejemplo, confiar en los datos del DANE en Colombia o de institutos nacionales de estadÃ­stica en otros paÃ­ses es una prÃ¡ctica fundamental. Estas instituciones diseÃ±an cuidadosamente sus encuestas (como la ECH, ENUT o ENDS) para asegurar que sean representativas de la poblaciÃ³n. Si el muestreo estÃ¡ bien hecho desde el inicio, el margen de error se reduce y nuestras inferencias serÃ¡n mucho mÃ¡s confiables. ğŸ’¡ Mensaje clave: La incertidumbre no es un error: es una caracterÃ­stica natural del trabajo con datos. Lo importante no es eliminarla, sino medirla bien, comunicarla con claridad y tenerla en cuenta al tomar decisiones. Â¿Y si mantengo fija la muestra? Hasta ahora nos enfocamos en la incertidumbre que surge por el azar de la muestra. Ahora exploramos otra fuente igual de importante: la variabilidad del tÃ©rmino de error , incluso si la muestra es fija. Hasta ahora vimos que la incertidumbre puede surgir del hecho de que trabajamos con una muestra: cada subconjunto aleatorio de la poblaciÃ³n genera estimadores ligeramente diferentes. Pero hay una segunda fuente de incertidumbre: el tÃ©rmino de error \\(u_i\\). Recordemos el modelo: \\[ y_i = \\beta_0 + \\beta_1 x_i + u_i \\] Aunque tuviÃ©ramos toda la poblaciÃ³n (o una muestra perfecta), no podrÃ­amos predecir perfectamente \\(y_i\\) porque el valor de \\(u_i\\) sigue siendo desconocido. El termino de error \\(u_i\\) representa todo lo que influye en \\(y_i\\) pero no estÃ¡ capturado por \\(x_i\\). Por ejemplo, en un modelo donde \\(y_i\\) es el salario y \\(x_i\\) es la educaciÃ³n: Habilidades intrÃ­nsecas Redes de contacto Experiencia laboral previa Suerte (Â¡sÃ­, tambiÃ©n cuenta!) Todo eso se concentra en \\(u_i\\), que es no observable, pero no irrelevante. Incluso si estimamos \\(\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i\\) con una muestra fija: Nuestra recta de regresiÃ³n capta la tendencia promedio. Pero los valores observados de \\(y_i\\) se dispersan alrededor de la recta por culpa de \\(u_i\\). Esto se ve asÃ­: Al repetir el proceso de muestreo muchas veces, cada muestra tendrÃ¡ su propia recta de regresiÃ³n, pero todas estarÃ¡n dispersas alrededor de la recta poblacional. Vamos a hacer lo siguiente: Tomamos una sola muestra fija de 30 individuos de la poblaciÃ³n. Mantenemos sus valores de \\(x_i\\) constantes. Re-generamos el tÃ©rmino de error \\(u_i \\sim N(0,1)\\) varias veces. Calculamos nuevos valores de \\(y_i = 3 + 0.5x_i + u_i\\) en cada iteraciÃ³n. Estimamos una regresiÃ³n para cada muestra simulada. Esto nos permite ver cÃ³mo las estimaciones varÃ­an Ãºnicamente por culpa del tÃ©rmino de error, manteniendo fija la muestra. AquÃ­ vemos que, aunque la muestra es fija, las rectas de regresiÃ³n se dispersan alrededor de la recta poblacional. Esto es porque el tÃ©rmino de error \\(u_i\\) introduce variabilidad en los valores de \\(y_i\\). â˜ï¸ Una aclaraciÃ³n importante sobre el insesgamiento Al repetir el proceso de muestreo muchas veces, cada muestra tendrÃ¡ su propia recta de regresiÃ³n. Pero ojo: aunque esas rectas se vean â€œalrededorâ€ de la recta poblacional en nuestras simulaciones, esto no siempre ocurre en la vida real. El hecho de que las estimaciones se agrupen alrededor de los verdaderos valores poblacionales depende de que se cumplan los supeustos de modelo de regresiÃ³n lineal. Por ejemplo: Si el tÃ©rmino de error \\(u_i\\) estÃ¡ correlacionado con \\(x_i\\) (por ejemplo, porque omitimos una variable relevante), entonces nuestro estimador de \\(\\beta_1\\) estarÃ¡ sesgado. Si hay errores de mediciÃ³n, mala especificaciÃ³n del modelo o selecciÃ³n no aleatoria, tambiÃ©n se viola el insesgamiento. ğŸ¯ Â¿CuÃ¡ndo es cierto que nuestras estimaciones â€œse agrupanâ€ alrededor del verdadero \\(\\beta_1\\)? Cuando se cumplen los supuestos del modelo clÃ¡sico de regresiÃ³n lineal, en particular exogeneidad estricta o independencia del tÃ©rmino de error \\(u_i\\) respecto a las variables explicativas \\(x_i\\). En algunos libros se conoce como esperanza condicional igual cero: \\[ \\mathbb{E}[u_i \\mid x_i] = 0 \\] ğŸ” Si este supuestos se cumplen, entonces nuestro estimador de MÃ­nimos Cuadrados Ordinarios (MCO) es insesgado: \\[ \\mathbb{E}[\\hat{\\beta}_1] = \\beta_1 \\] Es decir, si repitiÃ©ramos el experimento de muestreo muchas veces, el promedio de nuestras estimaciones convergerÃ­a al verdadero valor poblacional. Pero si no se cumplen, como veremos mÃ¡s adelante, podemos tener: Estimadores sesgados Estimadores inconsistentes Intervalos de confianza y pruebas de hipÃ³tesis invÃ¡lidos ğŸ’¬ Entonces, Â¿las simulaciones que hicimos son â€œrealistasâ€? SÃ­â€¦ bajo los supuestos del modelo clÃ¡sico. En nuestras simulaciones controlamos todo: sabemos exactamente cÃ³mo se genera \\(y_i\\), y aseguramos que \\(u_i\\) sea independiente de \\(x_i\\). Por eso nuestras estimaciones tienden a agruparse cerca de la recta poblacional. Pero en el mundo real, los datos no vienen con etiqueta de â€œsupuestos cumplidosâ€. Por eso uno de los grandes desafÃ­os del anÃ¡lisis economÃ©trico es diagnosticar y justificar si los supuestos se cumplen. Y si no se cumplen, buscar soluciones: variables instrumentales, variables omitidas, diseÃ±os cuasiexperimentales, diseÃ±os experimentales, etc. ğŸ’¡ ConclusiÃ³n clave: El tÃ©rmino de error no desaparece, incluso cuando tenemos una muestra grande o bien diseÃ±ada. Por eso, cualquier estimaciÃ³n puntual (\\(\\hat{\\beta}_1\\)) debe ir acompaÃ±ada de una medida de incertidumbre, como el error estÃ¡ndar o un intervalo de confianza. Esto nos prepara para el siguiente paso: la inferencia estadÃ­stica. ğŸ“˜ Preguntas de repaso Â¿QuÃ© diferencia hay entre el modelo poblacional y el modelo muestral? Â¿CuÃ¡l de los dos observamos y cuÃ¡l inferimos? Â¿Por quÃ© decimos que el tÃ©rmino de error \\(u_i\\) es una fuente de incertidumbre, incluso si la muestra estÃ¡ fija? Â¿QuÃ© condiciones deben cumplirse para que el estimador de MÃ­nimos Cuadrados Ordinarios (MCO) sea insesgado? Â¿QuÃ© significa que un estimador sea insesgado â€œen promedioâ€? Â¿Eso garantiza que cualquier muestra nos darÃ¡ un buen resultado? Â¿QuÃ© implicaciones tiene usar una muestra mal diseÃ±ada o no representativa? En la simulaciÃ³n Monte Carlo, Â¿por quÃ© las rectas de regresiÃ³n estimadas con diferentes muestras se agrupan alrededor de la recta poblacional? Â¿QuÃ© observas cuando repetimos la estimaciÃ³n con una misma muestra, pero re-generamos el tÃ©rmino de error? Â¿QuÃ© se mantiene constante y quÃ© varÃ­a? En tus propias palabras, Â¿por quÃ© no podemos predecir perfectamente \\(y_i\\) aunque conozcamos bien \\(x_i\\)? Â¿Por quÃ© se llama â€œMonte Carloâ€ a este mÃ©todo de simulaciÃ³n? Â¿QuÃ© relaciÃ³n tiene con el azar? Â¿QuÃ© riesgos implica asumir que los supuestos del modelo clÃ¡sico se cumplen cuando en realidad no lo hacen? Â¿QuÃ© estrategias puedes usar si sospechas que \\(u_i\\) estÃ¡ correlacionado con \\(x_i\\)? Menciona al menos dos. Â¿Crees que todas las fuentes oficiales de datos (como el DANE) garantizan muestras perfectamente representativas? Â¿QuÃ© condiciones lo permitirÃ­an? âœï¸ Opcional para prÃ¡ctica adicional: simula tu propia poblaciÃ³n de juguete con una relaciÃ³n negativa entre \\(x\\) y \\(y\\), y repite el ejercicio de Monte Carlo. Â¿QuÃ© cambia? Â¿QuÃ© se mantiene? "],["regresiÃ³n-lineal.html", "2 RegresiÃ³n lineal ğŸ¯ Objetivo del capÃ­tulo ğŸ” Â¿QuÃ© significa encontrar la â€œmejor lÃ­neaâ€? MCO ğŸ“Š Propiedades y supuestos ğŸ“˜ Preguntas de repaso", " 2 RegresiÃ³n lineal ğŸ¯ Objetivo del capÃ­tulo En este capitulo vamos a: 1. Entender quÃ© es una regresiÃ³n lineal y cÃ³mo se ve grÃ¡ficamente. 2. Aprender cÃ³mo se calcula la mejor lÃ­nia con mÃ­nimos cuadrados ordinarios (MCO) 3. Explorar quÃ© hace un buen estiamdor y cÃ³mo evaluarlo ğŸ” Â¿QuÃ© significa encontrar la â€œmejor lÃ­neaâ€? Antes de hablar de estimaciones, pensemos en cÃ³mo se generan los datos: Supondremos que hay un modelo poblacional o proceso generador de datos: \\[ y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i \\] \\(y_i\\): variable dependiente (lo que queremos explicar) \\(x_i\\): variable independiente \\(\\beta_0, \\beta_1\\): parÃ¡metros poblacionales \\(\\epsilon_i\\): tÃ©rmino de error: todo lo que afecta a \\(y_i\\) y no estÃ¡ en \\(x_i\\) El tÃ©rmino \\(\\epsilon_i\\) captura factores no observados, errores de mediciÃ³n, y variaciÃ³n aleatoria. Es fundamental porque incluso si tuviÃ©ramos los valores verdaderos de \\(\\beta_0\\) y \\(\\beta_1\\), seguirÃ­amos sin poder predecir perfectamente \\(y_i\\) debido a este componente. En la prÃ¡ctica, estimamos los parÃ¡metros a partir de una muestra. Esto nos da una versiÃ³n estimada del modelo: \\[ \\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i \\] Y calculamos los residuos (errores estimados): \\[ \\hat{\\epsilon}_i = y_i - \\hat{y}_i \\] Queremos encontrar la lÃ­nea que prediga \\(y_i\\) con la menor cantidad posible de errores. Eso significa minimizar: \\[ \\text{SRC} = \\sum_{i = 1}^{n} \\hat{\\epsilon}_i^2 \\] Esto se conoce como el criterio de mÃ­nimos cuadrados. ğŸ¨ Ilustremos esto con un ejemplo visual Creemos unos nuevos datos para ilustrar esto. La linea de regresiÃ³n es igual a \\(\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x\\) donde _0$ y \\(\\hat{\\beta}_1\\) son los parÃ¡metros estimados de la regresiÃ³n. En este caso, \\(\\hat{\\beta}_0 = 6\\) y \\(\\hat{\\beta}_1 = 0.2\\). Para cada una de las observaciones podemos encontrar el y estimado \\(\\hat{y}_i\\). En la siguiente figura, la lÃ­nea naranja representa la lÃ­nea de regresiÃ³n estimada. Para cada una de las observaciones podemos calcular los errores: \\(\\epsilon_i = y_i - \\hat{y}_i\\), como se observa en el siguiente grÃ¡fico. Ahora podemos probar con otras lineas y ver cÃ³mo se comportan los errores. En el siguiente grafico, la lÃ­nea de regresiÃ³n estimada es \\(\\hat{y} = 3 + 0.2 x\\). Es evidente que los errores estiamdos son mÃ¡s grandes que los errores estimados en el grÃ¡fico anterior. Probemos ahora con una lÃ­nea de regresiÃ³n estimada que no se ajusta a los datos, \\(\\hat{y} = 10 - 0.8 x\\). En este caso, los errores son aÃºn mÃ¡s grandes. Recuerda que SRC es igual a: \\(\\left(\\sum e_i^2\\right)\\): Errores mÃ¡s grandes reciben penalizaciones mÃ¡s grandes. La estimaciÃ³n de MCO es la combinaciÃ³n de \\(\\hat{\\beta}_0\\) y \\(\\hat{\\beta}_1\\) que minimiza la SRC MCO Formalmente En una regresiÃ³n lineal simple, el estimador de MCO proviene de escoger \\(\\hat{\\beta}_0\\) y \\(\\hat{\\beta}_1\\) que minimice la suma de residuos al cuadrado (SRC), i.e., \\[ \\min_{\\hat{\\beta}_0,\\, \\hat{\\beta}_1} \\text{SRC} \\] donde \\[ \\text{SRC} = \\sum_{i = 1}^{n} (y_i - \\hat{y}_i)^2 = \\sum_{i = 1}^{n} (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)^2 \\] El estimador de MCO es el valor de \\(\\hat{\\beta}_0\\) y \\(\\hat{\\beta}_1\\) que minimiza la SRC. pero nosotros sabemos que \\(\\text{SRC} = \\sum_i \\tilde{\\epsilon_i}^2\\). Now use the definitions of \\(\\tilde{\\epsilon_i}\\) and \\(\\hat{y}\\). \\[ \\begin{aligned} \\tilde{\\epsilon_i}^2 &amp;= \\left( y_i - \\hat{y}_i \\right)^2 = \\left( y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i \\right)^2 \\\\ &amp;= y_i^2 - 2 y_i \\hat{\\beta}_0 - 2 y_i \\hat{\\beta}_1 x_i + \\hat{\\beta}_0^2 + 2 \\hat{\\beta}_0 \\hat{\\beta}_1 x_i + \\hat{\\beta}_1^2 x_i^2 \\end{aligned} \\] Recordatorio: Minimizar una funciÃ³n multivariada requiere (1) que las primeras derivadas sean iguales a cero (las condiciones de primer orden) y (2) las condiciones de segundo orden (concavidad). Nos estamos acercando. Necesitamos minimizar la SRC. \\[ \\text{SRE} = \\sum_i \\tilde{e_i}^2 = \\sum_i \\left( y_i^2 - 2 y_i \\hat{\\beta}_0 - 2 y_i \\hat{\\beta}_1 x_i + \\hat{\\beta}_0^2 + 2 \\hat{\\beta}_0 \\hat{\\beta}_1 x_i + \\hat{\\beta}_1^2 x_i^2 \\right) \\] For the first-order conditions of minimization, we now take the first derivates of SSE with respect to \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\). \\[ \\begin{aligned} \\dfrac{\\partial \\text{SRC}}{\\partial \\hat{\\beta}_0} &amp;= \\sum_i \\left( 2 \\hat{\\beta}_0 + 2 \\hat{\\beta}_1 x_i - 2 y_i \\right) = 2n \\hat{\\beta}_0 + 2 \\hat{\\beta}_1 \\sum_i x_i - 2 \\sum_i y_i \\\\ &amp;= 2n \\hat{\\beta}_0 + 2n \\hat{\\beta}_1 \\overline{x} - 2n \\overline{y} \\end{aligned} \\] donde \\(\\overline{x} = \\frac{\\sum x_i}{n}\\) y \\(\\overline{y} = \\frac{\\sum y_i}{n}\\) son medias muestrales de \\(x\\) y \\(y\\) (de tamaÃ±o \\(n\\)). Las condiciones de primer orden establecen que las derivadas deben ser iguales a cero: \\[ \\dfrac{\\partial \\text{SSE}}{\\partial \\hat{\\beta}_0} = 2n \\hat{\\beta}_0 + 2n \\hat{\\beta}_1 \\overline{x} - 2n \\overline{y} = 0 \\] Lo que implica \\[ \\hat{\\beta}_0 = \\overline{y} - \\hat{\\beta}_1 \\overline{x} \\] Ahora para \\(\\hat{\\beta}_1\\). Tomemos la derivada de la SRC con respecto a \\(\\hat{\\beta}_1\\) \\[ \\begin{aligned} \\dfrac{\\partial \\text{SSE}}{\\partial \\hat{\\beta}_1} &amp;= \\sum_i \\left( 2 \\hat{\\beta}_0 x_i + 2 \\hat{\\beta}_1 x_i^2 - 2 y_i x_i \\right) = 2 \\hat{\\beta}_0 \\sum_i x_i + 2 \\hat{\\beta}_1 \\sum_i x_i^2 - 2 \\sum_i y_i x_i \\\\ &amp;= 2n \\hat{\\beta}_0 \\overline{x} + 2 \\hat{\\beta}_1 \\sum_i x_i^2 - 2 \\sum_i y_i x_i \\end{aligned} \\] Igualarlo a cero \\[ \\dfrac{\\partial \\text{SSE}}{\\partial \\hat{\\beta}_1} = 2n \\hat{\\beta}_0 \\overline{x} + 2 \\hat{\\beta}_1 \\sum_i x_i^2 - 2 \\sum_i y_i x_i = 0 \\] y reemplazarlo \\(\\hat{\\beta}_0\\), i.e., \\(\\hat{\\beta}_0 = \\overline{y} - \\hat{\\beta}_1 \\overline{x}\\). Thus, \\[ 2n \\left(\\overline{y} - \\hat{\\beta}_1 \\overline{x}\\right) \\overline{x} + 2 \\hat{\\beta}_1 \\sum_i x_i^2 - 2 \\sum_i y_i x_i = 0 \\] Continuando \\[ 2n \\left(\\overline{y} - \\hat{\\beta}_1 \\overline{x}\\right) \\overline{x} + 2 \\hat{\\beta}_1 \\sum_i x_i^2 - 2 \\sum_i y_i x_i = 0 \\] \\[ 2n \\overline{y}\\,\\overline{x} - 2n \\hat{\\beta}_1 \\overline{x}^2 + 2 \\hat{\\beta}_1 \\sum_i x_i^2 - 2 \\sum_i y_i x_i = 0 \\] \\[ \\implies 2 \\hat{\\beta}_1 \\left( \\sum_i x_i^2 - n \\overline{x}^2 \\right) = 2 \\sum_i y_i x_i - 2n \\overline{y}\\,\\overline{x} \\] \\[ \\implies \\hat{\\beta}_1 = \\dfrac{\\sum_i y_i x_i - 2n \\overline{y}\\,\\overline{x}}{\\sum_i x_i^2 - n \\overline{x}^2} = \\dfrac{\\sum_i (x_i - \\overline{x})(y_i - \\overline{y})}{\\sum_i (x_i - \\overline{x})^2} \\] LISTOO! Ahora tenemos nuestros lindos estimadores \\[ \\hat{\\beta}_1 = \\dfrac{\\sum_i (x_i - \\overline{x})(y_i - \\overline{y})}{\\sum_i (x_i - \\overline{x})^2} \\] and the intercept \\[ \\hat{\\beta}_0 = \\overline{y} - \\hat{\\beta}_1 \\overline{x} \\] Ya sabes de dÃ³nde proviene la parte de mÃ­nimos cuadrados en el tÃ©rmino â€œmÃ­nimos cuadrados ordinariosâ€. ğŸŠ Ahora pasamos a las propiedades (implÃ­citas) de los MÃ­nimos Cuadrados Ordinarios (MCO / OLS). ğŸ“Š Propiedades y supuestos Â¿QuÃ© hace a un buen estimador? Antes de hablar de propiedades del estimador de MCO, recordemos algunas herramientas fundamentales de estadÃ­stica. 2.0.1 ğŸ“ˆ Repaso: Funciones de densidad Las funciones de densidad de probabilidad (FDP, o PDF en inglÃ©s) describen la probabilidad de que una variable aleatoria continua tome valores dentro de un intervalo dado. La probabilidad total bajo la curva es 1. Ejemplo: para una variable normal estÃ¡ndar, la probabilidad de que tome un valor entre -2 y 0 es: \\[ \\mathop{\\text{P}}\\left(-2 \\leq X \\leq 0\\right) = 0.48 \\] Otro ejemplo clÃ¡sico es la probabilidad de que una variable aleatoria normal estÃ¡ndar tome un valor entre -1.96 y 1.96: \\(\\mathop{\\text{P}}\\left(-1.96 \\leq X \\leq 1.96\\right) = 0.95\\) O la probabilidad de que una variable aleatoria normal estÃ¡ndar tome un valor mayor a 2: \\(\\mathop{\\text{P}}\\left(X &gt; 2\\right) = 0.023\\) ğŸ¤” Â¿QuÃ© propiedades buscamos en un estimador? Imaginemos que intentamos estimar un parÃ¡metro verdadero \\(\\beta\\), y tenemos tres mÃ©todos distintos. Cada uno produce una distribuciÃ³n diferente para \\(\\hat{\\beta}\\). Pregunta: Â¿QuÃ© propiedades podrÃ­an ser importantes para un estimador? Propiedad 1. Insesgamiento Es decir, si repitiÃ©ramos el experimento muchas veces, Â¿el estimador tiende a acercarse al valor verdadero del parÃ¡metro que estamos tratando de estimar? El sesgo mide si el estimador se acerca al valor real en promedio: ğŸ§ª Â¿QuÃ© significa â€œrepetir el experimentoâ€? En este contexto, repetir el experimento puede entenderse de tres formas, todas vÃ¡lidas para pensar en la incertidumbre de un estimador: Cambiar la muestra: imaginar que tomamos muchas muestras aleatorias distintas de la poblaciÃ³n. Mantener fija la muestra, pero cambiar los errores: incluso si los valores de \\(x_i\\) no cambian, los valores de \\(y_i\\) pueden variar si asumimos que los errores \\(\\epsilon_i\\) son aleatorios. Recuerda que \\(y_i\\) sigue un proceso generador de datos subyacente. Cambiar ambos simultÃ¡neamente: es el caso mÃ¡s comÃºn en simulaciones â€” se sortean tanto los \\(x_i\\) como los \\(\\epsilon_i\\). En cualquiera de los tres escenarios, obtendrÃ­amos distintos valores de \\(\\hat{\\beta}\\). Eso nos permite construir una distribuciÃ³n muestral del estimador y analizar propiedades como el sesgo. âš ï¸ Importante: cuando hablamos de â€œrepetir el experimentoâ€, no queremos decir que volvamos a observar a las mismas personas varias veces con diferentes valores de \\(x\\) (por ejemplo, dÃ¡ndoles distintos aÃ±os de educaciÃ³n). Lo que estamos haciendo es imaginar escenarios hipotÃ©ticos en los que la muestra o los errores cambian, y ver cÃ³mo eso afecta al estimador. Estos experimentos no se pueden realizar en la realidad con una misma persona, pero sÃ­ los podemos simular por computadora o analizar teÃ³ricamente. MÃ¡s formalmente: Â¿La media de la distribuciÃ³n del estimador es igual al parÃ¡metro que estima? En promedio (despuÃ©s de muchas repeticiones), Â¿el estimador tiende hacia el valor correcto? MÃ¡s formalmente: Â¿La media de la distribuciÃ³n del estimador es igual al parÃ¡metro que estima? \\[ \\mathop{\\text{Sesgo}}_\\beta \\left( \\hat{\\beta} \\right) = \\mathop{\\boldsymbol{E}}\\left[ \\hat{\\beta} \\right] - \\beta \\] Estimador Insesagado: \\(\\mathop{\\boldsymbol{E}}\\left[ \\hat{\\beta} \\right] = \\beta\\) Estimador Sesagado: \\(\\mathop{\\boldsymbol{E}}\\left[ \\hat{\\beta} \\right] \\neq \\beta\\) Propiedad 2: Varianza TambiÃ©n queremos que nuestras estimaciones no varÃ­en demasiado de una muestra a otra. En otras palabras: queremos un estimador que sea estable, no que en cada muestra nos dÃ© un valor completamente distinto. La varianza mide cuÃ¡nta variaciÃ³n hay en las estimaciones \\(\\hat{\\beta}\\) que obtenemos al repetir el experimento (cambiando la muestra, los errores, o ambos): \\[ \\text{Var} \\left( \\hat{\\beta} \\right) = \\mathbb{E} \\left[ \\left( \\hat{\\beta} - \\mathbb{E}[\\hat{\\beta}] \\right)^2 \\right] \\] Un estimador con menor varianza produce resultados mÃ¡s consistentes entre muestras. Esto lo hace mÃ¡s confiable, incluso si no es perfecto. ğŸ¯ Queremos que nuestras estimaciones estÃ©n â€œconcentradasâ€ cerca del valor esperado, no dispersas como tiros al aire. Veamos un ejemplo visual de cÃ³mo la varianza afecta a las distribuciones de los estimadores. La curva rosada representa un estimador con baja varianza: la mayorÃ­a de los valores de \\(\\hat{\\beta}\\) estÃ¡n cerca de \\(\\beta\\). Mientras que la curva gris oscuro representa un estimador con alta varianza: sus valores estÃ¡n mÃ¡s dispersos. A igualdad de sesgo, preferimos el estimador que tenga menor varianza. ğŸ¯ El trade-off: sesgo vs.Â varianza Hasta ahora hablamos del sesgo y de la varianza por separado. Pero muchas veces, mejorar uno implica empeorar el otro. Esto se conoce como el ** trade-off entre sesgo y varianza**. Â¿DeberÃ­amos aceptar un poco de sesgo si eso nos permite reducir mucho la varianza? En econometrÃ­a, solemos preferir estimadores insesgados (o al menos consistentes), porque valoramos la interpretaciÃ³n causal y teÃ³rica de los parÃ¡metros. Pero en otras disciplinas, como el aprendizaje automÃ¡tico o la predicciÃ³n estadÃ­stica, es comÃºn aceptar un pequeÃ±o sesgo si con ello se logra una gran reducciÃ³n en la varianza y, en consecuencia, una mejor predicciÃ³n promedio. veÃ¡mos esta idea: Propiedad 3: Consistencia La consistencia es una propiedad clave que nos dice quÃ© pasa con el estimador cuando la muestra es cada vez mÃ¡s grande. Intuitivamente, un estimador es consistente si, al aumentar el tamaÃ±o de la muestra, sus valores se acercan cada vez mÃ¡s al valor verdadero del parÃ¡metro \\(\\beta\\). Esto nos da confianza de que, con datos suficientes, estaremos muy cerca del valor correcto. Formalmente, un estimador \\(\\hat{\\beta}\\) es consistente si: \\[ \\hat{\\beta} \\xrightarrow{p} \\beta \\quad \\text{cuando } n \\to \\infty \\] Esto se lee como: â€œ\\(\\hat{\\beta}\\) converge en probabilidad a \\(\\beta\\)â€. Es decir, la probabilidad de que \\(\\hat{\\beta}\\) se aleje mucho de \\(\\beta\\) se hace cada vez mÃ¡s pequeÃ±a a medida que usamos muestras mÃ¡s grandes. VeÃ¡mos lo que ocurre cuando la muestra crece: La curva naranja representa una estimaciÃ³n con mucha incertidumbre (muestra pequeÃ±a). La curva gris oscura representa una muestra de tamaÃ±o mediano. Mientras que la curva rosada muestra cÃ³mo la estimaciÃ³n se concentra alrededor de \\(\\beta\\) con una muestra grande. ğŸ¯ Un estimador consistente se â€œafinaâ€ con mÃ¡s datos: no solo mejora su varianza, sino que tiende a decir la verdad. Propiedad 4: Eficiencia La eficiencia combina las ideas de sesgo y varianza. Entre todos los estimadores insesgados, el mÃ¡s eficiente es aquel que tiene la menor varianza posible. Es decir, si dos estimadores son igual de â€œcorrectos en promedioâ€, preferimos el que sea mÃ¡s estable. La eficiencia no se refiere a un Ãºnico estimador, sino a una comparaciÃ³n entre estimadores. Formalmente, un estimador \\(\\hat{\\beta}\\) es eficiente si: \\[ \\text{Var}(\\hat{\\beta}) \\leq \\text{Var}(\\hat{\\beta}&#39;) \\] para cualquier otro estimador \\(\\hat{\\beta}&#39;\\) que tambiÃ©n sea insesgado. Esto significa que ningÃºn otro estimador insesgado tiene una varianza menor que \\(\\hat{\\beta}\\). ğŸ’¡ En el contexto de mÃ­nimos cuadrados ordinarios (MCO), cuando se cumplen ciertos supuestos (los del teorema de Gauss-Markov), el estimador \\(\\hat{\\beta}_{\\text{MCO}}\\) es el Mejor Estimador Lineal Insesgado, tambiÃ©n conocido como MELI: âœ”ï¸ Mejor â†’ tiene la menor varianza âœ”ï¸ Estimador Lineal â†’ combinaciÃ³n lineal de los datos âœ”ï¸ Insesgado â†’ \\(\\mathbb{E}[\\hat{\\beta}] = \\beta\\) ğŸ“ En resumen, un estimador eficiente es tan preciso como permite la informaciÃ³n disponible en los datos, sin sacrificar insesgamiento. Resumen de las propiedades Propiedad DescripciÃ³n Insesgamiento El estimador no se aleja sistemÃ¡ticamente del valor verdadero. Varianza El estimador tiene poca variaciÃ³n entre muestras. Consistencia A medida que aumenta el tamaÃ±o de la muestra, el estimador converge al valor verdadero. Eficiencia El estimador tiene la menor varianza posible entre todos los estimadores insesgados. ğŸ§  Nota de cierre: cÃ³mo interpretar cada propiedad Cada propiedad que vimos tiene un enfoque ligeramente distinto sobre cÃ³mo pensar la incertidumbre: Sesgo: Â¿En promedio (tras repetir el experimento), el estimador acierta? Varianza: Â¿QuÃ© tanto cambia el estimador de una muestra a otra? Consistencia: Â¿El estimador se acerca al valor verdadero si usamos una muestra mÃ¡s grande del mismo experimento? Eficiencia: Â¿Este estimador es mejor (mÃ¡s preciso) que otros estimadores insesgados disponibles? ğŸ” Las primeras dos propiedades (sesgo y varianza) se entienden a travÃ©s de repeticiones hipotÃ©ticas del experimento. ğŸ“ˆ La consistencia se analiza observando lo que ocurre cuando crece el tamaÃ±o muestral. âš–ï¸ La eficiencia es una comparaciÃ³n entre estimadores, dado que todos sean insesgados. Estas ideas son fundamentales para entender cÃ³mo evaluar y justificar un estimador en econometrÃ­a. ğŸ“˜ Preguntas de repaso Verdadero o falso (V/F) Un estimador puede ser insesgado pero tener alta varianza. (V/F) La consistencia se refiere a repetir el experimento muchas veces. (V/F) Un estimador eficiente siempre es consistente. (V/F) Si un estimador es insesgado y eficiente, no puede ser mejorado bajo los supuestos del modelo. SelecciÃ³n mÃºltiple {-} Â¿CuÃ¡l de las siguientes afirmaciones es correcta respecto a la eficiencia? A. Es una propiedad absoluta de un estimador. B. Se refiere a quÃ© tan cerca estÃ¡ \\(\\hat{\\beta}\\) del promedio de los datos. C. Compara la varianza entre estimadores insesgados. D. Es sinÃ³nimo de consistencia. Â¿QuÃ© pasa con un estimador consistente cuando el tamaÃ±o muestral crece? A. Se vuelve insesgado automÃ¡ticamente. B. Se aleja del valor verdadero. C. Su varianza se hace infinita. D. Se aproxima al valor verdadero con alta probabilidad. Respuesta abierta Explica con tus palabras quÃ© significa que un estimador sea insesgado. Â¿Por quÃ© esta propiedad es importante en econometrÃ­a?** Â¿Por quÃ© puede ser Ãºtil, en algunos contextos, aceptar un estimador sesgado? Da un ejemplo donde podrÃ­a ser preferible.** Â¿En quÃ© se diferencia el concepto de varianza del de eficiencia? Â¿Pueden dos estimadores tener la misma varianza pero distinta eficiencia?** SupÃ³n que tienes dos estimadores: - A es insesgado pero tiene alta varianza. - B tiene un pequeÃ±o sesgo pero varianza muy baja. Â¿CuÃ¡l elegirÃ­as para un problema donde la prioridad es predecir bien el valor de \\(y\\)? Â¿CambiarÃ­a tu respuesta si el objetivo fuera estimar un efecto causal? Justifica tu elecciÃ³n. "],["repaso-de-matrices.html", "3 Repaso de matrices Matrices Vectores Operaciones con matrices Determinantes Matriz inversa Rango de una matriz Sistemas de ecuaciones lineales Matrices cuadradas especiales Derivadas de una funciÃ³n multidimensional ğŸ“˜ Preguntas de repaso", " 3 Repaso de matrices Antes de introducir los supuestos fundamentales del modelo de regresiÃ³n lineal, es importante repasar algunos conceptos clave del Ã¡lgebra matricial. Este lenguaje permite expresar de forma compacta y elegante muchos de los resultados economÃ©tricos, facilitando la comprensiÃ³n de los modelos lineales y sus propiedades. Matrices Una matriz \\(A \\in \\mathbb{R}^{m \\times n}\\) es un conjunto de elementos \\(a_{ij}\\), donde \\(i = 1, \\ldots, m\\) (filas) y \\(j = 1, \\ldots, n\\) (columnas), organizados de la siguiente manera: \\[ A = \\begin{pmatrix} a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1n} \\\\ a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{m1} &amp; a_{m2} &amp; \\cdots &amp; a_{mn} \\end{pmatrix} \\] La dimensiÃ³n o orden de la matriz es \\(m \\times n\\), lo que indica que tiene \\(m\\) filas y \\(n\\) columnas. Cuando \\(m = n\\), se dice que la matriz es cuadrada; si \\(m \\neq n\\), es una matriz rectangular. Las matrices se representan con letras mayÃºsculas en negrita, como \\(\\mathbf{A}\\), y sus elementos con letras minÃºsculas con subÃ­ndices, como \\(a_{ij}\\). Los elementos de una matriz pueden ser nÃºmeros reales, \\(a_{ij} \\in \\mathbb{R}\\). Ejemplo. La matriz \\[ B = \\begin{pmatrix} 3 &amp; 1 &amp; 7 \\\\ 2 &amp; 4 &amp; 5 \\end{pmatrix} \\] es una matriz rectangular de orden \\(2 \\times 3\\). Tiene 2 filas y 3 columnas. El elemento en la fila 2 y columna 3 es \\(b_{23} = 5\\). Traspuesta de una matriz La traspuesta de una matriz \\(A = [a_{ij}]\\) de dimensiÃ³n \\(m \\times n\\) es otra matriz \\(A&#39; = [a_{ji}]\\) de dimensiÃ³n \\(n \\times m\\), obtenida al intercambiar filas por columnas. Es decir, la primera fila de \\(A\\) se convierte en la primera columna de \\(A&#39;\\), la segunda fila en la segunda columna, y asÃ­ sucesivamente. \\[ A&#39; = \\begin{pmatrix} a_{11} &amp; a_{21} &amp; \\cdots &amp; a_{m1} \\\\ a_{12} &amp; a_{22} &amp; \\cdots &amp; a_{m2} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{1n} &amp; a_{2n} &amp; \\cdots &amp; a_{mn} \\end{pmatrix} \\] Ejemplo. Sea la matriz \\[ A = \\begin{pmatrix} 6 &amp; 5 &amp; 7 &amp; 4 \\\\ 5 &amp; 4 &amp; 2 &amp; 5 \\\\ 1 &amp; 1 &amp; 11 &amp; 1 \\end{pmatrix} \\] que es de orden \\(3 \\times 4\\). Su traspuesta es: \\[ A&#39; = \\begin{pmatrix} 6 &amp; 5 &amp; 1 \\\\ 5 &amp; 4 &amp; 1 \\\\ 7 &amp; 2 &amp; 11 \\\\ 4 &amp; 5 &amp; 1 \\end{pmatrix} \\] Esta nueva matriz es de orden \\(4 \\times 3\\). Vectores Un vector columna es una matriz de orden \\(m \\times 1\\), es decir, una matriz que solo tiene una columna: \\[ \\mathbf{a} = \\begin{pmatrix} a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_m \\end{pmatrix} \\] Se denota con una letra minÃºscula en negrita y se puede escribir de forma abreviada como \\(\\mathbf{a} = [a_i]\\). Cada elemento \\(a_i\\) indica la posiciÃ³n del componente dentro del vector. Un vector fila, en cambio, es una matriz de orden \\(1 \\times m\\), es decir, solo tiene una fila: \\[ \\mathbf{a}&#39; = \\begin{pmatrix} a_1 &amp; a_2 &amp; \\cdots &amp; a_m \\end{pmatrix} \\] La traspuesta de un vector columna es un vector fila, y viceversa. En lÃ­nea, se escribe \\(\\mathbf{a} = (a_1, a_2, \\ldots, a_m)&#39;\\) para indicar que es columna, usando la notaciÃ³n de traspuesta. Ejemplo Sea el vector columna \\[ \\mathbf{v} = \\begin{pmatrix} 2 \\\\ -1 \\\\ 4 \\end{pmatrix} \\quad \\text{de orden } 3 \\times 1, \\quad \\text{su traspuesta es } \\mathbf{v}&#39; = \\begin{pmatrix} 2 &amp; -1 &amp; 4 \\end{pmatrix} \\] Producto escalar DefiniciÃ³n. Sean \\(\\mathbf{a} = (a_1, \\ldots, a_m)&#39;\\) y \\(\\mathbf{b} = (b_1, \\ldots, b_m)&#39;\\) dos vectores columna del mismo orden \\(m \\times 1\\), su producto escalar es: \\[ \\mathbf{a}&#39;\\mathbf{b} = \\sum_{i=1}^m a_i b_i = a_1b_1 + a_2b_2 + \\cdots + a_m b_m \\] Ejemplo Si \\(\\mathbf{a} = (1, 2, 3)&#39;\\) y \\(\\mathbf{b} = (4, 5, 6)&#39;\\), entonces: \\[ \\mathbf{a}&#39;\\mathbf{b} = 1 \\cdot 4 + 2 \\cdot 5 + 3 \\cdot 6 = 4 + 10 + 18 = 32 \\] Norma y normalizaciÃ³n DefiniciÃ³n. La norma de un vector \\(\\mathbf{x}\\) se define como: \\[ \\|\\mathbf{x}\\| = \\sqrt{\\mathbf{x}&#39;\\mathbf{x}} = \\sqrt{x_1^2 + x_2^2 + \\cdots + x_m^2} \\] El vector normalizado es: \\[ \\frac{\\mathbf{x}}{\\|\\mathbf{x}\\|} \\] Ortogonalidad DefiniciÃ³n. Dos vectores \\(\\mathbf{a}\\) y \\(\\mathbf{b}\\) son ortogonales (se denota \\(\\mathbf{a} \\perp \\mathbf{b}\\)) si: \\[ \\mathbf{a}&#39;\\mathbf{b} = 0 \\] \\[ \\bar{y} = \\frac{\\mathbf{i}&#39;\\mathbf{y}}{\\mathbf{i}&#39;\\mathbf{i}} \\] Operaciones con matrices Igualdad de matrices Dos matrices \\(A = [a_{ij}]\\) y \\(B = [b_{ij}]\\) de igual orden \\(m \\times n\\) son iguales si: \\[ a_{ij} = b_{ij}, \\quad \\text{para todo } i = 1, \\ldots, m; \\; j = 1, \\ldots, n \\] Suma y resta de matrices La suma de dos matrices del mismo orden es la matriz \\(C = A + B = [c_{ij}]\\) donde: \\[ c_{ij} = a_{ij} + b_{ij} \\] Propiedades: Conmutativa: \\(A + B = B + A\\) Asociativa: \\((A + B) + C = A + (B + C)\\) Elemento neutro: \\(A + 0 = A\\) Opuesto: \\(A + (-A) = 0\\) Ejemplo: \\[ A = \\begin{pmatrix} 6 &amp; 5 &amp; 7 &amp; 4 \\\\ 5 &amp; 4 &amp; 2 &amp; 5 \\\\ 1 &amp; 1 &amp; 11 &amp; 1 \\end{pmatrix}, \\quad B = \\begin{pmatrix} 7 &amp; 11 &amp; 2 &amp; 9 \\\\ 5 &amp; 8 &amp; 8 &amp; 1 \\\\ 6 &amp; 10 &amp; 8 &amp; 10 \\end{pmatrix} \\] \\[ A + B = \\begin{pmatrix} 13 &amp; 16 &amp; 9 &amp; 13 \\\\ 10 &amp; 12 &amp; 10 &amp; 6 \\\\ 7 &amp; 11 &amp; 19 &amp; 11 \\end{pmatrix} \\] Resta: se define como \\(A - B = A + (-B)\\) MultiplicaciÃ³n por un escalar \\[ \\lambda A = [\\lambda a_{ij}] \\] Ejemplo: \\[ 2A = \\begin{pmatrix} 12 &amp; 10 &amp; 14 &amp; 8 \\\\ 10 &amp; 8 &amp; 4 &amp; 10 \\\\ 2 &amp; 2 &amp; 22 &amp; 2 \\end{pmatrix} \\] MultiplicaciÃ³n de matrices Sean \\(A \\in \\mathbb{R}^{m \\times n}\\) y \\(B \\in \\mathbb{R}^{n \\times p}\\), el producto \\(AB \\in \\mathbb{R}^{m \\times p}\\) se define por: \\[ c_{ij} = \\sum_{k=1}^{n} a_{ik} b_{kj} \\] Propiedades: Asociativa: \\((AB)C = A(BC)\\) Distributiva: \\(A(B + C) = AB + AC\\) No conmutativa: en general \\(AB \\neq BA\\) Ejemplo: \\[ A = \\begin{pmatrix} 6 &amp; 5 &amp; 7 &amp; 4 \\\\ 5 &amp; 4 &amp; 2 &amp; 5 \\\\ 1 &amp; 1 &amp; 11 &amp; 1 \\end{pmatrix}, \\quad B&#39; = \\begin{pmatrix} 7 &amp; 5 &amp; 6 \\\\ 11 &amp; 8 &amp; 10 \\\\ 2 &amp; 8 &amp; 8 \\\\ 9 &amp; 1 &amp; 10 \\end{pmatrix} \\] \\[ F = A B&#39; = \\begin{pmatrix} 147 &amp; 130 &amp; 182 \\\\ 128 &amp; 78 &amp; 136 \\\\ 49 &amp; 102 &amp; 114 \\end{pmatrix} \\] TransposiciÃ³n de matrices Ya definida en la secciÃ³n anterior. Propiedades clave: \\((A&#39;)&#39; = A\\) \\((A + B)&#39; = A&#39; + B&#39;\\) \\((AB)&#39; = B&#39;A&#39;\\) Traza de una matriz La traza de una matriz cuadrada es la suma de los elementos de su diagonal principal: \\[ \\text{tr}(A) = \\sum_{i=1}^{n} a_{ii} \\] Propiedades: \\(\\text{tr}(A) = \\text{tr}(A&#39;)\\) \\(\\text{tr}(A + B) = \\text{tr}(A) + \\text{tr}(B)\\) \\(\\text{tr}(AB) = \\text{tr}(BA)\\) Ejemplo: \\[ \\text{tr}(F) = 147 + 78 + 114 = 339 \\] Determinantes Para matrices cuadradas \\(A \\in \\mathbb{R}^{n \\times n}\\), el determinante se denota \\(|A|\\). Para \\(2 \\times 2\\): \\[ |A| = \\begin{vmatrix} a &amp; b \\\\ c &amp; d \\end{vmatrix} = ad - bc \\] Para \\(3 \\times 3\\): \\[ |A| = a_{11}a_{22}a_{33} + a_{12}a_{23}a_{31} + a_{13}a_{21}a_{32} - a_{13}a_{22}a_{31} - a_{12}a_{21}a_{33} - a_{11}a_{23}a_{32} \\] Ejemplo: \\[ G = \\begin{pmatrix} 1 &amp; 1 &amp; 3 \\\\ 1 &amp; 1 &amp; 0 \\\\ 3 &amp; 1 &amp; 2 \\end{pmatrix} \\Rightarrow |G| = -6 \\] Matriz inversa Una matriz cuadrada \\(A\\) es invertible si existe \\(A^{-1}\\) tal que: \\[ A A^{-1} = A^{-1} A = I \\] Se calcula como: \\[ A^{-1} = \\frac{1}{|A|} \\cdot \\text{adj}(A) \\] donde \\(\\text{adj}(A)\\) es la matriz adjunta (traspuesta de los cofactores). Propiedades: \\((A^{-1})^{-1} = A\\) \\((AB)^{-1} = B^{-1} A^{-1}\\) \\((A&#39;)^{-1} = (A^{-1})&#39;\\) Ejemplo: \\[ G^{-1} = \\frac{1}{-6} \\begin{pmatrix} 2 &amp; 1 &amp; -3 \\\\ -2 &amp; -7 &amp; 3 \\\\ -2 &amp; 2 &amp; 0 \\end{pmatrix} \\] Rango de una matriz El rango de una matriz es el nÃºmero mÃ¡ximo de filas (o columnas) linealmente independientes. Definiciones: Vectores son linealmente dependientes si \\(c_1a_1 + \\cdots + c_n a_n = 0\\) con \\(c_i \\neq 0\\) Son independientes si la Ãºnica combinaciÃ³n que da cero es con todos los \\(c_i = 0\\) Propiedades: \\(\\text{rang}(AB) \\leq \\min\\{\\text{rang}(A), \\text{rang}(B)\\}\\) Si \\(A\\) es invertible: \\(\\text{rang}(AB) = \\text{rang}(B)\\) \\(\\text{rang}(A) = \\text{rang}(A A&#39;) = \\text{rang}(A&#39; A)\\) Sistemas de ecuaciones lineales Un sistema de ecuaciones lineales con \\(m\\) ecuaciones y \\(n\\) incÃ³gnitas se puede escribir de la forma: \\[ \\begin{aligned} a_{11}x_1 + a_{12}x_2 + \\cdots + a_{1n}x_n &amp;= b_1 \\\\ a_{21}x_1 + a_{22}x_2 + \\cdots + a_{2n}x_n &amp;= b_2 \\\\ &amp;\\vdots \\\\ a_{m1}x_1 + a_{m2}x_2 + \\cdots + a_{mn}x_n &amp;= b_m \\end{aligned} \\] En forma matricial, este sistema se escribe como: \\[ A \\mathbf{x} = \\mathbf{b} \\] donde \\[ A = \\begin{pmatrix} a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1n} \\\\ a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{m1} &amp; a_{m2} &amp; \\cdots &amp; a_{mn} \\end{pmatrix}, \\quad \\mathbf{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{pmatrix}, \\quad \\mathbf{b} = \\begin{pmatrix} b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_m \\end{pmatrix} \\] Sistema de Cramer DefiniciÃ³n. Un sistema de ecuaciones lineales se denomina sistema de Cramer si: La matriz \\(A\\) es cuadrada (\\(m = n\\)) La matriz \\(A\\) es no singular, es decir, \\(|A| \\neq 0\\) En este caso, el sistema tiene una Ãºnica soluciÃ³n dada por: \\[ \\mathbf{x} = A^{-1} \\mathbf{b} \\] Ejemplo numÃ©rico Considere el sistema: \\[ \\begin{aligned} 12x_1 + 20x_2 &amp;= 388 \\\\ 4x_1 + 17x_2 &amp;= 212 \\end{aligned} \\] En forma matricial: \\[ \\begin{pmatrix} 12 &amp; 20 \\\\ 4 &amp; 17 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} 388 \\\\ 212 \\end{pmatrix} \\] Paso 1: Calcular la inversa de \\(A\\) Primero calculamos el determinante: \\[ |A| = 12 \\cdot 17 - 20 \\cdot 4 = 204 - 80 = 124 \\] Luego, la matriz de cofactores traspuesta (adjunta): \\[ \\text{adj}(A) = \\begin{pmatrix} 17 &amp; -20 \\\\ -4 &amp; 12 \\end{pmatrix} \\] Entonces, \\[ A^{-1} = \\frac{1}{124} \\begin{pmatrix} 17 &amp; -20 \\\\ -4 &amp; 12 \\end{pmatrix} \\] Paso 2: Multiplicar \\(A^{-1} \\mathbf{b}\\) \\[ \\mathbf{x} = A^{-1} \\mathbf{b} = \\frac{1}{124} \\begin{pmatrix} 17 &amp; -20 \\\\ -4 &amp; 12 \\end{pmatrix} \\begin{pmatrix} 388 \\\\ 212 \\end{pmatrix} \\] Calculamos el producto: \\[ \\begin{aligned} x_1 &amp;= \\frac{1}{124}(17 \\cdot 388 - 20 \\cdot 212) = \\frac{1}{124}(6596 - 4240) = \\frac{2356}{124} = 19 \\\\ x_2 &amp;= \\frac{1}{124}(-4 \\cdot 388 + 12 \\cdot 212) = \\frac{1}{124}(-1552 + 2544) = \\frac{992}{124} = 8 \\end{aligned} \\] SoluciÃ³n final: \\[ \\boxed{ x_1 = 19, \\quad x_2 = 8 } \\] Este procedimiento es vÃ¡lido siempre que la matriz \\(A\\) sea cuadrada y su determinante no sea cero. Si \\(|A| = 0\\), el sistema no tiene soluciÃ³n Ãºnica: puede ser incompatible o tener infinitas soluciones. Matrices cuadradas especiales Las siguientes matrices cuadradas tienen propiedades estructurales claves que facilitan el desarrollo de mÃ©todos economÃ©tricos. 1. Matriz diagonal Una matriz diagonal \\(A = [a_{ij}] \\in \\mathbb{R}^{m \\times m}\\) tiene ceros fuera de la diagonal principal: \\[ A = \\begin{pmatrix} a_{11} &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; a_{22} &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; a_{mm} \\end{pmatrix} = \\text{diag}(a_{11}, a_{22}, \\ldots, a_{mm}) \\] 2. Matriz identidad La matriz identidad \\(I_m\\) es una matriz diagonal con unos en la diagonal: \\[ I_m = \\begin{pmatrix} 1 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; 1 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; 1 \\end{pmatrix} \\] Propiedades: \\(AI_m = I_mA = A\\), \\(I_m^{-1} = I_m\\) 3. Matriz escalar Una matriz escalar es una matriz diagonal cuyos elementos en la diagonal son iguales a un mismo nÃºmero \\(\\lambda\\): \\[ A = \\lambda I_m \\] 4. Matriz triangular inferior Una matriz triangular inferior cumple: \\[ a_{ij} = 0 \\quad \\text{para todo } i &lt; j \\] \\[ A = \\begin{pmatrix} a_{11} &amp; 0 &amp; \\cdots &amp; 0 \\\\ a_{21} &amp; a_{22} &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{m1} &amp; a_{m2} &amp; \\cdots &amp; a_{mm} \\end{pmatrix} \\] 5. Matriz nula La matriz nula tiene todos sus elementos iguales a cero: \\[ 0 = \\begin{pmatrix} 0 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; 0 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; 0 \\end{pmatrix} \\] 6. Matriz simÃ©trica Una matriz \\(A \\in \\mathbb{R}^{m \\times m}\\) es simÃ©trica si: \\[ A = A&#39; \\quad \\text{es decir, } a_{ij} = a_{ji} \\] 7. Matriz idempotente Una matriz \\(A\\) es idempotente si: \\[ A^2 = A \\] Ejemplo clave: la matriz de proyecciÃ³n sobre el espacio generado por las columnas de \\(X\\): \\[ P = X(X&#39;X)^{-1}X&#39; \\] Cumple: \\(P = P&#39;\\) (simÃ©trica) \\(P^2 = P\\) (idempotente) 8. Matriz ortogonal Una matriz \\(Q\\) es ortogonal si: \\[ Q&#39;Q = QQ&#39; = I \\Rightarrow Q^{-1} = Q&#39; \\] Sus columnas (y filas) son vectores ortonormales. 9. Matrices de proyecciÃ³n: \\(P\\) y \\(M\\) En regresiÃ³n lineal, dos matrices juegan un rol fundamental: a) Matriz de proyecciÃ³n sobre el espacio columna de \\(X\\): \\[ P = X(X&#39;X)^{-1}X&#39; \\] Idempotente: \\(P^2 = P\\) SimÃ©trica: \\(P&#39; = P\\) Proyecta cualquier vector \\(y\\) sobre el espacio generado por las columnas de \\(X\\): \\(\\hat{y} = P y\\) b) Matriz de aniquilaciÃ³n o proyecciÃ³n ortogonal: \\[ M = I - P \\] Idempotente: \\(M^2 = M\\) SimÃ©trica: \\(M&#39; = M\\) Proyecta sobre el complemento ortogonal del espacio generado por \\(X\\): \\(e = M y\\) (residuos) Estas matrices son centrales para expresar la descomposiciÃ³n: \\[ y = \\hat{y} + e = P y + M y \\] donde \\(\\hat{y}\\) es la parte explicada por \\(X\\), y \\(e\\) es la parte no explicada (residuos). Derivadas de una funciÃ³n multidimensional Derivadas de una forma lineal Sea la forma lineal \\(\\mathbf{a}&#39;\\mathbf{x} = a_1x_1 + a_2x_2 + \\cdots + a_nx_n\\), una funciÃ³n escalar de \\(n\\) variables independientes \\(x_1, \\ldots, x_n\\). La derivada parcial con respecto a una variable \\(x_i\\) es simplemente: \\[ \\frac{\\partial \\mathbf{a}&#39;\\mathbf{x}}{\\partial x_i} = a_i \\] La derivada de \\(\\mathbf{a}&#39;\\mathbf{x}\\) con respecto al vector \\(\\mathbf{x}\\) es: \\[ \\frac{\\partial \\mathbf{a}&#39;\\mathbf{x}}{\\partial \\mathbf{x}} = \\begin{pmatrix} \\frac{\\partial \\mathbf{a}&#39;\\mathbf{x}}{\\partial x_1} \\\\ \\frac{\\partial \\mathbf{a}&#39;\\mathbf{x}}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial \\mathbf{a}&#39;\\mathbf{x}}{\\partial x_n} \\end{pmatrix} = \\begin{pmatrix} a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_n \\end{pmatrix} = \\mathbf{a} \\] De forma anÃ¡loga, la derivada de \\(\\mathbf{a}&#39;\\mathbf{x}\\) respecto de \\(\\mathbf{x}&#39;\\) es un vector fila: \\[ \\frac{\\partial \\mathbf{a}&#39;\\mathbf{x}}{\\partial \\mathbf{x}&#39;} = \\begin{pmatrix} a_1 &amp; a_2 &amp; \\cdots &amp; a_n \\end{pmatrix} = \\mathbf{a}&#39; \\] Derivadas de una forma cuadrÃ¡tica Sea la forma cuadrÃ¡tica \\(\\mathbf{x}&#39;A\\mathbf{x}\\), donde \\(A\\) es una matriz simÃ©trica. Esta puede escribirse como: \\[ \\mathbf{x}&#39;A\\mathbf{x} = \\sum_{i=1}^{n} a_{ii}x_i^2 + 2\\sum_{i=1}^{n-1}\\sum_{j=i+1}^{n} a_{ij}x_ix_j \\] La derivada de \\(\\mathbf{x}&#39;A\\mathbf{x}\\) con respecto al vector \\(\\mathbf{x}\\) es: \\[ \\frac{\\partial (\\mathbf{x}&#39;A\\mathbf{x})}{\\partial \\mathbf{x}} = 2A\\mathbf{x} \\] Esto es, un vector columna cuya i-Ã©sima componente es: \\[ \\frac{\\partial (\\mathbf{x}&#39;A\\mathbf{x})}{\\partial x_i} = 2(a_{i1}x_1 + a_{i2}x_2 + \\cdots + a_{in}x_n) \\] Derivadas de segundo orden (matriz Hessiana) La derivada segunda de \\(\\mathbf{x}&#39;A\\mathbf{x}\\) con respecto a \\(x_i\\) es: \\[ \\frac{\\partial^2 (\\mathbf{x}&#39;A\\mathbf{x})}{\\partial x_i^2} = 2a_{ii} \\] La derivada mixta con respecto a \\(x_i\\) y \\(x_j\\) es: \\[ \\frac{\\partial^2 (\\mathbf{x}&#39;A\\mathbf{x})}{\\partial x_i \\partial x_j} = 2a_{ij} \\] La matriz de segundas derivadas (Hessiana) es: \\[ \\frac{\\partial^2 (\\mathbf{x}&#39;A\\mathbf{x})}{\\partial \\mathbf{x} \\partial \\mathbf{x}&#39;} = 2A \\] Resumen Derivada de forma lineal: \\(\\frac{\\partial (\\mathbf{a}&#39;\\mathbf{x})}{\\partial \\mathbf{x}} = \\mathbf{a}\\) Derivada de forma cuadrÃ¡tica: \\(\\frac{\\partial (\\mathbf{x}&#39;A\\mathbf{x})}{\\partial \\mathbf{x}} = 2A\\mathbf{x}\\) Matriz Hessiana: \\(\\frac{\\partial^2 (\\mathbf{x}&#39;A\\mathbf{x})}{\\partial \\mathbf{x} \\partial \\mathbf{x}&#39;} = 2A\\) ğŸ“˜ Preguntas de repaso Sea \\(\\mathbf{i} = (1, 1, \\ldots, 1)&#39;\\) un vector \\(m \\times 1\\) de unos. Calcule \\(\\mathbf{i}&#39;\\mathbf{i}\\). Sean \\(\\mathbf{i} = (1, \\ldots, 1)&#39;\\) y \\(\\mathbf{y} = (y_1, \\ldots, y_m)&#39;\\). Calcule \\(\\mathbf{i}&#39;\\mathbf{y}\\). Demuestre que la media de las observaciones \\(y_1, \\ldots, y_m\\) puede expresarse como: "],["supuestos-de-mco.html", "4 Supuestos de MCO Proceso Generador de Datos Tabla Resumen de Supuestos S1. Linealidad en los ParÃ¡metros S2. Exogeneidad Estricta S3. Colinealidad Imperfecta S4. Perturbaciones EsfÃ©ricas S5. Regresores No EstocÃ¡sticos S6. Normalidad del Error Glosario de SÃ­mbolos ğŸ“˜ Preguntas de repaso", " 4 Supuestos de MCO Proceso Generador de Datos El modelo de regresiÃ³n lineal parte de la siguiente estructura: \\[ Y_i = X_i \\beta + \\epsilon_i \\] Donde: - \\(Y_i\\): variable dependiente (observaciÃ³n i) - \\(X_i\\): vector fila con los regresores de la observaciÃ³n i - \\(\\beta\\): vector de parÃ¡metros poblacionales - \\(\\epsilon_i\\): error poblacional (componentes no observables) - \\(i = 1, 2, ..., n\\) Esta formulaciÃ³n describe el proceso generador de datos (PGD), base para los supuestos del MCO. Tabla Resumen de Supuestos Supuesto NotaciÃ³n ImplicaciÃ³n principal S1. Linealidad en los parÃ¡metros \\(y_i = X_i \\beta + \\epsilon_i\\) El modelo es lineal en los parÃ¡metros S2. Exogeneidad estricta \\(\\mathbb{E}[\\epsilon_i \\mid X] = 0\\) No hay correlaciÃ³n entre regresores y error S3. Colinealidad imperfecta \\(\\text{Rango}(X) = K\\) No hay multicolinealidad perfecta; modelo identificable S4. Perturbaciones esfÃ©ricas \\(\\text{Var}(\\epsilon_i \\mid X) = \\sigma^2\\), \\(\\text{Cov}(\\epsilon_i, \\epsilon_j \\mid X) = 0\\) Homocedasticidad y no autocorrelaciÃ³n S5. Regresores no estocÃ¡sticos \\(X\\) es fija en repetidas muestras Simplifica demostraciones teÃ³ricas S6. Normalidad \\(\\epsilon \\mid X \\sim \\mathcal{N}(0, \\sigma^2 I)\\) Solo necesaria para inferencia exacta S1. Linealidad en los ParÃ¡metros El valor esperado de \\(y\\) estÃ¡ relacionado linealmente con los regresores: \\[ \\mathbb{E}[Y_i \\mid X_i] = X_i \\beta \\] Esto permite distintas formas funcionales (lineales en parÃ¡metros): Lineal: \\(y_i = \\beta_1 + \\beta_2 x_i + \\epsilon_i\\) Log-log: \\(\\log(y_i) = \\beta_1 + \\beta_2 \\log(x_i) + \\epsilon_i\\) Log-lineal: \\(\\log(y_i) = \\beta_1 + \\beta_2 x_i + \\epsilon_i\\) Lineal-log: \\(y_i = \\beta_1 + \\beta_2 \\log(x_i) + \\epsilon_i\\) CuadrÃ¡tico: \\(y_i = \\beta_1 + \\beta_2 x_i + \\beta_3 x_i^2 + \\epsilon_i\\) Interactuado: \\(y_i = \\beta_1 + \\beta_2 x_1 + \\beta_3 x_2 + \\beta_4(x_1 x_2) + \\epsilon_i\\) S2. Exogeneidad Estricta \\[ \\mathbb{E}[\\epsilon_i \\mid X] = 0 \\] Esto implica que no existe relaciÃ³n sistemÃ¡tica entre los regresores y el tÃ©rmino de error. Ejemplos: \\(\\mathbb{E}[u \\mid X = 1] = 0\\) \\(\\mathbb{E}[u \\mid X_2 = \\text{Mujer}] = 0\\) DemostraciÃ³n (Ley de la esperanza iterada): \\[ \\mathbb{E}[\\epsilon_i] = \\mathbb{E}\\left[ \\mathbb{E}[\\epsilon_i \\mid X] \\right] = \\mathbb{E}[0] = 0 \\] Equivalencia: Si \\(\\mathbb{E}[\\epsilon_i \\mid X] = 0\\), entonces: \\[ \\text{Cov}(\\epsilon_i, X_j) = 0 \\quad \\forall j \\] Pero quÃ© quiere decir? Una forma de pensar en esta definiciÃ³n es: Para cualquier valor de \\(X\\), el valor esperado de los residuos debe ser igual a cero E.g., \\(\\mathop{E}\\left[ u \\mid X=1 \\right]=0\\) and \\(\\mathop{E}\\left[ u \\mid X=100 \\right]=0\\) E.g., \\(\\mathop{E}\\left[ u \\mid X_2=\\text{Mujer} \\right]=0\\) and \\(\\mathop{E}\\left[ u \\mid X_2=\\text{Hombre} \\right]=0\\) Note: \\(\\mathop{E}\\left[ u \\mid X \\right]=0\\) es mÃ¡s restrictivo que \\(\\mathop{E}\\left[ u \\right]=0\\) Graficamenteâ€¦ Exogeneidad Estricta se Incumple, i.e., \\(\\mathop{E}\\left[ \\epsilon \\mid X \\right] \\neq 0\\) S3. Colinealidad Imperfecta \\[ \\text{Rango}(X) = K \\] Para que el modelo estÃ© identificado, debe cumplirse que el nÃºmero de observaciones sea mayor que el nÃºmero de regresores: \\(n &gt; K\\). Violaciones comunes: Regresor constante: \\(X_j = c\\) Dos variables idÃ©nticas: \\(X_j = X_k\\) CombinaciÃ³n lineal exacta: \\(X_3 = X_1 + X_2\\) Trampa de las variables binarias Ejemplo de matriz con rango 3: \\[ A = \\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 3 &amp; 5 &amp; 7 \\\\ 4 &amp; 6 &amp; 5 \\\\ \\end{bmatrix} \\quad \\Rightarrow \\text{Rango}(A) = 3 \\] Ejemplo de matriz con rango menor a 3: \\[ B = \\begin{bmatrix} 1 &amp; 3 &amp; 1 \\\\ 3 &amp; 8 &amp; 2 \\\\ 2 &amp; 9 &amp; 5 \\\\ \\end{bmatrix} \\quad \\Rightarrow \\text{Rango}(B) \\neq 3 \\] La tercera columna de \\(B\\) es combinaciÃ³n lineal de las otras dos: \\(C_3 = -2 \\cdot C_1 + C_2\\) Wooldridge (2003) aclara que este supuesto permite que los regresores estÃ©n correlacionados, siempre que no haya una relaciÃ³n lineal exacta entre ellos. S4. Perturbaciones EsfÃ©ricas Este supuesto se compone de dos condiciones: ğŸ”¹ Homocedasticidad \\[ \\text{Var}(\\epsilon_i \\mid X) = \\sigma^2 \\quad \\forall i \\] La dispersiÃ³n del tÃ©rmino de error es constante para todos los individuos. Esto significa que la varianza de los errores no depende de los regresores. ğŸ”¹ No autocorrelaciÃ³n \\[ \\text{Cov}(\\epsilon_i, \\epsilon_j \\mid X) = 0 \\quad \\text{para } i \\neq j \\] Los errores no estÃ¡n correlacionados entre sÃ­. Es especialmente relevante en series de tiempo, pero tambiÃ©n puede violarse en datos de corte transversal (e.g., por correlaciÃ³n espacial). ğŸ”¸ ImplicaciÃ³n conjunta Cuando se cumplen homocedasticidad y no autocorrelaciÃ³n: \\[ \\text{Var}(\\epsilon \\mid X) = \\sigma^2 I \\] La matriz de varianzas-covarianzas de los errores es escalar y diagonal, tambiÃ©n llamada matriz esfÃ©rica. ğŸ§  DerivaciÃ³n paso a paso {-} \\[ \\text{Var}(\\epsilon \\mid X) = \\mathbb{E}[\\epsilon \\epsilon&#39; \\mid X] - \\mathbb{E}[\\epsilon \\mid X] \\cdot \\mathbb{E}[\\epsilon&#39; \\mid X] \\] Por el supuesto de exogeneidad estricta (S2), sabemos que: \\[ \\mathbb{E}[\\epsilon \\mid X] = 0 \\quad \\Rightarrow \\quad \\text{Var}(\\epsilon \\mid X) = \\mathbb{E}[\\epsilon \\epsilon&#39; \\mid X] \\] Entonces, la matriz resultante es: \\[ \\text{Var}(\\epsilon \\mid X) = \\begin{bmatrix} \\mathbb{E}[\\epsilon_1^2 \\mid X] &amp; \\mathbb{E}[\\epsilon_1 \\epsilon_2 \\mid X] &amp; \\cdots &amp; \\mathbb{E}[\\epsilon_1 \\epsilon_n \\mid X] \\\\ \\mathbb{E}[\\epsilon_2 \\epsilon_1 \\mid X] &amp; \\mathbb{E}[\\epsilon_2^2 \\mid X] &amp; \\cdots &amp; \\mathbb{E}[\\epsilon_2 \\epsilon_n \\mid X] \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\mathbb{E}[\\epsilon_n \\epsilon_1 \\mid X] &amp; \\mathbb{E}[\\epsilon_n \\epsilon_2 \\mid X] &amp; \\cdots &amp; \\mathbb{E}[\\epsilon_n^2 \\mid X] \\end{bmatrix} \\] Aplicando los supuestos: \\(\\text{Var}(\\epsilon_i \\mid X) = \\sigma^2\\) \\(\\text{Cov}(\\epsilon_i, \\epsilon_j \\mid X) = 0\\) para \\(i \\neq j\\) \\[ \\Rightarrow \\text{Var}(\\epsilon \\mid X) = \\begin{bmatrix} \\sigma^2 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\sigma^2 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; \\sigma^2 \\end{bmatrix} = \\sigma^2 I \\] Este supuesto es necesario para garantizar la eficiencia del estimador MCO bajo los supuestos clÃ¡sicos (Teorema de Gauss-Markov). S5. Regresores No EstocÃ¡sticos Este supuesto establece que la matriz de regresores \\(X\\) no es aleatoria: sus valores permanecen fijos en repeticiones del experimento o entre muestras. \\[ X = \\text{constante} \\quad \\text{(no varÃ­a entre muestras)} \\] ğŸ”¹ Â¿QuÃ© significa? Aunque en la prÃ¡ctica \\(X\\) proviene de una muestra aleatoria, asumir que es no estocÃ¡stica permite tratarlo como fijo en la teorÃ­a. Esto implica que cualquier inferencia o estimaciÃ³n se condiciona sobre \\(X\\). âœ… Ventajas teÃ³ricas Simplifica la demostraciÃ³n de propiedades como insesgamiento y varianza mÃ­nima. Permite eliminar la distinciÃ³n entre: valor esperado condicional: \\(\\mathbb{E}[\\hat{\\beta} \\mid X]\\) y valor esperado incondicional: \\(\\mathbb{E}[\\hat{\\beta}]\\) âš ï¸ En la prÃ¡cticaâ€¦ Este supuesto rara vez se cumple literalmente, ya que \\(X\\) normalmente proviene de una muestra aleatoria. Sin embargo, es comÃºn en teorÃ­a clÃ¡sica porque: No afecta la validez del MCO si se asume que \\(X\\) es independiente de \\(\\epsilon\\). Se puede relajar en contextos de modelos mÃ¡s generales (paneles, variables instrumentales, etc.). En modelos con regresores estocÃ¡sticos, se requiere en cambio que \\(\\mathbb{E}[\\epsilon \\mid X] = 0\\), lo que recupera el supuesto de exogeneidad estricta (S2). S6. Normalidad del Error \\[ \\epsilon \\mid X \\sim \\mathcal{N}(0, \\sigma^2 I) \\] Este supuesto establece que los errores, condicionales a los regresores, siguen una distribuciÃ³n normal multivariada con media cero y matriz de varianza-covarianza esfÃ©rica \\(\\sigma^2 I\\). ğŸ¯ Â¿Para quÃ© sirve? Este supuesto no es necesario para que el estimador de MÃ­nimos Cuadrados Ordinarios (MCO) sea: Insesgado (S2 ya garantiza eso), Eficiente entre estimadores lineales insesgados (por el Teorema de Gauss-Markov). Sin embargo, sÃ­ es crucial para que se cumpla la distribuciÃ³n exacta de ciertos estadÃ­sticos en muestras pequeÃ±as. âœ… Aplicaciones de la normalidad: Validez de las pruebas t para significancia individual. Validez de las pruebas F para restricciones conjuntas. ConstrucciÃ³n exacta de intervalos de confianza para \\(\\beta\\). ğŸ§  Â¿QuÃ© pasa en muestras grandes? Gracias al Teorema Central del LÃ­mite y **La Ley de los Grandes NÃºmeros*, incluso si \\(\\epsilon\\) no es normal, el estimador \\(\\hat{\\beta}\\) tenderÃ¡ a seguir una distribuciÃ³n normal asintÃ³tica: \\[ \\hat{\\beta} \\overset{approx}{\\sim} \\mathcal{N}\\left(\\beta, \\sigma^2 (X&#39;X)^{-1}\\right) \\] Por eso, la normalidad puede relajarse si \\(n\\) es suficientemente grande. Glosario de SÃ­mbolos SÃ­mbolo Significado \\(Y_i\\) Variable dependiente \\(X_{ij}\\) Regresor j para observaciÃ³n i \\(\\beta_j\\) ParÃ¡metro poblacional \\(\\epsilon_i\\) Error poblacional \\(n\\) NÃºmero de observaciones \\(k\\) NÃºmero de regresores (sin constante) ğŸ“˜ Preguntas de repaso ğŸ“˜ 1. Conceptuales Defina brevemente los siguientes tÃ©rminos: EconometrÃ­a teÃ³rica EconometrÃ­a aplicada Â¿QuÃ© papel juega cada uno de los seis supuestos del modelo clÃ¡sico de regresiÃ³n lineal en garantizar las propiedades del estimador de MCO? ğŸ§® 2. ClasificaciÃ³n de modelos Clasifique los siguientes modelos como lineales en parÃ¡metros o no lineales: \\(y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\\) \\(\\log(y_i) = \\beta_0 + \\beta_1 \\log(x_i) + \\epsilon_i\\) \\(y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\epsilon_i\\) \\(y_i = \\frac{\\beta_0}{1 + e^{-\\beta_1 x_i}} + \\epsilon_i\\) \\(y_i = \\alpha + \\theta^{x_i} + \\epsilon_i\\) ğŸ“ 3. InterpretaciÃ³n de la pendiente Interprete el coeficiente \\(\\beta_1\\) en los siguientes modelos de regresiÃ³n lineal simple: \\(y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\\) \\(\\log(y_i) = \\beta_0 + \\beta_1 x_i + \\epsilon_i\\) \\(y_i = \\beta_0 + \\beta_1 \\log(x_i) + \\epsilon_i\\) \\(\\log(y_i) = \\beta_0 + \\beta_1 \\log(x_i) + \\epsilon_i\\) En cada caso, explique quÃ© representa un aumento marginal en \\(x_i\\), y si los efectos son absolutos, porcentuales o elÃ¡sticos. ğŸ¥ Recursos audiovisuales Â¿QuÃ© hacen los economistas? (Video 1) An Uneven Paying Field (Video 2) "],["regresiÃ³n-por-mÃ­nimos-cuadrados-ordinarios-mco.html", "5 RegresiÃ³n por MÃ­nimos Cuadrados Ordinarios (MCO) Modelo Â¿QuÃ© hace MCO? Â¿CÃ³mo se encuentra el vector \\(\\hat{\\beta}\\)? Â¿Es un mÃ­nimo? InterpretaciÃ³n en tÃ©rminos de contraparte muestral Supuestos clave empleados hasta acÃ¡ Diferencia entre la regresiÃ³n simple y la regresiÃ³n mÃºltiple ApÃ©ndice RegresiÃ³n mÃºltiple Estimador de MCO en R empleando matrices Estimador de MCO en Stata usando MATA Estimador de MCO en Python usando NumPy", " 5 RegresiÃ³n por MÃ­nimos Cuadrados Ordinarios (MCO) Modelo Antes de lanzarnos a minimizar errores, hagamos una pausa para ver cÃ³mo funciona esto con Ã¡lgebra lineal, que es el corazÃ³n de MCO. Tenemos una regresiÃ³n con n observaciones y k variables explicativas (incluyendo la constante). Entonces: \\(y\\): vector de resultados observados (\\(n \\times 1\\)) \\(X\\): matriz de variables explicativas (\\(n \\times k\\)) \\(\\beta\\): vector de coeficientes (\\(k \\times 1\\)) \\(\\varepsilon\\): vector de errores aleatorios (\\(n \\times 1\\)) El modelo lineal es: \\[ y = X\\beta + \\varepsilon \\] En notaciÃ³n matricial, esto se expresa como: \\[ \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix} = \\begin{bmatrix} 1 &amp; x_{11} &amp; \\cdots &amp; x_{1k} \\\\ 1 &amp; x_{21} &amp; \\cdots &amp; x_{2k} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 1 &amp; x_{n1} &amp; \\cdots &amp; x_{nk} \\end{bmatrix} \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_k \\end{bmatrix} + \\begin{bmatrix} \\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\vdots \\\\ \\varepsilon_n \\end{bmatrix} \\] Â¿QuÃ© hace MCO? La regresiÃ³n por MÃ­nimos Cuadrados Ordinarios (MCO) es una de las herramientas mÃ¡s fundamentales y utilizadas en econometrÃ­a. Su objetivo es simple pero poderoso: encontrar la mejor recta (o hiperplano) que se ajusta a los datos observados, minimizando los errores que cometemos al predecir. Â¿QuÃ© quiere decir â€œminimizar erroresâ€? Supongamos que tenemos una relaciÃ³n entre una variable dependiente \\(y\\) y un conjunto de variables explicativas \\(X\\). Queremos encontrar un vector de parÃ¡metros \\(\\beta\\) que explique esa relaciÃ³n lo mejor posible. El problema es que los datos reales no siguen perfectamente la recta que proponemos. Siempre hay un error o residuo que capta lo que no podemos explicar. Pero aunque no conocemos los verdaderos errores (denotados por \\(\\varepsilon\\)), sÃ­ podemos calcular cuÃ¡nto nos estamos equivocando si asumimos un valor hipotÃ©tico de \\(\\beta\\), al que llamaremos \\(\\tilde{\\beta}\\). El error estimado para cada observaciÃ³n \\(i\\) serÃ­a: \\[ \\tilde{\\epsilon}_i = y_i - X_i \\tilde{\\beta} \\] Y si elevamos estos errores al cuadrado (para que los negativos no se cancelen con los positivos) y los sumamos para todas las observaciones, obtenemos lo que llamamos la Suma de los Residuos al Cuadrado (SRC) es: \\[ SRC(\\tilde{\\beta}) = \\sum_{i=1}^n (y_i - X_i \\tilde{\\beta})^2 \\] En notaciÃ³n matricial: \\[ SRC(\\tilde{\\beta}) = (y - X\\tilde{\\beta})&#39;(y - X\\tilde{\\beta}) \\] La idea de MCO es encontrar un valor de \\(\\tilde{\\beta}\\) que minimice esta suma de residuos al cuadrado. En otras palabras, queremos encontrar el mejor ajuste posible a nuestros datos. En la siguiente grafica veremos cÃ³mo se ve esta funciÃ³n objetivo y cÃ³mo el valor Ã³ptimo de \\(\\tilde{\\beta}\\) se encuentra en el mÃ­nimo de la curva. Cuando tenemos mÃ¡s de una variable explicativa, MCO sigue buscando el punto que minimiza la suma de los residuos al cuadrado (SRC), pero ahora en una superficie 3D. En el grÃ¡fico a continuaciÃ³n, mostramos cÃ³mo la SRC varÃ­a con diferentes combinaciones de valores hipotÃ©ticos para \\(\\tilde{\\beta}_1\\) y \\(\\tilde{\\beta}_2\\). La superficie muestra los errores para cada combinaciÃ³n, y el mÃ­nimo (el punto mÃ¡s bajo) representa los valores estimados \\((\\hat{\\beta}_1, \\hat{\\beta}_2)\\). Â¿CÃ³mo se encuentra el vector \\(\\hat{\\beta}\\)? Queremos minimizar la SRC: \\[ \\hat{\\beta} = \\underset{\\tilde{\\beta}}{\\arg\\min} \\; (y - X\\tilde{\\beta})&#39;(y - X\\tilde{\\beta}) \\] Expandiendo: \\[ SRC(\\tilde{\\beta}) = y&#39;y - 2\\tilde{\\beta}&#39;X&#39;y + \\tilde{\\beta}&#39;X&#39;X\\tilde{\\beta} \\] Para minimizar esta funciÃ³n deerivamos con respecto a \\(\\tilde{\\beta}\\): \\[ \\frac{\\partial SRC}{\\partial \\tilde{\\beta}} = -2X&#39;y + 2X&#39;X\\tilde{\\beta} = 0 \\] Reordenando, llegamos a las Ecuaciones normales de MCO: \\[ X&#39;X \\hat{\\beta} = X&#39;y \\] Si la matriz \\(X&#39;X\\) es invertible (lo que requiere, por ejemplo, que no haya multicolinealidad perfecta), entonces podemos despejar \\(\\hat{\\beta}\\) directamente \\[ \\hat{\\beta} = (X&#39;X)^{-1}X&#39;y \\] Â¿Es un mÃ­nimo? SÃ­, y lo podemos verificar con la segunda derivada (matriz Hessiana) es: \\[ \\frac{\\partial^2 SRC}{\\partial \\tilde{\\beta} \\partial \\tilde{\\beta}&#39;} = 2X&#39;X \\] Como \\(X&#39;X\\) es semidefinida positiva (o positiva definida si no hay colinealidad), eso nos garantiza que el punto que encontramos con la primera derivada es en efecto un mÃ­nimo. ğŸ§  Recordatorio: Para funciones cuadrÃ¡ticas, si la matriz que acompaÃ±a al tÃ©rmino cuadrÃ¡tico (en este caso \\(X&#39;X\\)) es positiva definida, entonces el punto crÃ­tico que encontramos resolviendo la condiciÃ³n de primer orden es un mÃ­nimo global. ğŸ§  Â¿QuÃ© significa que una matriz sea semidefinida positiva? Una matriz simÃ©trica \\(A\\) es: Positiva semidefinida (PSD) si para cualquier vector no nulo \\(z\\), se cumple: \\[ z&#39;Az \\geq 0 \\] Positiva definida (PD) si para cualquier vector no nulo \\(z\\), se cumple: \\[ z&#39;Az &gt; 0 \\] En el contexto de mÃ­nimos cuadrados, la matriz \\(X&#39;X\\) es siempre simÃ©trica y al menos semidefinida positiva. SerÃ¡ positiva definida si las columnas de \\(X\\) no tienen colinealidad perfecta (es decir, no hay combinaciÃ³n lineal exacta entre las variables). Â¿Y por quÃ© importa esto? Porque si una funciÃ³n cuadrÃ¡tica tiene la forma: \\[ f(\\beta) = \\beta&#39;A\\beta + b&#39;\\beta + c \\] entonces \\(f(\\beta)\\) tiene un mÃ­nimo en el punto donde su derivada es cero si \\(A\\) es positiva definida (o semidefinida positiva en algunos casos). Esto es justo lo que ocurre en MCO. ğŸ“Œ En otras palabras: el hecho de que \\(X&#39;X\\) sea positiva definida es lo que garantiza que el estimador \\(\\hat{\\beta}\\) minimiza la suma de los residuos al cuadrado, y no la maximiza o genera un punto de silla. InterpretaciÃ³n en tÃ©rminos de contraparte muestral Podemos escribir: \\[ \\hat{\\beta} = \\left( \\frac{X&#39;X}{n} \\right)^{-1} \\left( \\frac{X&#39;y}{n} \\right) \\] Esto sugiere que estamos usando promedios muestrales. Bajo ciertas condiciones: \\[ \\hat{\\beta} \\to E[X&#39;X]^{-1}E[X&#39;y] \\] Supuestos clave empleados hasta acÃ¡ S1: Linealidad en los parÃ¡metros S2: \\(E[\\varepsilon_i \\mid X_i] = 0\\) S3: \\(X&#39;X\\) invertible (no colinealidad perfecta) Diferencia entre la regresiÃ³n simple y la regresiÃ³n mÃºltiple Vamos a ver diferentes casos de regresiÃ³n, desde la mÃ¡s simple hasta la mÃ¡s compleja, para entender la anatomÃ­a de la regresiÃ³n y cÃ³mo MCO se adapta a cada uno. RegresiÃ³n sin variables explicativas En este caso, el modelo es simplemente: \\[ y = \\beta + \\varepsilon \\] AquÃ­, \\(\\beta\\) es el Ãºnico parÃ¡metro a estimar. El estimador de MCO es simplemente el promedio de \\(y\\): \\[ \\hat{\\beta} = \\frac{1}{n} \\sum_{i=1}^{n} y_i \\] RegresiÃ³n simple con una variable explicativa En este caso, el modelo es: \\[ y = \\beta_0 + \\beta_1 x + \\varepsilon \\] AquÃ­, tenemos dos parÃ¡metros a estimar: \\(\\beta_0\\) (intercepto) y \\(\\beta_1\\) (pendiente). El estimador de MCO se calcula como: \\[ \\hat{\\beta} = (X&#39;X)^{-1}X&#39;y \\] ğŸ“ Pausa Demostrar que el estimador de MCO para \\(\\beta_0\\) y \\(\\beta_1\\) en la regresiÃ³n simple son: \\[ \\hat{\\beta}_1 = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2} \\] \\[ \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x} \\] Debes partir de la formulaciÃ³n matricial de MCO y aplicar Ã¡lgebra paso a paso. ğŸ’ª Recuerda las siguientes propiedades de la sumatoria Promedio y suma Si \\[ \\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} X_i \\quad \\Rightarrow \\quad n \\bar{X} = \\sum_{i=1}^{n} X_i \\] Suma de cuadrados centrados \\[ \\sum_{i=1}^{n} (X_i - \\bar{X})^2 = \\sum_{i=1}^{n} X_i^2 - n \\bar{X}^2 \\] DemostraciÃ³n: \\[ \\sum_{i=1}^{n} (X_i - \\bar{X})^2 = \\sum X_i^2 - 2 \\bar{X} \\sum X_i + n \\bar{X}^2 \\] \\[ = \\sum X_i^2 - 2n \\bar{X}^2 + n \\bar{X}^2 = \\sum X_i^2 - n \\bar{X}^2 \\] Covarianza muestral \\[ \\sum_{i=1}^{n} (X_i - \\bar{X})(Y_i - \\bar{Y}) = \\sum X_i (Y_i - \\bar{Y}) - \\bar{X} \\sum (Y_i - \\bar{Y}) \\] \\[ = \\sum X_i Y_i - \\bar{Y} \\sum X_i - \\bar{X} \\cdot 0 = \\sum X_i Y_i - n \\bar{Y} \\bar{X} = \\sum X_i (Y_i - \\bar{Y}) \\] RegresiÃ³n mÃºltiple con mÃºltiples variables explicativas En este caso, el modelo es: \\[ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_k x_k + \\varepsilon \\] AquÃ­, tenemos \\(k+1\\) parÃ¡metros a estimar. El estimador de MCO sigue siendo: \\[ \\hat{\\beta} = (X&#39;X)^{-1}X&#39;y \\] En el siguiente capÃ­tulo entenederemos mejor la anatomÃ­a de la regresiÃ³n mÃºltiple para entender perfectamente Â¿cuÃ¡l es la diferencia entre la regresiÃ³n simple y la mÃºltiple? ApÃ©ndice Ãlgebraâ€¦ mucha Ã¡lgebra Partimos del modelo de regresiÃ³n simple en forma matricial: \\[ y = X\\beta + \\varepsilon \\] Donde: \\(y\\) es un vector columna de dimensiÃ³n \\(n \\times 1\\), \\(X\\) es una matriz \\(n \\times 2\\), con una columna de unos y una columna con la variable explicativa \\(X_1\\), es decir: \\(X = [1 \\;\\; X_1]\\), \\(\\varepsilon\\) es el vector de perturbaciones, \\(\\beta\\) es un vector \\(2 \\times 1\\) con los parÃ¡metros a estimar. El estimador de MCO es: \\[ \\begin{bmatrix} \\hat{\\beta}_0 \\\\ \\hat{\\beta}_1 \\end{bmatrix} = \\underbrace{(X&#39;X)^{-1}}_a \\cdot \\underbrace{X&#39;y}_b \\] CÃ¡lculo de \\(X&#39;X\\) \\[ X&#39;X = \\begin{bmatrix} 1 &amp; 1 &amp; \\cdots &amp; 1 \\\\ X_{11} &amp; X_{12} &amp; \\cdots &amp; X_{1n} \\end{bmatrix} \\begin{bmatrix} 1 &amp; X_{11} \\\\ 1 &amp; X_{12} \\\\ \\vdots &amp; \\vdots \\\\ 1 &amp; X_{1n} \\end{bmatrix} = \\begin{bmatrix} n &amp; \\sum X_{1i} \\\\ \\sum X_{1i} &amp; \\sum X_{1i}^2 \\end{bmatrix} \\] Su inversa es: \\[ (X&#39;X)^{-1} = \\frac{1}{n \\sum X_{1i}^2 - (\\sum X_{1i})^2} \\begin{bmatrix} \\sum X_{1i}^2 &amp; -\\sum X_{1i} \\\\ -\\sum X_{1i} &amp; n \\end{bmatrix} \\] 5.0.1 CÃ¡lculo de \\(X&#39;y\\) \\[ X&#39;y = \\begin{bmatrix} 1 &amp; 1 &amp; \\cdots &amp; 1 \\\\ X_{11} &amp; X_{12} &amp; \\cdots &amp; X_{1n} \\end{bmatrix} \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix} = \\begin{bmatrix} \\sum y_i \\\\ \\sum X_{1i} y_i \\end{bmatrix} \\] Producto final Sustituyendo en la fÃ³rmula del estimador: \\[ \\begin{aligned} \\begin{bmatrix} \\hat{\\beta}_0 \\\\ \\hat{\\beta}_1 \\end{bmatrix} &amp;= \\frac{1}{n \\sum X_{1i}^2 - (\\sum X_{1i})^2} \\begin{bmatrix} \\sum X_{1i}^2 &amp; -\\sum X_{1i} \\\\ -\\sum X_{1i} &amp; n \\end{bmatrix} \\begin{bmatrix} \\sum y_i \\\\ \\sum X_{1i} y_i \\end{bmatrix} \\\\ &amp;= \\frac{1}{n \\sum X_{1i}^2 - (\\sum X_{1i})^2} \\begin{bmatrix} \\sum X_{1i}^2 \\sum y_i - \\sum X_{1i} \\sum X_{1i} y_i \\\\ - \\sum X_{1i} \\sum y_i + n \\sum X_{1i} y_i \\end{bmatrix} \\end{aligned} \\] Estimador de \\(\\hat{\\beta}_1\\) \\[ \\begin{aligned} \\hat{\\beta}_1 &amp;= \\frac{-\\sum X_{1i} \\sum y_i + n \\sum X_{1i} y_i}{n \\sum X_{1i}^2 - (\\sum X_{1i})^2} \\\\ &amp;= \\frac{\\sum (X_{1i} - \\bar{X}_1)(y_i - \\bar{y})}{\\sum (X_{1i} - \\bar{X}_1)^2} \\end{aligned} \\] Estimador de \\(\\hat{\\beta}_0\\) \\[ \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{X}_1 \\] RegresiÃ³n mÃºltiple La matriz \\(X&#39;X\\) contiene la covarianza entre las variables explicativas. Si las variables no estÃ¡n correlacionadas, no se necesita regresiÃ³n mÃºltiple. Asumimos: \\(\\sum X_{ji} = 0\\) para todas las variables (centradas en cero) \\(\\sum X_{ji} X_{hi} = 0\\) para todo \\(j \\ne h\\) (no correlaciÃ³n entre regresores) Entonces \\(X&#39;X\\) se convierte en: \\[ X&#39;X = \\begin{bmatrix} n &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\sum X_{1i}^2 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; \\sum X_{ki}^2 \\end{bmatrix} \\] Y su inversa: \\[ (X&#39;X)^{-1} = \\begin{bmatrix} 1/n &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; 1/\\sum X_{1i}^2 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; 1/\\sum X_{ki}^2 \\end{bmatrix} \\] El estimador de MCO en este caso es: \\[ \\hat{\\beta} = \\begin{bmatrix} \\frac{\\sum y}{n} \\\\ \\frac{\\sum X_1 y}{\\sum X_1^2} \\\\ \\vdots \\\\ \\frac{\\sum X_k y}{\\sum X_k^2} \\end{bmatrix} \\] ConclusiÃ³n: si las variables explicativas estÃ¡n centradas y no estÃ¡n correlacionadas entre sÃ­, entonces el estimador de MCO para cada coeficiente coincide con el estimador de regresiÃ³n simple. Solo cuando hay correlaciÃ³n entre regresores es necesaria la regresiÃ³n mÃºltiple para obtener efectos parciales. Estimador de MCO en R empleando matrices # Simular datos set.seed(123) n &lt;- 100 x &lt;- runif(n, 0, 10) epsilon &lt;- rnorm(n, 0, 1.5) y &lt;- 2 + 0.7 * x + epsilon # Crear matrices X &lt;- cbind(1, x) # matriz de diseÃ±o con constante Y &lt;- matrix(y, ncol = 1) # Estimador de MCO XtX_inv &lt;- solve(t(X) %*% X) XtY &lt;- t(X) %*% Y b_hat &lt;- XtX_inv %*% XtY # Valores ajustados y residuos Y_hat &lt;- X %*% b_hat e &lt;- Y - Y_hat # Varianza del error y errores estÃ¡ndar n &lt;- nrow(X) k &lt;- ncol(X) s2 &lt;- sum(e^2) / (n - k) var_cov_matrix &lt;- s2 * XtX_inv se &lt;- sqrt(diag(var_cov_matrix)) # Resultados b_hat #&gt; [,1] #&gt; 1.9865602 #&gt; x 0.6865253 se #&gt; x #&gt; 0.29408249 0.05127318 Estimador de MCO en Stata usando MATA clear all set more off sysuse auto, clear * Generamos la constante gen cons = 1 * Entramos a MATA mata // Creamos las matrices X y Y st_view(Y=., ., &quot;price&quot;) st_view(X=., ., (&quot;cons&quot;, &quot;weight&quot;)) n = rows(X) k = cols(X) df = n - k // Estimador de MCO b = invsym(X&#39;X)*X&#39;Y // Valores ajustados y residuos Y_hat = X*b e = Y - Y_hat // Suma de residuos al cuadrado SRC = e&#39;e // Varianza del error s2 = SRC / df s = sqrt(s2) // Matriz de varianzas y covarianzas V = s2 * invsym(X&#39;X) se = sqrt(diagonal(V)) // Resultados b se end * ComparaciÃ³n con comando base reg price weight Estimador de MCO en Python usando NumPy # Estimador de MCO en Python usando matrices (numpy) import numpy as np # Simular datos np.random.seed(123) n = 100 x = np.random.uniform(0, 10, size=n) epsilon = np.random.normal(0, 1.5, size=n) y = 2 + 0.7 * x + epsilon # Crear matriz X con constante X = np.column_stack((np.ones(n), x)) # n x 2 Y = y.reshape(-1, 1) # n x 1 # Estimador MCO: beta_hat = (X&#39;X)^(-1) X&#39;Y XtX_inv = np.linalg.inv(X.T @ X) XtY = X.T @ Y beta_hat = XtX_inv @ XtY # Valores ajustados y residuos Y_hat = X @ beta_hat e = Y - Y_hat # Varianza del error k = X.shape[1] s2 = (e.T @ e) / (n - k) var_cov_matrix = s2[0, 0] * XtX_inv se = np.sqrt(np.diag(var_cov_matrix)) # Resultados print(&quot;Coeficientes (beta_hat):&quot;) print(beta_hat.flatten()) print(&quot;\\nErrores estÃ¡ndar:&quot;) print(se) import matplotlib.pyplot as plt plt.scatter(x, y, label=&#39;Datos&#39;, alpha=0.6) plt.plot(x, Y_hat, color=&#39;red&#39;, label=&#39;Ajuste MCO&#39;) plt.xlabel(&#39;x&#39;) plt.ylabel(&#39;y&#39;) plt.title(&#39;RegresiÃ³n lineal por MCO&#39;) plt.legend() plt.grid(True) plt.show() ğŸ“˜ Preguntas de repaso A continuaciÃ³n encontrarÃ¡s algunas preguntas conceptuales y computacionales para reforzar lo aprendido sobre el estimador de MÃ­nimos Cuadrados Ordinarios (MCO). Puedes intentar resolverlas en papel o programando en R, Stata o Python. âœï¸ InterpretaciÃ³n de coeficientes Suponga que ha estimado la siguiente regresiÃ³n: \\[ \\widehat{\\text{salario}} = \\beta_0 + \\beta_1 \\cdot \\text{educaciÃ³n} \\] donde: salario = salario por hora (en pesos) educaciÃ³n = aÃ±os de educaciÃ³n completados Para cada uno de los siguientes coeficientes estimados, interprete el resultado en tÃ©rminos econÃ³micos: \\(\\hat{\\beta}_1 = 2000\\) \\(\\hat{\\beta}_1 = 0\\) \\(\\hat{\\beta}_1 = -500\\) \\(\\hat{\\beta}_1 = 0.15\\) \\(\\hat{\\beta}_1 = 10\\) \\(\\hat{\\beta}_0 = 2500\\) ğŸ§  Ãlgebra y transformaciones Â¿QuÃ© ocurre con el estimador de MCO si multiplicamos la variable dependiente por una constante \\(c\\)? Demuestre usando la expresiÃ³n matricial \\(\\hat{\\beta} = (X&#39;X)^{-1} X&#39;y\\). Â¿QuÃ© ocurre si multiplicamos una de las variables explicativas por una constante \\(c\\)? Â¿QuÃ© sucede con el coeficiente estimado y con el resto de los coeficientes? Si a la variable dependiente se le suma una constante, Â¿quÃ© ocurre con los estimadores? Si todas las variables explicativas tienen media cero, Â¿cuÃ¡l es el valor estimado del intercepto? Â¿Por quÃ©? ğŸ’» Preguntas de programaciÃ³n Implemente una funciÃ³n en R que reciba dos vectores \\(x\\) e \\(y\\), y devuelva \\(\\hat{\\beta}_0\\), \\(\\hat{\\beta}_1\\) y los errores estÃ¡ndar, usando solo Ã¡lgebra matricial. En Stata, estime una regresiÃ³n por MCO con reg y luego reproduzca todos los pasos usando mata. Compare los resultados y verifique que los residuos sean iguales. Escriba una funciÃ³n en Python que tome \\(X\\) e \\(Y\\) como matrices de NumPy y devuelva: \\(\\hat{\\beta}\\), la matriz de varianza-covarianza, los errores estÃ¡ndar, y el \\(R^2\\). ğŸ“š Para reflexionar Â¿Por quÃ© es importante que la matriz \\(X&#39;X\\) sea positiva definida? Â¿QuÃ© ocurre si no lo es? Â¿QuÃ© implicaciones tiene usar variables altamente correlacionadas en un modelo de regresiÃ³n mÃºltiple? Â¿CÃ³mo lo podrÃ­as detectar? "],["anatomÃ­a-de-la-regresiÃ³n-mÃºltiple.html", "6 AnatomÃ­a de la RegresiÃ³n MÃºltiple Matrices de ProyecciÃ³n Teorema de Frisch-Waugh-Lovell (FWL) ğŸ¯ NotaciÃ³n y MotivaciÃ³n â“Â¿CuÃ¡l es el problema que resuelve el FWL? âœ¨ Paso a paso del Teorema de Frisch-Waugh-Lovell (FWL) ğŸ§© Paso 1: Proyectar \\(y\\) sobre \\(X_s\\) y obtener los residuos ğŸ§© Paso 2: Proyectar \\(X_r\\) sobre \\(X_s\\) y obtener los residuos ğŸ§© Paso 3: Regresar \\(\\tilde{y}\\) sobre \\(\\tilde{X}_r\\) âœ… InterpretaciÃ³n final ğŸ“¦ ConclusiÃ³n DemostraciÃ³n Formal ğŸ§ª Ejemplo prÃ¡ctico del Teorema de Frisch-Waugh-Lovell en Stata, R y Python", " 6 AnatomÃ­a de la RegresiÃ³n MÃºltiple Matrices de ProyecciÃ³n Supongamos que estamos estimando una regresiÃ³n lineal mÃºltiple del tipo: \\[ y = X\\beta + \\varepsilon \\] donde: \\(y \\in \\mathbb{R}^n\\) es el vector de la variable dependiente \\(X \\in \\mathbb{R}^{n \\times k}\\) es la matriz de regresores de rango completo \\(\\beta \\in \\mathbb{R}^k\\) es el vector de parÃ¡metros La estimaciÃ³n de mÃ­nimos cuadrados se basa en proyectar el vector \\(y\\) sobre el espacio columna de \\(X\\), denotado \\(\\mathcal{C}(X)\\). Esta proyecciÃ³n busca encontrar el punto mÃ¡s cercano en ese subespacio a \\(y\\), minimizando la suma de los residuos al cuadrado. Esta proyecciÃ³n se logra mediante la matriz de proyecciÃ³n: \\[ \\mathbf{P}_X = X(X&#39;X)^{-1}X&#39; \\] Y su complemento ortogonal (que proyecta sobre el espacio ortogonal a \\(\\mathcal{C}(X)\\)) es: \\[ \\mathbf{M}_X = I - \\mathbf{P}_X \\] \\[ \\hat{y} = \\mathbf{P}_X y \\quad \\text{y} \\quad \\hat{\\varepsilon} = \\mathbf{M}_X y \\] ğŸ” IntuiciÃ³n geomÃ©trica \\(y\\): vector observado \\(\\hat{y} = \\mathbf{P}_X y\\): predicciÃ³n, o â€œsombraâ€ de \\(y\\) sobre el plano generado por \\(X\\) \\(\\hat{\\varepsilon} = y - \\hat{y}\\): residuo, perpendicular al plano El estimador de MCO se obtiene al minimizar: \\[ \\min_{\\beta} (y - X\\beta)&#39;(y - X\\beta) \\] La soluciÃ³n es: \\[ \\hat{\\beta} = (X&#39;X)^{-1}X&#39;y \\] ğŸ“ Propiedades algebraicas clave Propiedad \\(\\mathbf{P}_X\\) \\(\\mathbf{M}_X\\) SimetrÃ­a \\(\\mathbf{P}_X&#39; = \\mathbf{P}_X\\) \\(\\mathbf{M}_X&#39; = \\mathbf{M}_X\\) Idempotencia \\(\\mathbf{P}_X^2 = \\mathbf{P}_X\\) \\(\\mathbf{M}_X^2 = \\mathbf{M}_X\\) Ortogonalidad \\(\\mathbf{P}_X \\mathbf{M}_X = 0\\) \\(\\hat{y} \\perp \\hat{\\varepsilon}\\) Estas propiedades nos permitirÃ¡n probar resultados como: ortogonalidad entre predicciÃ³n y residuos, descomposiciÃ³n de la varianza total, y derivar el estimador de varianza del error. âœ¨ VisualizaciÃ³n tridimensional de la proyecciÃ³n La figura muestra cÃ³mo el vector \\(y\\) se proyecta sobre el espacio generado por las columnas de \\(X\\). La diferencia \\(y - P_X y\\) es ortogonal al plano y corresponde a los residuos. Teorema de Frisch-Waugh-Lovell (FWL) El Teorema de Frisch-Waugh-Lovell (FWL) es uno de los resultados mÃ¡s importantes de la econometrÃ­a, porque nos permite entender quÃ© significa realmente el coeficiente de una variable en una regresiÃ³n mÃºltiple. FWL nos dice que, si estamos interesados en el efecto de una variable \\(X_r\\) sobre \\(y\\), controlando por otras variables \\(X_s\\), podemos obtener exactamente el mismo coeficiente de una manera alternativa, usando proyecciones. Esto es extremadamente Ãºtil porque: Da una interpretaciÃ³n clara del coeficiente como un efecto â€œdepuradoâ€. Permite construir regresiones parciales paso a paso. Ayuda a entender la mecÃ¡nica interna de los modelos de regresiÃ³n mÃºltiples. ğŸ¯ NotaciÃ³n y MotivaciÃ³n Considera el siguiente modelo de regresiÃ³n lineal: \\[ y = X_r \\beta_r + X_s \\beta_s + \\varepsilon \\] donde: \\(y \\in \\mathbb{R}^{n \\times 1}\\) es la variable dependiente (por ejemplo, salario). \\(X_r \\in \\mathbb{R}^{n \\times 1}\\) es la variable de interÃ©s (por ejemplo, educaciÃ³n). \\(X_s \\in \\mathbb{R}^{n \\times k}\\) es el conjunto de variables de control (por ejemplo, experiencia, edad, gÃ©nero, etc.). \\(\\varepsilon \\in \\mathbb{R}^{n \\times 1}\\) es el error. Esta forma tambiÃ©n puede escribirse mÃ¡s compactamente asÃ­: \\[ y = [X_r \\quad X_s] \\begin{bmatrix} \\beta_r \\\\ \\beta_s \\end{bmatrix} + \\varepsilon \\] Donde la matriz \\(X = [X_r \\quad X_s] \\in \\mathbb{R}^{n \\times (k+1)}\\) contiene todas las variables explicativas, y el vector de parÃ¡metros \\(\\beta \\in \\mathbb{R}^{k+1}\\) contiene sus coeficientes respectivos. â“Â¿CuÃ¡l es el problema que resuelve el FWL? Imagina que quieres saber el efecto de la educaciÃ³n sobre el salario, pero sabes que hay muchas otras variables (edad, experiencia, gÃ©nero, etc.) que tambiÃ©n afectan el salario. Entonces haces una regresiÃ³n mÃºltiple, y obtienes el coeficiente de educaciÃ³n controlando por todo lo demÃ¡s. El teorema FWL te dice: â€œEse coeficiente se puede obtener en tres pasos, sin necesidad de correr la regresiÃ³n completa.â€ Y lo mejor: el resultado serÃ¡ exactamente igual. Eso es lo que veremos a continuaciÃ³n en el paso a paso. âœ¨ Paso a paso del Teorema de Frisch-Waugh-Lovell (FWL) El Teorema de FWL nos dice que podemos obtener el coeficiente \\(\\beta_r\\) de una regresiÃ³n mÃºltiple: \\[ y = X_r \\beta_r + X_s \\beta_s + \\varepsilon \\] realizando tres regresiones parciales, sin necesidad de incluir todos los regresores al mismo tiempo. A continuaciÃ³n explicamos cada paso con todo el detalle necesario. ğŸ§© Paso 1: Proyectar \\(y\\) sobre \\(X_s\\) y obtener los residuos Primero, eliminamos de \\(y\\) la parte que puede ser explicada por los controles \\(X_s\\). Esto se hace regresando \\(y\\) sobre \\(X_s\\) y guardando los residuos. Es decir: \\[ \\tilde{y} = M_s y = (I - P_s)y \\] donde: \\(P_s = X_s (X_s&#39;X_s)^{-1} X_s&#39;\\) es la matriz de proyecciÃ³n sobre el espacio generado por los controles \\(X_s\\). \\(M_s = I - P_s\\) es la matriz de residuos o complemento ortogonal: elimina todo lo que estÃ© explicado por \\(X_s\\). ğŸ” Â¿QuÃ© significa esto? Estamos â€œlimpiandoâ€ a \\(y\\), quitÃ¡ndole la parte que se debe a los controles. El resultado \\(\\tilde{y}\\) representa la parte de \\(y\\) que es ortogonal a los controles. ğŸ“ InterpretaciÃ³n geomÃ©trica: proyectamos \\(y\\) sobre el subespacio generado por \\(X_s\\) y nos quedamos con la componente perpendicular. ğŸ§© Paso 2: Proyectar \\(X_r\\) sobre \\(X_s\\) y obtener los residuos Ahora hacemos lo mismo con la variable de interÃ©s \\(X_r\\): le quitamos la parte que puede explicarse con los controles. \\[ \\tilde{X}_r = M_s X_r = (I - P_s)X_r \\] ğŸ” Â¿QuÃ© significa esto? Estamos â€œdepurandoâ€ a \\(X_r\\), eliminando cualquier relaciÃ³n lineal con los controles. \\(\\tilde{X}_r\\) es la parte de \\(X_r\\) que no se puede predecir con \\(X_s\\). ğŸ“ InterpretaciÃ³n geomÃ©trica: proyectamos \\(X_r\\) sobre \\(\\mathcal{C}(X_s)\\) y guardamos el residuo, que es ortogonal a ese subespacio. ğŸ§© Paso 3: Regresar \\(\\tilde{y}\\) sobre \\(\\tilde{X}_r\\) Finalmente, estimamos el coeficiente que relaciona las dos variables â€œlimpiasâ€ o depuradas: \\[ \\tilde{y} = \\tilde{X}_r \\cdot \\gamma + u \\] El estimador de MCO de esta regresiÃ³n es: \\[ \\hat{\\gamma} = (\\tilde{X}_r&#39;\\tilde{X}_r)^{-1} \\tilde{X}_r&#39; \\tilde{y} \\] ğŸ”” Â¡Sorpresa! Este estimador es exactamente igual a: \\[ \\hat{\\beta}_r \\quad \\text{(el coeficiente de \\( X_r \\) en la regresiÃ³n completa)} \\] âœ… InterpretaciÃ³n final Este resultado nos dice que: El efecto de \\(X_r\\) sobre \\(y\\), controlando por \\(X_s\\), es igual al efecto de la parte de \\(X_r\\) que no se relaciona con \\(X_s\\) sobre la parte de \\(y\\) que tampoco se relaciona con \\(X_s\\). En palabras simples: es una regresiÃ³n entre los residuos. ğŸ“¦ ConclusiÃ³n Este teorema tiene implicaciones profundas: Muestra que controlar por variables equivale a quitarles su efecto tanto a la variable explicativa como a la dependiente, y luego ver cÃ³mo se relacionan esas partes â€œlimpiasâ€. Es la base para entender tÃ©cnicas como control por regresiÃ³n parcial, y tambiÃ©n para desarrollar intuiciones sobre variables instrumentales, efectos marginales y mÃ¡s. DemostraciÃ³n Formal Usando Ã¡lgebra matricial: \\[ \\hat{\\beta}_r = (X_r&#39;M_s X_r)^{-1} X_r&#39;M_s y \\] Esto se deduce de la forma general del estimador de MCO: \\[ \\hat{\\beta} = (X&#39;X)^{-1}X&#39;y \\] pero aplicado al modelo reducido, en el que \\(y\\) y \\(X_r\\) han sido â€œlimpiadosâ€ de \\(X_s\\). ğŸ§ª Ejemplo prÃ¡ctico del Teorema de Frisch-Waugh-Lovell en Stata, R y Python A continuaciÃ³n presentamos una simulaciÃ³n sencilla para ilustrar el Teorema de Frisch-Waugh-Lovell (FWL) y mostrar cÃ³mo cambia el coeficiente estimado de una variable de interÃ©s dependiendo de la correlaciÃ³n con los controles. TambiÃ©n implementamos paso a paso la construcciÃ³n del estimador usando residuos. 6.0.1 ğŸ”µ CÃ³digo en Stata clear set seed 6981473 set obs 1000 * Variable de interÃ©s Xr gen altura = runiform()*30+150 replace altura= round(altura) label var altura &quot;altura&quot; * Crear Xs con correlaciÃ³n positiva con Xr gen ingreso_hh = rnormal() + 5*altura * Variable dependiente Y gen salario = 1 + 2*altura + 5*ingreso_hh + rnormal() * CASO 1: cov(altura, ingreso_hh)&gt;0 reg salario altura reg salario altura ingreso_hh * CASO 2: cov(altura, ingreso_hh)=0 gen ingreso_hh2 = rnormal() gen salario2 = 1 + 2*altura + 5*ingreso_hh2 + rnormal() reg salario2 altura reg salario2 altura ingreso_hh2 * CASO 3: cov(salario, ingreso_hh)=0 gen ingreso_hh3 = rnormal() + 3*altura gen salario3 = 1 + 2*altura + 0*ingreso_hh3 + rnormal() reg salario3 altura reg salario3 altura ingreso_hh3 * Teorema de FWL paso a paso reg salario ingreso_hh predict My, res reg altura ingreso_hh predict MXr, res reg salario altura ingreso_hh reg My MXr * Comparar coeficientes di _b[MXr] _b[altura] 6.0.2 ğŸŸ¢ CÃ³digo en R library(tidyverse) set.seed(6981473) n &lt;- 1000 # Variable de interÃ©s altura &lt;- runif(n, 0, 30) + 150 altura &lt;- round(altura) # Control correlacionado con altura ingreso_hh &lt;- rnorm(n) + 5 * altura # Variable dependiente salario &lt;- 1 + 2 * altura + 5 * ingreso_hh + rnorm(n) df &lt;- tibble(altura, ingreso_hh, salario) # CASO 1: cov(altura, ingreso_hh)&gt;0 summary(lm(salario ~ altura, data = df)) summary(lm(salario ~ altura + ingreso_hh, data = df)) # CASO 2: cov(altura, ingreso_hh)=0 df$ingreso_hh2 &lt;- rnorm(n) df$salario2 &lt;- 1 + 2 * df$altura + 5 * df$ingreso_hh2 + rnorm(n) summary(lm(salario2 ~ altura, data = df)) summary(lm(salario2 ~ altura + ingreso_hh2, data = df)) # CASO 3: cov(y, ingreso_hh)=0 df$ingreso_hh3 &lt;- rnorm(n) + 3 * df$altura df$salario3 &lt;- 1 + 2 * df$altura + 0 * df$ingreso_hh3 + rnorm(n) summary(lm(salario3 ~ altura, data = df)) summary(lm(salario3 ~ altura + ingreso_hh3, data = df)) # FWL paso a paso modelo_y &lt;- lm(salario ~ ingreso_hh, data = df) df$My &lt;- resid(modelo_y) modelo_xr &lt;- lm(altura ~ ingreso_hh, data = df) df$MXr &lt;- resid(modelo_xr) summary(lm(My ~ MXr, data = df)) # Igual al coef. de altura summary(lm(salario ~ altura + ingreso_hh, data = df)) # VerificaciÃ³n # Comparar coeficientes cat(&quot;Coeficiente de altura (residuos):&quot;, coef(lm(My ~ MXr, data = df))[&quot;MXr&quot;], &quot;\\n&quot;) cat(&quot;Coeficiente de altura (modelo completo):&quot;, coef(lm(salario ~ altura + ingreso_hh, data = df))[&quot;altura&quot;], &quot;\\n&quot;) 6.0.3 ğŸ”´ CÃ³digo en Python import numpy as np import pandas as pd import statsmodels.api as sm import statsmodels.formula.api as smf np.random.seed(6981473) n = 1000 # Variable de interÃ©s altura = np.round(np.random.uniform(0, 30, n) + 150) # Control correlacionado ingreso_hh = np.random.normal(0, 1, n) + 5 * altura # Variable dependiente salario = 1 + 2 * altura + 5 * ingreso_hh + np.random.normal(0, 1, n) df = pd.DataFrame({ &#39;altura&#39;: altura, &#39;ingreso_hh&#39;: ingreso_hh, &#39;salario&#39;: salario }) # CASO 1 print(sm.OLS.from_formula(&#39;salario ~ altura&#39;, data=df).fit().summary()) print(sm.OLS.from_formula(&#39;salario ~ altura + ingreso_hh&#39;, data=df).fit().summary()) # CASO 2 df[&#39;ingreso_hh2&#39;] = np.random.normal(0, 1, n) df[&#39;salario2&#39;] = 1 + 2 * df[&#39;altura&#39;] + 5 * df[&#39;ingreso_hh2&#39;] + np.random.normal(0, 1, n) print(sm.OLS.from_formula(&#39;salario2 ~ altura&#39;, data=df).fit().summary()) print(sm.OLS.from_formula(&#39;salario2 ~ altura + ingreso_hh2&#39;, data=df).fit().summary()) # CASO 3 df[&#39;ingreso_hh3&#39;] = np.random.normal(0, 1, n) + 3 * df[&#39;altura&#39;] df[&#39;salario3&#39;] = 1 + 2 * df[&#39;altura&#39;] + np.random.normal(0, 1, n) print(sm.OLS.from_formula(&#39;salario3 ~ altura&#39;, data=df).fit().summary()) print(sm.OLS.from_formula(&#39;salario3 ~ altura + ingreso_hh3&#39;, data=df).fit().summary()) # FWL paso a paso modelo_y = sm.OLS.from_formula(&#39;salario ~ ingreso_hh&#39;, data=df).fit() df[&#39;My&#39;] = modelo_y.resid modelo_xr = sm.OLS.from_formula(&#39;altura ~ ingreso_hh&#39;, data=df).fit() df[&#39;MXr&#39;] = modelo_xr.resid print(sm.OLS.from_formula(&#39;My ~ MXr&#39;, data=df).fit().summary()) # VerificaciÃ³n print(sm.OLS.from_formula(&#39;salario ~ altura + ingreso_hh&#39;, data=df).fit().summary()) # Comparar coeficientes print(&quot;Coeficiente de altura (residuos):&quot;, sm.OLS.from_formula(&#39;My ~ MXr&#39;, data=df).fit().params[&#39;MXr&#39;]) print(&quot;Coeficiente de altura (modelo completo):&quot;, sm.OLS.from_formula(&#39;salario ~ altura + ingreso_hh&#39;, data=df).fit().params[&#39;altura&#39;]) ğŸ“˜ Preguntas de repaso Â¿QuÃ© significa que el coeficiente de una variable en una regresiÃ³n mÃºltiple sea â€œdepuradoâ€? ğŸ“Œ Preguntas sobre FWL y matrices de proyecciÃ³n Sea \\(y \\in \\mathbb{R}^{n \\times 1}\\), \\(X \\in \\mathbb{R}^{n \\times k}\\), y \\(D \\in \\mathbb{R}^{n \\times 1}\\) una variable binaria tal que \\(D_i = 1\\) solo para una observaciÃ³n \\(i\\), y \\(D_j = 0\\) para \\(j \\neq i\\). Use los pasos del teorema de Frisch-Waugh-Lovell para demostrar que el coeficiente estimado de \\(D\\) representa la diferencia entre la observaciÃ³n \\(i\\) y la predicciÃ³n para esa observaciÃ³n basada en el resto de la muestra. Â¿QuÃ© ocurre con la matriz de proyecciÃ³n \\(P_D\\)? Â¿QuÃ© dimensiÃ³n tiene y cÃ³mo se interpreta cuando solo proyecta sobre una observaciÃ³n? Â¿QuÃ© ocurre con la matriz de aniquilaciÃ³n \\(M_D = I - P_D\\)? Â¿QuÃ© efecto tiene sobre el resto del vector \\(y\\)? Use esta informaciÃ³n para demostrar que al incluir \\(D\\) en la regresiÃ³n, se estÃ¡ excluyendo efectivamente la observaciÃ³n \\(i\\) del resto del modelo. Es decir, la estimaciÃ³n de los coeficientes asociados a \\(X\\) se hace como si se eliminara la observaciÃ³n \\(i\\). Repita el anÃ¡lisis anterior, pero ahora asuma que la variable \\(D\\) es una constante. Â¿QuÃ© ocurre con las matrices de proyecciÃ³n y aniquilaciÃ³n? Â¿QuÃ© interpretaciÃ³n tiene proyectar sobre una constante? Suponga ahora que \\(X\\) es una dummy igual a 1 si el individuo es hombre, y que \\(D\\) es una dummy igual a 1 si el individuo es mujer. Â¿QuÃ© ocurre en este caso? Â¿CÃ³mo se interpretan los coeficientes al incluir ambas dummies en la regresiÃ³n? ğŸ“˜ Preguntas sobre modelos con variables binarias y constantes Se desea estudiar el nÃºmero de horas de lectura diaria \\(Y\\) como funciÃ³n del nivel educativo. Se tiene una muestra de \\(N\\) individuos clasificados en tres grupos: Grupo 1: estudios superiores Grupo 2: estudios medios Grupo 3: estudios bajos Se definen tres variables binarias \\(D_1, D_2, D_3\\), donde: \\[ D_j = \\begin{cases} 1 &amp; \\text{si el individuo pertenece al grupo } j \\\\ 0 &amp; \\text{en caso contrario} \\end{cases} \\] Al estimar el siguiente modelo: \\[ Y = 10 D_1 + 5 D_2 + 2 D_3 + u \\] Demuestre matemÃ¡ticamente que las medias condicionales de horas de lectura son 10, 5 y 2 para cada grupo. Â¿QuÃ© sucede si se incluye una constante en este modelo? Â¿QuÃ© problema empÃ­rico surge? Proponga una soluciÃ³n (por ejemplo, eliminar una de las dummies para evitar colinealidad perfecta). ğŸ§® Preguntas sobre regresiones simples con constantes y dummies Si se estima una regresiÃ³n de \\(Y\\) contra una constante y \\(D_1\\), Â¿cuÃ¡l es el intercepto y cuÃ¡l es el coeficiente de \\(D_1\\)? InterprÃ©telos. Si se estima una regresiÃ³n de \\(Y\\) contra una constante y \\(D_2\\), Â¿quÃ© coeficiente acompaÃ±a a \\(D_2\\)? Â¿CÃ³mo cambia la interpretaciÃ³n con respecto al caso anterior? ğŸ“Š Pregunta sobre FWL y Ã¡lgebra matricial Considere el siguiente modelo lineal sin constante: \\[ Y = X \\beta + u \\] donde \\(Y \\in \\mathbb{R}^{n \\times 1}\\), \\(X \\in \\mathbb{R}^{n \\times k}\\), y \\(u \\in \\mathbb{R}^{n \\times 1}\\). Suponga ademÃ¡s que desea controlar por un conjunto adicional de variables \\(Z \\in \\mathbb{R}^{n \\times m}\\). Defina \\(\\tilde{Y} = M_Z Y\\) y \\(\\tilde{X} = M_Z X\\), donde \\(M_Z = I - P_Z\\) y \\(P_Z = Z(Z&#39;Z)^{-1}Z&#39;\\). Explique con detalle quÃ© representan estas transformaciones. Â¿QuÃ© parte de \\(Y\\) y de \\(X\\) estÃ¡n conservando? Â¿QuÃ© parte estÃ¡n eliminando? Demuestre que el vector de coeficientes \\(\\hat{\\beta}\\) para \\(X\\), en la regresiÃ³n de \\(Y\\) sobre \\(X\\) y \\(Z\\), puede obtenerse a partir de la siguiente expresiÃ³n: \\[ \\hat{\\beta} = (\\tilde{X}&#39; \\tilde{X})^{-1} \\tilde{X}&#39; \\tilde{Y} \\] Â¿Bajo quÃ© condiciones es vÃ¡lida esta expresiÃ³n? Â¿QuÃ© sucede si \\(\\tilde{X}&#39; \\tilde{X}\\) no es invertible? Explique cÃ³mo se interpreta este resultado en tÃ©rminos del Teorema de Frisch-Waugh-Lovell. "]]
