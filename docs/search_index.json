[["index.html", "Econometría II Información general Descripción del curso Material bibliográfico Evaluación Programa semanal Recursos adicionales Inclusión Integridad académica", " Econometría II Ana María Díaz 2025-07-16 Información general Campo Detalle Curso Econometría Avanzada (Cod 1420) Docente Ana María Díaz Oficina / Atención Séptimo piso Edificio 20 | Lunes 9–11 a.m. (o por Teams) Sitio web http://adiazescobar.com Correo a.diaze@javeriana.edu.co Prerequisito Econometría I Horario de clase Martes y Jueves 11:00–13:00 | Salones 67‑208 y 67‑314 Monitor Miguel Ángel Cortés — horarios y oficina por definir Descripción del curso El objetivo principal es proporcionar herramientas para el análisis econométrico de datos de corte transversal, series de tiempo y panel. Se revisa el modelo clásico de regresión lineal, las consecuencias de violar sus supuestos, modelos para variables dependientes discretas o limitadas y técnicas básicas de series de tiempo y panel. Al finalizar, el estudiante podrá ejecutar regresiones múltiples, diagnosticar problemas comunes y aplicar soluciones apropiadas. Material bibliográfico Libro obligatorio Verbeek, Marno (2004). A Guide to Modern Econometrics. Wiley. Libros recomendados Greene, William (2003). Econometric Analysis. Prentice Hall. Wooldridge, Jeffrey (2003). Introductory Econometrics: A Modern Approach. Thomson. Wooldridge, Jeffrey (2002). Econometric Analysis of Cross Section and Panel Data. MIT Press. Montenegro, Álvaro (2009). Series de Tiempo. Javegraf, PUJ. Stock, J. &amp; Watson, M. (2006). Introduction to Econometrics. Addison‑Wesley. Hayashi, Fumio (2000). Econometrics. Princeton UP. Angrist, J.D. &amp; Pischke, J.S. (2009). Mostly Harmless Econometrics. Princeton UP. Cameron, A.C. &amp; Trivedi, P.K. (2009). Microeconometrics Using Stata. Stata Press. Stata 11 Time Series Reference Manual. Stata Press. Rosales R. et al. (2010). Fundamentos de Econometría Intermedia. CEDE. Evaluación Porcentaje Actividad 25 % Parcial 1 teórico 25 % Parcial 2 teórico 3 % Talleres en clase 7 % Monitorías 15 % Trabajo final 25 % Examen final +0.5 (para el mejor) Video‑bono examen final Los exámenes son con libro cerrado y sin dispositivos electrónicos. El incumplimiento se sanciona según el reglamento de integridad académica. Programa semanal Semana Tema principal Lecturas clave 1 Supuestos del MCRL Verbeek cap. 1‑2 (oblig.)  | Hayashi cap. 1; Wooldridge cap. 1‑2 (opc.) 2 Regresión simple vs múltiple; Teorema FWL Verbeek cap. 1‑2  | The Stata Journal (2013) 13(1): 92‑106 3 Propiedades de MCO en muestras finitas; Teorema Gauss‑Markov Verbeek cap. 2 4 Inferencia y predicción; Propiedades asintóticas de MCO Verbeek cap. 3 5 Primer parcial — 6 No linealidad; Multicolinealidad Verbeek cap. 3‑4 7 Heterocedasticidad Verbeek cap. 4 8 Endogeneidad: simultaneidad, omitidas, medición Verbeek cap. 5 9 Variables instrumentales, MCO2E, GMM Verbeek cap. 5 10 Modelos LPM, logit y probit Verbeek cap. 7 11 Máximo verosimilitud; DID, RD, duración, cuantílica (opc.) — 12 Semana Santa / Receso — 13 Segundo parcial — 14 Series de tiempo: conceptos básicos Verbeek cap. 8 15 AR, MA y VAR estacionarios Verbeek cap. 9 16 Datos de panel: pooled, between, FE, RE Verbeek cap. 10 17 Examen final — Recursos adicionales Ben Lambert – Econometrics on YouTube Mastering Econometrics (MRU) AEA Journal of Economic Perspectives – Classroom Google Dataset Search Stata Cheat Sheets Seeing Theory – Visual Probability Inclusión Este curso da la bienvenida a personas de todas las edades, orígenes, creencias, etnias, géneros, identidades, orientaciones sexuales y capacidades. Se espera un ambiente respetuoso e inclusivo. Integridad académica La Universidad Javeriana fomenta la honestidad y establece sanciones por fraude o plagio según el reglamento de estudiantes. Cualquier uso no autorizado de materiales durante evaluaciones se considera falta grave. "],["repaso.html", "1 Repaso Construimos una población de juguete ¿Y si mi muestra es mala? ¿Y si mantengo fija la muestra? 📘 Preguntas de repaso", " 1 Repaso ¿Qué estudia la econometría? La econometría es la herramienta que usamos para entender el mundo usando datos. Nos ayuda a responder preguntas como: ¿cuánto gana una persona según su nivel educativo? ¿Cómo influye la experiencia laboral en el salario? ¿Cuál es el impacto de una política pública sobre el empleo? Pero aquí hay un reto importante: casi nunca podemos observar a toda la población. En vez de eso, trabajamos con una muestra. Usamos esta muestra para hacer inferencias sobre cómo funciona el mundo real, ese que no podemos ver completamente. En este capítulo vamos a entender, paso a paso, por qué eso genera incertidumbre —y por qué esa incertidumbre es una parte inevitable (¡y valiosa!) del análisis econométrico. El proceso generador de datos Supongamos que el salario de un individuo, \\(y_i\\), depende de forma lineal de su nivel educativo, \\(x_i\\): \\[ y_i \\;=\\; \\beta_0 \\;+\\; \\beta_1\\,x_i \\;+\\; u_i, \\] donde \\(u_i\\) recoge todo lo que no observamos (habilidad, contactos, suerte…). A esta ecuación la llamaremos Proceso Generador de Datos (PGD) o modelo poblacional. El problema es que no podemos observar \\(u_i\\) porque es un término de error. Ni tenemos acceso a todos los individuos de la población. ¿Qué hacemos entonces? En la práctica, tomamos una muestra aleatoria de individuos y observamos sus salarios y años de educación \\((y_i,\\,x_i)\\), el termino de error \\(u_i\\) permanece oculto. Con una muestra aleatoria de tamaño \\(n\\), estimamos los parámetros \\(\\beta_0\\) y \\(\\beta_1\\) de la siguiente manera: \\[ y_i \\;=\\; \\hat{\\beta}_0 + \\hat{\\beta}_1\\,x_i + e_i, \\qquad \\hat{y}_i \\;=\\; \\hat{\\beta}_0 + \\hat{\\beta}_1\\,x_i, \\] Donde \\(\\hat{\\beta}_0\\) y \\(\\hat{\\beta}_1\\) son los estimadores de los parámetros poblacionales \\(\\beta_0\\) y \\(\\beta_1\\), y \\(e_i\\) es el término de error muestral. A la recta que obtenemos se le llama modelo muestral. La direfencia entre este modelo y el PGD es precisamente lo que genera incertidumbre en nuestras estimaciones. Es decir que tenemos dos fuentes de incertidumbre, la muestra que compone nuestros datos y el término de error \\(u_i\\) que no podemos observar. Para entender todo esto mejor, vamos primero a enfocarnos en la muestra que tenemos y cómo podemos usarla para estimar el PGD. Luego veremos cómo la incertidumbre afecta nuestras estimaciones y por qué es importante. Construimos una población de juguete Para ilustrar el proceso generador de datos y la incertidumbre, vamos a crear una población de juguete. Esta población será un conjunto de 100 individuos con características específicas. Luego tomaremos muestras aleatorias de esta población y realizaremos regresiones para ver cómo se comportan nuestras estimaciones en comparación con el PGD real. Vamos a crear un mundo ficticio con 100 individuos. A cada uno le asignamos: \\(x\\) (años de educación) sigue una normal con media 5 y desviación 1.5. \\(y\\) depende linealmente de \\(x\\) con pendiente 0.5 y un término aleatorio \\(u\\sim N(0,1)\\). La relación verdadera en la población El modelo poblacional que usamos, es decir el PGD, es: \\(y = 3 + 0.5x\\): Así que en promedio los salarios de los individuos aumentan en 0.5 por cada año adicional de educación. Esta es la verdad de nuestra población simulada. Obtenemos que los coeficientes son muy similares a los que usamos para generar la población: \\[ y_i = 2.53 + 0.57 x_i + u_i \\] Esto significa que el modelo poblacional es: \\[ y_i = \\beta_0 + \\beta_1 x_i + u_i \\] Sin embargo, esa linea está fuera de nuestro alcance porque requeriría encuestar a todos los egresados. Podemos estimar la relación entre \\(y\\) y \\(x\\) en una muestra aleatoria de individuos. Comencemos tomando 30 graduados al azar de nuestro grupo de 100 individuos: Estimemos la relación que existe entre \\(y\\) y \\(x\\) en esta muestra de 30 individuos. En la siguiente gráfica, la línea roja es el modelo poblacional y la línea negra discontinua es el modelo muestral. Ahora encontramos unos coeficientes estimados que son diferentes a los del modelo poblacional: PGD Modelo Poblacional \\(y_i = 2.53 + 0.57 x_i + u_i\\) Modelo muestral \\(\\hat{y}_i = 2.36 + 0.61 x_i\\) Tomemos otros 30 individuos al azar de la población y veamos cómo se comporta la regresión. Ahora encontramos los siguientes coeficientes estimados: PGD Modelo Poblacional \\(y_i = 2.53 + 0.57 x_i + u_i\\) Modelo muestral \\(\\hat{y}_i = 2.79 + 0.56 x_i\\) Podemos ver que los coeficientes estimados son diferentes a los del modelo poblacional y también diferentes entre sí. Esto es normal, porque cada muestra aleatoria puede dar lugar a diferentes estimaciones. Tomemos una tercera muestra aleatoria de 30 individuos y veamos cómo se comporta la regresión. Ahora encontramos los siguientes coeficientes estimados: PGD Modelo Poblacional \\(y_i = 2.53 + 0.57 x_i + u_i\\) Modelo muestral \\(\\hat{y}_i = 3.21 + 0.45 x_i\\) Siguen siendo diferentes a los del modelo poblacional y también diferentes entre sí. A veces se parece mucho, a veces no tanto. La razón es simple; cada muestra incluye un conjunto diferente de personas y eso cambia los resultados. Ahora repitamos esto 10,000 veces. Este ejercicio se conoce como Ejercicio de Monte Carlo. Vamos a tomar 10,000 muestras aleatorias de 30 individuos de nuestra población y estimar los coeficientes de regresión para cada muestra. Luego, graficaremos todas las líneas de regresión obtenidas para ver cómo se distribuyen en relación con la línea poblacional. ¿Lo interesante? Aunque cada recta individual es distinta, en promedio todas convergen hacia la recta verdadera (la primera que estimamos). En resumen, en promedio las líneas de regresión se ajustan muy bien a la línea de la población. Sin embargo, las líneas individuales (muestras) pueden desviarse significativamente. Las diferencias entre las muestras individuales y la población generan incertidumbre para el econometrista. 👉 Este resultado es tranquilizador: aunque nuestras estimaciones varían de muestra a muestra, en promedio nos acercamos a la verdad. Esto es lo que se conoce como insesgamiento del estimador MCO. Eso implica que cuando estimamos los coeficientes de regresión, no podemos estar seguros de que nuestros estimadores sean exactamente iguales a los parámetros poblacionales. En cambio, obtenemos estimaciones que son variables aleatorias. En otras palabras, \\(\\hat{\\beta}\\) en sí mismo es una variable aleatoria, dependiente de la muestra aleatoria. Cuando tomamos una muestra y realizamos una regresión, no sabemos si es una muestra ‘buena’ ( \\(\\hat{\\beta}\\) está cerca de \\(\\beta\\)) o una muestra ‘mala’ (nuestra muestra difiere significativamente de la población). Mantener un registro de esta incertidumbre es clave para el análisis econométrico. Nos permite entender la precisión de nuestras estimaciones y cómo podemos mejorar nuestro modelo. ¿Y si mi muestra es mala? ❓ Pregunta del lector: ¿Qué pasa si me toca una muestra mala? ¿Cómo lo sé? ¿Se puede hacer algo para reducir esa incertidumbre? Una muestra mala es una muestra que, por puro azar, no representa bien a la población. Esto puede pasar incluso si tomamos la muestra correctamente. En esos casos, los estimadores como \\(\\hat{\\beta}_1\\) pueden estar lejos de su valor verdadero \\(\\beta_1\\), y nuestras conclusiones podrían ser engañosas. ¿Cómo saber si me tocó una muestra mala? No podemos saberlo con certeza, porque no conocemos la verdad poblacional. Pero hay señales que nos pueden alertar: Errores estándar grandes: indican mucha variabilidad en la estimación. Intervalos de confianza anchos: reflejan gran incertidumbre. Signos o tamaños inesperados en los coeficientes: pueden deberse a una muestra no representativa. Pruebas de diagnóstico del modelo: pueden revelar si los supuestos no se cumplen (residuos no normales, heterocedasticidad, etc.). ¿Se puede reducir la incertidumbre muestral? ¡Sí! Estas son algunas estrategias comunes: Aumentar el tamaño de la muestra (\\(n\\)) Entre más observaciones, más cerca estará \\(\\hat{\\beta}\\) de \\(\\beta\\) (por la ley de los grandes números). Mejorar el diseño muestral Muestreos estratificados, por conglomerados o con pesos pueden hacer las estimaciones más precisas. Controlar por variables relevantes Incluir más covariables reduce la varianza al explicar mejor el comportamiento de \\(y\\). Usar estimadores eficientes o robustos Si hay heterocedasticidad, los errores estándar robustos o el uso de métodos como MCO ponderado pueden mejorar la precisión. ✅ La importancia de la fuente de los datos Una forma muy eficaz de minimizar el riesgo de una muestra sesgada es usar datos de fuentes confiables y con buen diseño muestral. Por ejemplo, confiar en los datos del DANE en Colombia o de institutos nacionales de estadística en otros países es una práctica fundamental. Estas instituciones diseñan cuidadosamente sus encuestas (como la ECH, ENUT o ENDS) para asegurar que sean representativas de la población. Si el muestreo está bien hecho desde el inicio, el margen de error se reduce y nuestras inferencias serán mucho más confiables. 💡 Mensaje clave: La incertidumbre no es un error: es una característica natural del trabajo con datos. Lo importante no es eliminarla, sino medirla bien, comunicarla con claridad y tenerla en cuenta al tomar decisiones. ¿Y si mantengo fija la muestra? Hasta ahora nos enfocamos en la incertidumbre que surge por el azar de la muestra. Ahora exploramos otra fuente igual de importante: la variabilidad del término de error , incluso si la muestra es fija. Hasta ahora vimos que la incertidumbre puede surgir del hecho de que trabajamos con una muestra: cada subconjunto aleatorio de la población genera estimadores ligeramente diferentes. Pero hay una segunda fuente de incertidumbre: el término de error \\(u_i\\). Recordemos el modelo: \\[ y_i = \\beta_0 + \\beta_1 x_i + u_i \\] Aunque tuviéramos toda la población (o una muestra perfecta), no podríamos predecir perfectamente \\(y_i\\) porque el valor de \\(u_i\\) sigue siendo desconocido. El termino de error \\(u_i\\) representa todo lo que influye en \\(y_i\\) pero no está capturado por \\(x_i\\). Por ejemplo, en un modelo donde \\(y_i\\) es el salario y \\(x_i\\) es la educación: Habilidades intrínsecas Redes de contacto Experiencia laboral previa Suerte (¡sí, también cuenta!) Todo eso se concentra en \\(u_i\\), que es no observable, pero no irrelevante. Incluso si estimamos \\(\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i\\) con una muestra fija: Nuestra recta de regresión capta la tendencia promedio. Pero los valores observados de \\(y_i\\) se dispersan alrededor de la recta por culpa de \\(u_i\\). Esto se ve así: Al repetir el proceso de muestreo muchas veces, cada muestra tendrá su propia recta de regresión, pero todas estarán dispersas alrededor de la recta poblacional. Vamos a hacer lo siguiente: Tomamos una sola muestra fija de 30 individuos de la población. Mantenemos sus valores de \\(x_i\\) constantes. Re-generamos el término de error \\(u_i \\sim N(0,1)\\) varias veces. Calculamos nuevos valores de \\(y_i = 3 + 0.5x_i + u_i\\) en cada iteración. Estimamos una regresión para cada muestra simulada. Esto nos permite ver cómo las estimaciones varían únicamente por culpa del término de error, manteniendo fija la muestra. Aquí vemos que, aunque la muestra es fija, las rectas de regresión se dispersan alrededor de la recta poblacional. Esto es porque el término de error \\(u_i\\) introduce variabilidad en los valores de \\(y_i\\). ☝️ Una aclaración importante sobre el insesgamiento Al repetir el proceso de muestreo muchas veces, cada muestra tendrá su propia recta de regresión. Pero ojo: aunque esas rectas se vean “alrededor” de la recta poblacional en nuestras simulaciones, esto no siempre ocurre en la vida real. El hecho de que las estimaciones se agrupen alrededor de los verdaderos valores poblacionales depende de que se cumplan los supeustos de modelo de regresión lineal. Por ejemplo: Si el término de error \\(u_i\\) está correlacionado con \\(x_i\\) (por ejemplo, porque omitimos una variable relevante), entonces nuestro estimador de \\(\\beta_1\\) estará sesgado. Si hay errores de medición, mala especificación del modelo o selección no aleatoria, también se viola el insesgamiento. 🎯 ¿Cuándo es cierto que nuestras estimaciones “se agrupan” alrededor del verdadero \\(\\beta_1\\)? Cuando se cumplen los supuestos del modelo clásico de regresión lineal, en particular exogeneidad estricta o independencia del término de error \\(u_i\\) respecto a las variables explicativas \\(x_i\\). En algunos libros se conoce como esperanza condicional igual cero: \\[ \\mathbb{E}[u_i \\mid x_i] = 0 \\] 🔁 Si este supuestos se cumplen, entonces nuestro estimador de Mínimos Cuadrados Ordinarios (MCO) es insesgado: \\[ \\mathbb{E}[\\hat{\\beta}_1] = \\beta_1 \\] Es decir, si repitiéramos el experimento de muestreo muchas veces, el promedio de nuestras estimaciones convergería al verdadero valor poblacional. Pero si no se cumplen, como veremos más adelante, podemos tener: Estimadores sesgados Estimadores inconsistentes Intervalos de confianza y pruebas de hipótesis inválidos 💬 Entonces, ¿las simulaciones que hicimos son “realistas”? Sí… bajo los supuestos del modelo clásico. En nuestras simulaciones controlamos todo: sabemos exactamente cómo se genera \\(y_i\\), y aseguramos que \\(u_i\\) sea independiente de \\(x_i\\). Por eso nuestras estimaciones tienden a agruparse cerca de la recta poblacional. Pero en el mundo real, los datos no vienen con etiqueta de “supuestos cumplidos”. Por eso uno de los grandes desafíos del análisis econométrico es diagnosticar y justificar si los supuestos se cumplen. Y si no se cumplen, buscar soluciones: variables instrumentales, variables omitidas, diseños cuasiexperimentales, diseños experimentales, etc. 💡 Conclusión clave: El término de error no desaparece, incluso cuando tenemos una muestra grande o bien diseñada. Por eso, cualquier estimación puntual (\\(\\hat{\\beta}_1\\)) debe ir acompañada de una medida de incertidumbre, como el error estándar o un intervalo de confianza. Esto nos prepara para el siguiente paso: la inferencia estadística. 📘 Preguntas de repaso ¿Qué diferencia hay entre el modelo poblacional y el modelo muestral? ¿Cuál de los dos observamos y cuál inferimos? ¿Por qué decimos que el término de error \\(u_i\\) es una fuente de incertidumbre, incluso si la muestra está fija? ¿Qué condiciones deben cumplirse para que el estimador de Mínimos Cuadrados Ordinarios (MCO) sea insesgado? ¿Qué significa que un estimador sea insesgado “en promedio”? ¿Eso garantiza que cualquier muestra nos dará un buen resultado? ¿Qué implicaciones tiene usar una muestra mal diseñada o no representativa? En la simulación Monte Carlo, ¿por qué las rectas de regresión estimadas con diferentes muestras se agrupan alrededor de la recta poblacional? ¿Qué observas cuando repetimos la estimación con una misma muestra, pero re-generamos el término de error? ¿Qué se mantiene constante y qué varía? En tus propias palabras, ¿por qué no podemos predecir perfectamente \\(y_i\\) aunque conozcamos bien \\(x_i\\)? ¿Por qué se llama “Monte Carlo” a este método de simulación? ¿Qué relación tiene con el azar? ¿Qué riesgos implica asumir que los supuestos del modelo clásico se cumplen cuando en realidad no lo hacen? ¿Qué estrategias puedes usar si sospechas que \\(u_i\\) está correlacionado con \\(x_i\\)? Menciona al menos dos. ¿Crees que todas las fuentes oficiales de datos (como el DANE) garantizan muestras perfectamente representativas? ¿Qué condiciones lo permitirían? ✏️ Opcional para práctica adicional: simula tu propia población de juguete con una relación negativa entre \\(x\\) y \\(y\\), y repite el ejercicio de Monte Carlo. ¿Qué cambia? ¿Qué se mantiene? "],["regresión-lineal.html", "2 Regresión lineal 🎯 Objetivo del capítulo 🔍 ¿Qué significa encontrar la “mejor línea”? MCO 📊 Propiedades y supuestos 📘 Preguntas de repaso", " 2 Regresión lineal 🎯 Objetivo del capítulo En este capitulo vamos a: 1. Entender qué es una regresión lineal y cómo se ve gráficamente. 2. Aprender cómo se calcula la mejor línia con mínimos cuadrados ordinarios (MCO) 3. Explorar qué hace un buen estiamdor y cómo evaluarlo 🔍 ¿Qué significa encontrar la “mejor línea”? Antes de hablar de estimaciones, pensemos en cómo se generan los datos: Supondremos que hay un modelo poblacional o proceso generador de datos: \\[ y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i \\] \\(y_i\\): variable dependiente (lo que queremos explicar) \\(x_i\\): variable independiente \\(\\beta_0, \\beta_1\\): parámetros poblacionales \\(\\epsilon_i\\): término de error: todo lo que afecta a \\(y_i\\) y no está en \\(x_i\\) El término \\(\\epsilon_i\\) captura factores no observados, errores de medición, y variación aleatoria. Es fundamental porque incluso si tuviéramos los valores verdaderos de \\(\\beta_0\\) y \\(\\beta_1\\), seguiríamos sin poder predecir perfectamente \\(y_i\\) debido a este componente. En la práctica, estimamos los parámetros a partir de una muestra. Esto nos da una versión estimada del modelo: \\[ \\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i \\] Y calculamos los residuos (errores estimados): \\[ \\hat{\\epsilon}_i = y_i - \\hat{y}_i \\] Queremos encontrar la línea que prediga \\(y_i\\) con la menor cantidad posible de errores. Eso significa minimizar: \\[ \\text{SRC} = \\sum_{i = 1}^{n} \\hat{\\epsilon}_i^2 \\] Esto se conoce como el criterio de mínimos cuadrados. 🎨 Ilustremos esto con un ejemplo visual Creemos unos nuevos datos para ilustrar esto. La linea de regresión es igual a \\(\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x\\) donde _0$ y \\(\\hat{\\beta}_1\\) son los parámetros estimados de la regresión. En este caso, \\(\\hat{\\beta}_0 = 6\\) y \\(\\hat{\\beta}_1 = 0.2\\). Para cada una de las observaciones podemos encontrar el y estimado \\(\\hat{y}_i\\). En la siguiente figura, la línea naranja representa la línea de regresión estimada. Para cada una de las observaciones podemos calcular los errores: \\(\\epsilon_i = y_i - \\hat{y}_i\\), como se observa en el siguiente gráfico. Ahora podemos probar con otras lineas y ver cómo se comportan los errores. En el siguiente grafico, la línea de regresión estimada es \\(\\hat{y} = 3 + 0.2 x\\). Es evidente que los errores estiamdos son más grandes que los errores estimados en el gráfico anterior. Probemos ahora con una línea de regresión estimada que no se ajusta a los datos, \\(\\hat{y} = 10 - 0.8 x\\). En este caso, los errores son aún más grandes. Recuerda que SRC es igual a: \\(\\left(\\sum e_i^2\\right)\\): Errores más grandes reciben penalizaciones más grandes. La estimación de MCO es la combinación de \\(\\hat{\\beta}_0\\) y \\(\\hat{\\beta}_1\\) que minimiza la SRC MCO Formalmente En una regresión lineal simple, el estimador de MCO proviene de escoger \\(\\hat{\\beta}_0\\) y \\(\\hat{\\beta}_1\\) que minimice la suma de residuos al cuadrado (SRC), i.e., \\[ \\min_{\\hat{\\beta}_0,\\, \\hat{\\beta}_1} \\text{SRC} \\] donde \\[ \\text{SRC} = \\sum_{i = 1}^{n} (y_i - \\hat{y}_i)^2 = \\sum_{i = 1}^{n} (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)^2 \\] El estimador de MCO es el valor de \\(\\hat{\\beta}_0\\) y \\(\\hat{\\beta}_1\\) que minimiza la SRC. pero nosotros sabemos que \\(\\text{SRC} = \\sum_i \\tilde{\\epsilon_i}^2\\). Now use the definitions of \\(\\tilde{\\epsilon_i}\\) and \\(\\hat{y}\\). \\[ \\begin{aligned} \\tilde{\\epsilon_i}^2 &amp;= \\left( y_i - \\hat{y}_i \\right)^2 = \\left( y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i \\right)^2 \\\\ &amp;= y_i^2 - 2 y_i \\hat{\\beta}_0 - 2 y_i \\hat{\\beta}_1 x_i + \\hat{\\beta}_0^2 + 2 \\hat{\\beta}_0 \\hat{\\beta}_1 x_i + \\hat{\\beta}_1^2 x_i^2 \\end{aligned} \\] Recordatorio: Minimizar una función multivariada requiere (1) que las primeras derivadas sean iguales a cero (las condiciones de primer orden) y (2) las condiciones de segundo orden (concavidad). Nos estamos acercando. Necesitamos minimizar la SRC. \\[ \\text{SRE} = \\sum_i \\tilde{e_i}^2 = \\sum_i \\left( y_i^2 - 2 y_i \\hat{\\beta}_0 - 2 y_i \\hat{\\beta}_1 x_i + \\hat{\\beta}_0^2 + 2 \\hat{\\beta}_0 \\hat{\\beta}_1 x_i + \\hat{\\beta}_1^2 x_i^2 \\right) \\] For the first-order conditions of minimization, we now take the first derivates of SSE with respect to \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\). \\[ \\begin{aligned} \\dfrac{\\partial \\text{SRC}}{\\partial \\hat{\\beta}_0} &amp;= \\sum_i \\left( 2 \\hat{\\beta}_0 + 2 \\hat{\\beta}_1 x_i - 2 y_i \\right) = 2n \\hat{\\beta}_0 + 2 \\hat{\\beta}_1 \\sum_i x_i - 2 \\sum_i y_i \\\\ &amp;= 2n \\hat{\\beta}_0 + 2n \\hat{\\beta}_1 \\overline{x} - 2n \\overline{y} \\end{aligned} \\] donde \\(\\overline{x} = \\frac{\\sum x_i}{n}\\) y \\(\\overline{y} = \\frac{\\sum y_i}{n}\\) son medias muestrales de \\(x\\) y \\(y\\) (de tamaño \\(n\\)). Las condiciones de primer orden establecen que las derivadas deben ser iguales a cero: \\[ \\dfrac{\\partial \\text{SSE}}{\\partial \\hat{\\beta}_0} = 2n \\hat{\\beta}_0 + 2n \\hat{\\beta}_1 \\overline{x} - 2n \\overline{y} = 0 \\] Lo que implica \\[ \\hat{\\beta}_0 = \\overline{y} - \\hat{\\beta}_1 \\overline{x} \\] Ahora para \\(\\hat{\\beta}_1\\). Tomemos la derivada de la SRC con respecto a \\(\\hat{\\beta}_1\\) \\[ \\begin{aligned} \\dfrac{\\partial \\text{SSE}}{\\partial \\hat{\\beta}_1} &amp;= \\sum_i \\left( 2 \\hat{\\beta}_0 x_i + 2 \\hat{\\beta}_1 x_i^2 - 2 y_i x_i \\right) = 2 \\hat{\\beta}_0 \\sum_i x_i + 2 \\hat{\\beta}_1 \\sum_i x_i^2 - 2 \\sum_i y_i x_i \\\\ &amp;= 2n \\hat{\\beta}_0 \\overline{x} + 2 \\hat{\\beta}_1 \\sum_i x_i^2 - 2 \\sum_i y_i x_i \\end{aligned} \\] Igualarlo a cero \\[ \\dfrac{\\partial \\text{SSE}}{\\partial \\hat{\\beta}_1} = 2n \\hat{\\beta}_0 \\overline{x} + 2 \\hat{\\beta}_1 \\sum_i x_i^2 - 2 \\sum_i y_i x_i = 0 \\] y reemplazarlo \\(\\hat{\\beta}_0\\), i.e., \\(\\hat{\\beta}_0 = \\overline{y} - \\hat{\\beta}_1 \\overline{x}\\). Thus, \\[ 2n \\left(\\overline{y} - \\hat{\\beta}_1 \\overline{x}\\right) \\overline{x} + 2 \\hat{\\beta}_1 \\sum_i x_i^2 - 2 \\sum_i y_i x_i = 0 \\] Continuando \\[ 2n \\left(\\overline{y} - \\hat{\\beta}_1 \\overline{x}\\right) \\overline{x} + 2 \\hat{\\beta}_1 \\sum_i x_i^2 - 2 \\sum_i y_i x_i = 0 \\] \\[ 2n \\overline{y}\\,\\overline{x} - 2n \\hat{\\beta}_1 \\overline{x}^2 + 2 \\hat{\\beta}_1 \\sum_i x_i^2 - 2 \\sum_i y_i x_i = 0 \\] \\[ \\implies 2 \\hat{\\beta}_1 \\left( \\sum_i x_i^2 - n \\overline{x}^2 \\right) = 2 \\sum_i y_i x_i - 2n \\overline{y}\\,\\overline{x} \\] \\[ \\implies \\hat{\\beta}_1 = \\dfrac{\\sum_i y_i x_i - 2n \\overline{y}\\,\\overline{x}}{\\sum_i x_i^2 - n \\overline{x}^2} = \\dfrac{\\sum_i (x_i - \\overline{x})(y_i - \\overline{y})}{\\sum_i (x_i - \\overline{x})^2} \\] LISTOO! Ahora tenemos nuestros lindos estimadores \\[ \\hat{\\beta}_1 = \\dfrac{\\sum_i (x_i - \\overline{x})(y_i - \\overline{y})}{\\sum_i (x_i - \\overline{x})^2} \\] and the intercept \\[ \\hat{\\beta}_0 = \\overline{y} - \\hat{\\beta}_1 \\overline{x} \\] Ya sabes de dónde proviene la parte de mínimos cuadrados en el término “mínimos cuadrados ordinarios”. 🎊 Ahora pasamos a las propiedades (implícitas) de los Mínimos Cuadrados Ordinarios (MCO / OLS). 📊 Propiedades y supuestos ¿Qué hace a un buen estimador? Antes de hablar de propiedades del estimador de MCO, recordemos algunas herramientas fundamentales de estadística. 2.0.1 📈 Repaso: Funciones de densidad Las funciones de densidad de probabilidad (FDP, o PDF en inglés) describen la probabilidad de que una variable aleatoria continua tome valores dentro de un intervalo dado. La probabilidad total bajo la curva es 1. Ejemplo: para una variable normal estándar, la probabilidad de que tome un valor entre -2 y 0 es: \\[ \\mathop{\\text{P}}\\left(-2 \\leq X \\leq 0\\right) = 0.48 \\] Otro ejemplo clásico es la probabilidad de que una variable aleatoria normal estándar tome un valor entre -1.96 y 1.96: \\(\\mathop{\\text{P}}\\left(-1.96 \\leq X \\leq 1.96\\right) = 0.95\\) O la probabilidad de que una variable aleatoria normal estándar tome un valor mayor a 2: \\(\\mathop{\\text{P}}\\left(X &gt; 2\\right) = 0.023\\) 🤔 ¿Qué propiedades buscamos en un estimador? Imaginemos que intentamos estimar un parámetro verdadero \\(\\beta\\), y tenemos tres métodos distintos. Cada uno produce una distribución diferente para \\(\\hat{\\beta}\\). Pregunta: ¿Qué propiedades podrían ser importantes para un estimador? Propiedad 1. Insesgamiento Es decir, si repitiéramos el experimento muchas veces, ¿el estimador tiende a acercarse al valor verdadero del parámetro que estamos tratando de estimar? El sesgo mide si el estimador se acerca al valor real en promedio: 🧪 ¿Qué significa “repetir el experimento”? En este contexto, repetir el experimento puede entenderse de tres formas, todas válidas para pensar en la incertidumbre de un estimador: Cambiar la muestra: imaginar que tomamos muchas muestras aleatorias distintas de la población. Mantener fija la muestra, pero cambiar los errores: incluso si los valores de \\(x_i\\) no cambian, los valores de \\(y_i\\) pueden variar si asumimos que los errores \\(\\epsilon_i\\) son aleatorios. Recuerda que \\(y_i\\) sigue un proceso generador de datos subyacente. Cambiar ambos simultáneamente: es el caso más común en simulaciones — se sortean tanto los \\(x_i\\) como los \\(\\epsilon_i\\). En cualquiera de los tres escenarios, obtendríamos distintos valores de \\(\\hat{\\beta}\\). Eso nos permite construir una distribución muestral del estimador y analizar propiedades como el sesgo. ⚠️ Importante: cuando hablamos de “repetir el experimento”, no queremos decir que volvamos a observar a las mismas personas varias veces con diferentes valores de \\(x\\) (por ejemplo, dándoles distintos años de educación). Lo que estamos haciendo es imaginar escenarios hipotéticos en los que la muestra o los errores cambian, y ver cómo eso afecta al estimador. Estos experimentos no se pueden realizar en la realidad con una misma persona, pero sí los podemos simular por computadora o analizar teóricamente. Más formalmente: ¿La media de la distribución del estimador es igual al parámetro que estima? En promedio (después de muchas repeticiones), ¿el estimador tiende hacia el valor correcto? Más formalmente: ¿La media de la distribución del estimador es igual al parámetro que estima? \\[ \\mathop{\\text{Sesgo}}_\\beta \\left( \\hat{\\beta} \\right) = \\mathop{\\boldsymbol{E}}\\left[ \\hat{\\beta} \\right] - \\beta \\] Estimador Insesagado: \\(\\mathop{\\boldsymbol{E}}\\left[ \\hat{\\beta} \\right] = \\beta\\) Estimador Sesagado: \\(\\mathop{\\boldsymbol{E}}\\left[ \\hat{\\beta} \\right] \\neq \\beta\\) Propiedad 2: Varianza También queremos que nuestras estimaciones no varíen demasiado de una muestra a otra. En otras palabras: queremos un estimador que sea estable, no que en cada muestra nos dé un valor completamente distinto. La varianza mide cuánta variación hay en las estimaciones \\(\\hat{\\beta}\\) que obtenemos al repetir el experimento (cambiando la muestra, los errores, o ambos): \\[ \\text{Var} \\left( \\hat{\\beta} \\right) = \\mathbb{E} \\left[ \\left( \\hat{\\beta} - \\mathbb{E}[\\hat{\\beta}] \\right)^2 \\right] \\] Un estimador con menor varianza produce resultados más consistentes entre muestras. Esto lo hace más confiable, incluso si no es perfecto. 🎯 Queremos que nuestras estimaciones estén “concentradas” cerca del valor esperado, no dispersas como tiros al aire. Veamos un ejemplo visual de cómo la varianza afecta a las distribuciones de los estimadores. La curva rosada representa un estimador con baja varianza: la mayoría de los valores de \\(\\hat{\\beta}\\) están cerca de \\(\\beta\\). Mientras que la curva gris oscuro representa un estimador con alta varianza: sus valores están más dispersos. A igualdad de sesgo, preferimos el estimador que tenga menor varianza. 🎯 El trade-off: sesgo vs. varianza Hasta ahora hablamos del sesgo y de la varianza por separado. Pero muchas veces, mejorar uno implica empeorar el otro. Esto se conoce como el ** trade-off entre sesgo y varianza**. ¿Deberíamos aceptar un poco de sesgo si eso nos permite reducir mucho la varianza? En econometría, solemos preferir estimadores insesgados (o al menos consistentes), porque valoramos la interpretación causal y teórica de los parámetros. Pero en otras disciplinas, como el aprendizaje automático o la predicción estadística, es común aceptar un pequeño sesgo si con ello se logra una gran reducción en la varianza y, en consecuencia, una mejor predicción promedio. veámos esta idea: Propiedad 3: Consistencia La consistencia es una propiedad clave que nos dice qué pasa con el estimador cuando la muestra es cada vez más grande. Intuitivamente, un estimador es consistente si, al aumentar el tamaño de la muestra, sus valores se acercan cada vez más al valor verdadero del parámetro \\(\\beta\\). Esto nos da confianza de que, con datos suficientes, estaremos muy cerca del valor correcto. Formalmente, un estimador \\(\\hat{\\beta}\\) es consistente si: \\[ \\hat{\\beta} \\xrightarrow{p} \\beta \\quad \\text{cuando } n \\to \\infty \\] Esto se lee como: “\\(\\hat{\\beta}\\) converge en probabilidad a \\(\\beta\\)”. Es decir, la probabilidad de que \\(\\hat{\\beta}\\) se aleje mucho de \\(\\beta\\) se hace cada vez más pequeña a medida que usamos muestras más grandes. Veámos lo que ocurre cuando la muestra crece: La curva naranja representa una estimación con mucha incertidumbre (muestra pequeña). La curva gris oscura representa una muestra de tamaño mediano. Mientras que la curva rosada muestra cómo la estimación se concentra alrededor de \\(\\beta\\) con una muestra grande. 🎯 Un estimador consistente se “afina” con más datos: no solo mejora su varianza, sino que tiende a decir la verdad. Propiedad 4: Eficiencia La eficiencia combina las ideas de sesgo y varianza. Entre todos los estimadores insesgados, el más eficiente es aquel que tiene la menor varianza posible. Es decir, si dos estimadores son igual de “correctos en promedio”, preferimos el que sea más estable. La eficiencia no se refiere a un único estimador, sino a una comparación entre estimadores. Formalmente, un estimador \\(\\hat{\\beta}\\) es eficiente si: \\[ \\text{Var}(\\hat{\\beta}) \\leq \\text{Var}(\\hat{\\beta}&#39;) \\] para cualquier otro estimador \\(\\hat{\\beta}&#39;\\) que también sea insesgado. Esto significa que ningún otro estimador insesgado tiene una varianza menor que \\(\\hat{\\beta}\\). 💡 En el contexto de mínimos cuadrados ordinarios (MCO), cuando se cumplen ciertos supuestos (los del teorema de Gauss-Markov), el estimador \\(\\hat{\\beta}_{\\text{MCO}}\\) es el Mejor Estimador Lineal Insesgado, también conocido como MELI: ✔️ Mejor → tiene la menor varianza ✔️ Estimador Lineal → combinación lineal de los datos ✔️ Insesgado → \\(\\mathbb{E}[\\hat{\\beta}] = \\beta\\) 🎓 En resumen, un estimador eficiente es tan preciso como permite la información disponible en los datos, sin sacrificar insesgamiento. Resumen de las propiedades Propiedad Descripción Insesgamiento El estimador no se aleja sistemáticamente del valor verdadero. Varianza El estimador tiene poca variación entre muestras. Consistencia A medida que aumenta el tamaño de la muestra, el estimador converge al valor verdadero. Eficiencia El estimador tiene la menor varianza posible entre todos los estimadores insesgados. 🧠 Nota de cierre: cómo interpretar cada propiedad Cada propiedad que vimos tiene un enfoque ligeramente distinto sobre cómo pensar la incertidumbre: Sesgo: ¿En promedio (tras repetir el experimento), el estimador acierta? Varianza: ¿Qué tanto cambia el estimador de una muestra a otra? Consistencia: ¿El estimador se acerca al valor verdadero si usamos una muestra más grande del mismo experimento? Eficiencia: ¿Este estimador es mejor (más preciso) que otros estimadores insesgados disponibles? 🔁 Las primeras dos propiedades (sesgo y varianza) se entienden a través de repeticiones hipotéticas del experimento. 📈 La consistencia se analiza observando lo que ocurre cuando crece el tamaño muestral. ⚖️ La eficiencia es una comparación entre estimadores, dado que todos sean insesgados. Estas ideas son fundamentales para entender cómo evaluar y justificar un estimador en econometría. 📘 Preguntas de repaso Verdadero o falso (V/F) Un estimador puede ser insesgado pero tener alta varianza. (V/F) La consistencia se refiere a repetir el experimento muchas veces. (V/F) Un estimador eficiente siempre es consistente. (V/F) Si un estimador es insesgado y eficiente, no puede ser mejorado bajo los supuestos del modelo. Selección múltiple {-} ¿Cuál de las siguientes afirmaciones es correcta respecto a la eficiencia? A. Es una propiedad absoluta de un estimador. B. Se refiere a qué tan cerca está \\(\\hat{\\beta}\\) del promedio de los datos. C. Compara la varianza entre estimadores insesgados. D. Es sinónimo de consistencia. ¿Qué pasa con un estimador consistente cuando el tamaño muestral crece? A. Se vuelve insesgado automáticamente. B. Se aleja del valor verdadero. C. Su varianza se hace infinita. D. Se aproxima al valor verdadero con alta probabilidad. Respuesta abierta Explica con tus palabras qué significa que un estimador sea insesgado. ¿Por qué esta propiedad es importante en econometría?** ¿Por qué puede ser útil, en algunos contextos, aceptar un estimador sesgado? Da un ejemplo donde podría ser preferible.** ¿En qué se diferencia el concepto de varianza del de eficiencia? ¿Pueden dos estimadores tener la misma varianza pero distinta eficiencia?** Supón que tienes dos estimadores: - A es insesgado pero tiene alta varianza. - B tiene un pequeño sesgo pero varianza muy baja. ¿Cuál elegirías para un problema donde la prioridad es predecir bien el valor de \\(y\\)? ¿Cambiaría tu respuesta si el objetivo fuera estimar un efecto causal? Justifica tu elección. "],["repaso-de-matrices.html", "3 Repaso de matrices Matrices Vectores Operaciones con matrices Determinantes Matriz inversa Rango de una matriz Sistemas de ecuaciones lineales Matrices cuadradas especiales Derivadas de una función multidimensional 📘 Preguntas de repaso", " 3 Repaso de matrices Antes de introducir los supuestos fundamentales del modelo de regresión lineal, es importante repasar algunos conceptos clave del álgebra matricial. Este lenguaje permite expresar de forma compacta y elegante muchos de los resultados econométricos, facilitando la comprensión de los modelos lineales y sus propiedades. Matrices Una matriz \\(A \\in \\mathbb{R}^{m \\times n}\\) es un conjunto de elementos \\(a_{ij}\\), donde \\(i = 1, \\ldots, m\\) (filas) y \\(j = 1, \\ldots, n\\) (columnas), organizados de la siguiente manera: \\[ A = \\begin{pmatrix} a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1n} \\\\ a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{m1} &amp; a_{m2} &amp; \\cdots &amp; a_{mn} \\end{pmatrix} \\] La dimensión o orden de la matriz es \\(m \\times n\\), lo que indica que tiene \\(m\\) filas y \\(n\\) columnas. Cuando \\(m = n\\), se dice que la matriz es cuadrada; si \\(m \\neq n\\), es una matriz rectangular. Las matrices se representan con letras mayúsculas en negrita, como \\(\\mathbf{A}\\), y sus elementos con letras minúsculas con subíndices, como \\(a_{ij}\\). Los elementos de una matriz pueden ser números reales, \\(a_{ij} \\in \\mathbb{R}\\). Ejemplo. La matriz \\[ B = \\begin{pmatrix} 3 &amp; 1 &amp; 7 \\\\ 2 &amp; 4 &amp; 5 \\end{pmatrix} \\] es una matriz rectangular de orden \\(2 \\times 3\\). Tiene 2 filas y 3 columnas. El elemento en la fila 2 y columna 3 es \\(b_{23} = 5\\). Traspuesta de una matriz La traspuesta de una matriz \\(A = [a_{ij}]\\) de dimensión \\(m \\times n\\) es otra matriz \\(A&#39; = [a_{ji}]\\) de dimensión \\(n \\times m\\), obtenida al intercambiar filas por columnas. Es decir, la primera fila de \\(A\\) se convierte en la primera columna de \\(A&#39;\\), la segunda fila en la segunda columna, y así sucesivamente. \\[ A&#39; = \\begin{pmatrix} a_{11} &amp; a_{21} &amp; \\cdots &amp; a_{m1} \\\\ a_{12} &amp; a_{22} &amp; \\cdots &amp; a_{m2} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{1n} &amp; a_{2n} &amp; \\cdots &amp; a_{mn} \\end{pmatrix} \\] Ejemplo. Sea la matriz \\[ A = \\begin{pmatrix} 6 &amp; 5 &amp; 7 &amp; 4 \\\\ 5 &amp; 4 &amp; 2 &amp; 5 \\\\ 1 &amp; 1 &amp; 11 &amp; 1 \\end{pmatrix} \\] que es de orden \\(3 \\times 4\\). Su traspuesta es: \\[ A&#39; = \\begin{pmatrix} 6 &amp; 5 &amp; 1 \\\\ 5 &amp; 4 &amp; 1 \\\\ 7 &amp; 2 &amp; 11 \\\\ 4 &amp; 5 &amp; 1 \\end{pmatrix} \\] Esta nueva matriz es de orden \\(4 \\times 3\\). Vectores Un vector columna es una matriz de orden \\(m \\times 1\\), es decir, una matriz que solo tiene una columna: \\[ \\mathbf{a} = \\begin{pmatrix} a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_m \\end{pmatrix} \\] Se denota con una letra minúscula en negrita y se puede escribir de forma abreviada como \\(\\mathbf{a} = [a_i]\\). Cada elemento \\(a_i\\) indica la posición del componente dentro del vector. Un vector fila, en cambio, es una matriz de orden \\(1 \\times m\\), es decir, solo tiene una fila: \\[ \\mathbf{a}&#39; = \\begin{pmatrix} a_1 &amp; a_2 &amp; \\cdots &amp; a_m \\end{pmatrix} \\] La traspuesta de un vector columna es un vector fila, y viceversa. En línea, se escribe \\(\\mathbf{a} = (a_1, a_2, \\ldots, a_m)&#39;\\) para indicar que es columna, usando la notación de traspuesta. Ejemplo Sea el vector columna \\[ \\mathbf{v} = \\begin{pmatrix} 2 \\\\ -1 \\\\ 4 \\end{pmatrix} \\quad \\text{de orden } 3 \\times 1, \\quad \\text{su traspuesta es } \\mathbf{v}&#39; = \\begin{pmatrix} 2 &amp; -1 &amp; 4 \\end{pmatrix} \\] Producto escalar Definición. Sean \\(\\mathbf{a} = (a_1, \\ldots, a_m)&#39;\\) y \\(\\mathbf{b} = (b_1, \\ldots, b_m)&#39;\\) dos vectores columna del mismo orden \\(m \\times 1\\), su producto escalar es: \\[ \\mathbf{a}&#39;\\mathbf{b} = \\sum_{i=1}^m a_i b_i = a_1b_1 + a_2b_2 + \\cdots + a_m b_m \\] Ejemplo Si \\(\\mathbf{a} = (1, 2, 3)&#39;\\) y \\(\\mathbf{b} = (4, 5, 6)&#39;\\), entonces: \\[ \\mathbf{a}&#39;\\mathbf{b} = 1 \\cdot 4 + 2 \\cdot 5 + 3 \\cdot 6 = 4 + 10 + 18 = 32 \\] Norma y normalización Definición. La norma de un vector \\(\\mathbf{x}\\) se define como: \\[ \\|\\mathbf{x}\\| = \\sqrt{\\mathbf{x}&#39;\\mathbf{x}} = \\sqrt{x_1^2 + x_2^2 + \\cdots + x_m^2} \\] El vector normalizado es: \\[ \\frac{\\mathbf{x}}{\\|\\mathbf{x}\\|} \\] Ortogonalidad Definición. Dos vectores \\(\\mathbf{a}\\) y \\(\\mathbf{b}\\) son ortogonales (se denota \\(\\mathbf{a} \\perp \\mathbf{b}\\)) si: \\[ \\mathbf{a}&#39;\\mathbf{b} = 0 \\] \\[ \\bar{y} = \\frac{\\mathbf{i}&#39;\\mathbf{y}}{\\mathbf{i}&#39;\\mathbf{i}} \\] Operaciones con matrices Igualdad de matrices Dos matrices \\(A = [a_{ij}]\\) y \\(B = [b_{ij}]\\) de igual orden \\(m \\times n\\) son iguales si: \\[ a_{ij} = b_{ij}, \\quad \\text{para todo } i = 1, \\ldots, m; \\; j = 1, \\ldots, n \\] Suma y resta de matrices La suma de dos matrices del mismo orden es la matriz \\(C = A + B = [c_{ij}]\\) donde: \\[ c_{ij} = a_{ij} + b_{ij} \\] Propiedades: Conmutativa: \\(A + B = B + A\\) Asociativa: \\((A + B) + C = A + (B + C)\\) Elemento neutro: \\(A + 0 = A\\) Opuesto: \\(A + (-A) = 0\\) Ejemplo: \\[ A = \\begin{pmatrix} 6 &amp; 5 &amp; 7 &amp; 4 \\\\ 5 &amp; 4 &amp; 2 &amp; 5 \\\\ 1 &amp; 1 &amp; 11 &amp; 1 \\end{pmatrix}, \\quad B = \\begin{pmatrix} 7 &amp; 11 &amp; 2 &amp; 9 \\\\ 5 &amp; 8 &amp; 8 &amp; 1 \\\\ 6 &amp; 10 &amp; 8 &amp; 10 \\end{pmatrix} \\] \\[ A + B = \\begin{pmatrix} 13 &amp; 16 &amp; 9 &amp; 13 \\\\ 10 &amp; 12 &amp; 10 &amp; 6 \\\\ 7 &amp; 11 &amp; 19 &amp; 11 \\end{pmatrix} \\] Resta: se define como \\(A - B = A + (-B)\\) Multiplicación por un escalar \\[ \\lambda A = [\\lambda a_{ij}] \\] Ejemplo: \\[ 2A = \\begin{pmatrix} 12 &amp; 10 &amp; 14 &amp; 8 \\\\ 10 &amp; 8 &amp; 4 &amp; 10 \\\\ 2 &amp; 2 &amp; 22 &amp; 2 \\end{pmatrix} \\] Multiplicación de matrices Sean \\(A \\in \\mathbb{R}^{m \\times n}\\) y \\(B \\in \\mathbb{R}^{n \\times p}\\), el producto \\(AB \\in \\mathbb{R}^{m \\times p}\\) se define por: \\[ c_{ij} = \\sum_{k=1}^{n} a_{ik} b_{kj} \\] Propiedades: Asociativa: \\((AB)C = A(BC)\\) Distributiva: \\(A(B + C) = AB + AC\\) No conmutativa: en general \\(AB \\neq BA\\) Ejemplo: \\[ A = \\begin{pmatrix} 6 &amp; 5 &amp; 7 &amp; 4 \\\\ 5 &amp; 4 &amp; 2 &amp; 5 \\\\ 1 &amp; 1 &amp; 11 &amp; 1 \\end{pmatrix}, \\quad B&#39; = \\begin{pmatrix} 7 &amp; 5 &amp; 6 \\\\ 11 &amp; 8 &amp; 10 \\\\ 2 &amp; 8 &amp; 8 \\\\ 9 &amp; 1 &amp; 10 \\end{pmatrix} \\] \\[ F = A B&#39; = \\begin{pmatrix} 147 &amp; 130 &amp; 182 \\\\ 128 &amp; 78 &amp; 136 \\\\ 49 &amp; 102 &amp; 114 \\end{pmatrix} \\] Transposición de matrices Ya definida en la sección anterior. Propiedades clave: \\((A&#39;)&#39; = A\\) \\((A + B)&#39; = A&#39; + B&#39;\\) \\((AB)&#39; = B&#39;A&#39;\\) Traza de una matriz La traza de una matriz cuadrada es la suma de los elementos de su diagonal principal: \\[ \\text{tr}(A) = \\sum_{i=1}^{n} a_{ii} \\] Propiedades: \\(\\text{tr}(A) = \\text{tr}(A&#39;)\\) \\(\\text{tr}(A + B) = \\text{tr}(A) + \\text{tr}(B)\\) \\(\\text{tr}(AB) = \\text{tr}(BA)\\) Ejemplo: \\[ \\text{tr}(F) = 147 + 78 + 114 = 339 \\] Determinantes Para matrices cuadradas \\(A \\in \\mathbb{R}^{n \\times n}\\), el determinante se denota \\(|A|\\). Para \\(2 \\times 2\\): \\[ |A| = \\begin{vmatrix} a &amp; b \\\\ c &amp; d \\end{vmatrix} = ad - bc \\] Para \\(3 \\times 3\\): \\[ |A| = a_{11}a_{22}a_{33} + a_{12}a_{23}a_{31} + a_{13}a_{21}a_{32} - a_{13}a_{22}a_{31} - a_{12}a_{21}a_{33} - a_{11}a_{23}a_{32} \\] Ejemplo: \\[ G = \\begin{pmatrix} 1 &amp; 1 &amp; 3 \\\\ 1 &amp; 1 &amp; 0 \\\\ 3 &amp; 1 &amp; 2 \\end{pmatrix} \\Rightarrow |G| = -6 \\] Matriz inversa Una matriz cuadrada \\(A\\) es invertible si existe \\(A^{-1}\\) tal que: \\[ A A^{-1} = A^{-1} A = I \\] Se calcula como: \\[ A^{-1} = \\frac{1}{|A|} \\cdot \\text{adj}(A) \\] donde \\(\\text{adj}(A)\\) es la matriz adjunta (traspuesta de los cofactores). Propiedades: \\((A^{-1})^{-1} = A\\) \\((AB)^{-1} = B^{-1} A^{-1}\\) \\((A&#39;)^{-1} = (A^{-1})&#39;\\) Ejemplo: \\[ G^{-1} = \\frac{1}{-6} \\begin{pmatrix} 2 &amp; 1 &amp; -3 \\\\ -2 &amp; -7 &amp; 3 \\\\ -2 &amp; 2 &amp; 0 \\end{pmatrix} \\] Rango de una matriz El rango de una matriz es el número máximo de filas (o columnas) linealmente independientes. Definiciones: Vectores son linealmente dependientes si \\(c_1a_1 + \\cdots + c_n a_n = 0\\) con \\(c_i \\neq 0\\) Son independientes si la única combinación que da cero es con todos los \\(c_i = 0\\) Propiedades: \\(\\text{rang}(AB) \\leq \\min\\{\\text{rang}(A), \\text{rang}(B)\\}\\) Si \\(A\\) es invertible: \\(\\text{rang}(AB) = \\text{rang}(B)\\) \\(\\text{rang}(A) = \\text{rang}(A A&#39;) = \\text{rang}(A&#39; A)\\) Sistemas de ecuaciones lineales Un sistema de ecuaciones lineales con \\(m\\) ecuaciones y \\(n\\) incógnitas se puede escribir de la forma: \\[ \\begin{aligned} a_{11}x_1 + a_{12}x_2 + \\cdots + a_{1n}x_n &amp;= b_1 \\\\ a_{21}x_1 + a_{22}x_2 + \\cdots + a_{2n}x_n &amp;= b_2 \\\\ &amp;\\vdots \\\\ a_{m1}x_1 + a_{m2}x_2 + \\cdots + a_{mn}x_n &amp;= b_m \\end{aligned} \\] En forma matricial, este sistema se escribe como: \\[ A \\mathbf{x} = \\mathbf{b} \\] donde \\[ A = \\begin{pmatrix} a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1n} \\\\ a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{m1} &amp; a_{m2} &amp; \\cdots &amp; a_{mn} \\end{pmatrix}, \\quad \\mathbf{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{pmatrix}, \\quad \\mathbf{b} = \\begin{pmatrix} b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_m \\end{pmatrix} \\] Sistema de Cramer Definición. Un sistema de ecuaciones lineales se denomina sistema de Cramer si: La matriz \\(A\\) es cuadrada (\\(m = n\\)) La matriz \\(A\\) es no singular, es decir, \\(|A| \\neq 0\\) En este caso, el sistema tiene una única solución dada por: \\[ \\mathbf{x} = A^{-1} \\mathbf{b} \\] Ejemplo numérico Considere el sistema: \\[ \\begin{aligned} 12x_1 + 20x_2 &amp;= 388 \\\\ 4x_1 + 17x_2 &amp;= 212 \\end{aligned} \\] En forma matricial: \\[ \\begin{pmatrix} 12 &amp; 20 \\\\ 4 &amp; 17 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} 388 \\\\ 212 \\end{pmatrix} \\] Paso 1: Calcular la inversa de \\(A\\) Primero calculamos el determinante: \\[ |A| = 12 \\cdot 17 - 20 \\cdot 4 = 204 - 80 = 124 \\] Luego, la matriz de cofactores traspuesta (adjunta): \\[ \\text{adj}(A) = \\begin{pmatrix} 17 &amp; -20 \\\\ -4 &amp; 12 \\end{pmatrix} \\] Entonces, \\[ A^{-1} = \\frac{1}{124} \\begin{pmatrix} 17 &amp; -20 \\\\ -4 &amp; 12 \\end{pmatrix} \\] Paso 2: Multiplicar \\(A^{-1} \\mathbf{b}\\) \\[ \\mathbf{x} = A^{-1} \\mathbf{b} = \\frac{1}{124} \\begin{pmatrix} 17 &amp; -20 \\\\ -4 &amp; 12 \\end{pmatrix} \\begin{pmatrix} 388 \\\\ 212 \\end{pmatrix} \\] Calculamos el producto: \\[ \\begin{aligned} x_1 &amp;= \\frac{1}{124}(17 \\cdot 388 - 20 \\cdot 212) = \\frac{1}{124}(6596 - 4240) = \\frac{2356}{124} = 19 \\\\ x_2 &amp;= \\frac{1}{124}(-4 \\cdot 388 + 12 \\cdot 212) = \\frac{1}{124}(-1552 + 2544) = \\frac{992}{124} = 8 \\end{aligned} \\] Solución final: \\[ \\boxed{ x_1 = 19, \\quad x_2 = 8 } \\] Este procedimiento es válido siempre que la matriz \\(A\\) sea cuadrada y su determinante no sea cero. Si \\(|A| = 0\\), el sistema no tiene solución única: puede ser incompatible o tener infinitas soluciones. Matrices cuadradas especiales Las siguientes matrices cuadradas tienen propiedades estructurales claves que facilitan el desarrollo de métodos econométricos. 1. Matriz diagonal Una matriz diagonal \\(A = [a_{ij}] \\in \\mathbb{R}^{m \\times m}\\) tiene ceros fuera de la diagonal principal: \\[ A = \\begin{pmatrix} a_{11} &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; a_{22} &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; a_{mm} \\end{pmatrix} = \\text{diag}(a_{11}, a_{22}, \\ldots, a_{mm}) \\] 2. Matriz identidad La matriz identidad \\(I_m\\) es una matriz diagonal con unos en la diagonal: \\[ I_m = \\begin{pmatrix} 1 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; 1 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; 1 \\end{pmatrix} \\] Propiedades: \\(AI_m = I_mA = A\\), \\(I_m^{-1} = I_m\\) 3. Matriz escalar Una matriz escalar es una matriz diagonal cuyos elementos en la diagonal son iguales a un mismo número \\(\\lambda\\): \\[ A = \\lambda I_m \\] 4. Matriz triangular inferior Una matriz triangular inferior cumple: \\[ a_{ij} = 0 \\quad \\text{para todo } i &lt; j \\] \\[ A = \\begin{pmatrix} a_{11} &amp; 0 &amp; \\cdots &amp; 0 \\\\ a_{21} &amp; a_{22} &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{m1} &amp; a_{m2} &amp; \\cdots &amp; a_{mm} \\end{pmatrix} \\] 5. Matriz nula La matriz nula tiene todos sus elementos iguales a cero: \\[ 0 = \\begin{pmatrix} 0 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; 0 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; 0 \\end{pmatrix} \\] 6. Matriz simétrica Una matriz \\(A \\in \\mathbb{R}^{m \\times m}\\) es simétrica si: \\[ A = A&#39; \\quad \\text{es decir, } a_{ij} = a_{ji} \\] 7. Matriz idempotente Una matriz \\(A\\) es idempotente si: \\[ A^2 = A \\] Ejemplo clave: la matriz de proyección sobre el espacio generado por las columnas de \\(X\\): \\[ P = X(X&#39;X)^{-1}X&#39; \\] Cumple: \\(P = P&#39;\\) (simétrica) \\(P^2 = P\\) (idempotente) 8. Matriz ortogonal Una matriz \\(Q\\) es ortogonal si: \\[ Q&#39;Q = QQ&#39; = I \\Rightarrow Q^{-1} = Q&#39; \\] Sus columnas (y filas) son vectores ortonormales. 9. Matrices de proyección: \\(P\\) y \\(M\\) En regresión lineal, dos matrices juegan un rol fundamental: a) Matriz de proyección sobre el espacio columna de \\(X\\): \\[ P = X(X&#39;X)^{-1}X&#39; \\] Idempotente: \\(P^2 = P\\) Simétrica: \\(P&#39; = P\\) Proyecta cualquier vector \\(y\\) sobre el espacio generado por las columnas de \\(X\\): \\(\\hat{y} = P y\\) b) Matriz de aniquilación o proyección ortogonal: \\[ M = I - P \\] Idempotente: \\(M^2 = M\\) Simétrica: \\(M&#39; = M\\) Proyecta sobre el complemento ortogonal del espacio generado por \\(X\\): \\(e = M y\\) (residuos) Estas matrices son centrales para expresar la descomposición: \\[ y = \\hat{y} + e = P y + M y \\] donde \\(\\hat{y}\\) es la parte explicada por \\(X\\), y \\(e\\) es la parte no explicada (residuos). Derivadas de una función multidimensional Derivadas de una forma lineal Sea la forma lineal \\(\\mathbf{a}&#39;\\mathbf{x} = a_1x_1 + a_2x_2 + \\cdots + a_nx_n\\), una función escalar de \\(n\\) variables independientes \\(x_1, \\ldots, x_n\\). La derivada parcial con respecto a una variable \\(x_i\\) es simplemente: \\[ \\frac{\\partial \\mathbf{a}&#39;\\mathbf{x}}{\\partial x_i} = a_i \\] La derivada de \\(\\mathbf{a}&#39;\\mathbf{x}\\) con respecto al vector \\(\\mathbf{x}\\) es: \\[ \\frac{\\partial \\mathbf{a}&#39;\\mathbf{x}}{\\partial \\mathbf{x}} = \\begin{pmatrix} \\frac{\\partial \\mathbf{a}&#39;\\mathbf{x}}{\\partial x_1} \\\\ \\frac{\\partial \\mathbf{a}&#39;\\mathbf{x}}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial \\mathbf{a}&#39;\\mathbf{x}}{\\partial x_n} \\end{pmatrix} = \\begin{pmatrix} a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_n \\end{pmatrix} = \\mathbf{a} \\] De forma análoga, la derivada de \\(\\mathbf{a}&#39;\\mathbf{x}\\) respecto de \\(\\mathbf{x}&#39;\\) es un vector fila: \\[ \\frac{\\partial \\mathbf{a}&#39;\\mathbf{x}}{\\partial \\mathbf{x}&#39;} = \\begin{pmatrix} a_1 &amp; a_2 &amp; \\cdots &amp; a_n \\end{pmatrix} = \\mathbf{a}&#39; \\] Derivadas de una forma cuadrática Sea la forma cuadrática \\(\\mathbf{x}&#39;A\\mathbf{x}\\), donde \\(A\\) es una matriz simétrica. Esta puede escribirse como: \\[ \\mathbf{x}&#39;A\\mathbf{x} = \\sum_{i=1}^{n} a_{ii}x_i^2 + 2\\sum_{i=1}^{n-1}\\sum_{j=i+1}^{n} a_{ij}x_ix_j \\] La derivada de \\(\\mathbf{x}&#39;A\\mathbf{x}\\) con respecto al vector \\(\\mathbf{x}\\) es: \\[ \\frac{\\partial (\\mathbf{x}&#39;A\\mathbf{x})}{\\partial \\mathbf{x}} = 2A\\mathbf{x} \\] Esto es, un vector columna cuya i-ésima componente es: \\[ \\frac{\\partial (\\mathbf{x}&#39;A\\mathbf{x})}{\\partial x_i} = 2(a_{i1}x_1 + a_{i2}x_2 + \\cdots + a_{in}x_n) \\] Derivadas de segundo orden (matriz Hessiana) La derivada segunda de \\(\\mathbf{x}&#39;A\\mathbf{x}\\) con respecto a \\(x_i\\) es: \\[ \\frac{\\partial^2 (\\mathbf{x}&#39;A\\mathbf{x})}{\\partial x_i^2} = 2a_{ii} \\] La derivada mixta con respecto a \\(x_i\\) y \\(x_j\\) es: \\[ \\frac{\\partial^2 (\\mathbf{x}&#39;A\\mathbf{x})}{\\partial x_i \\partial x_j} = 2a_{ij} \\] La matriz de segundas derivadas (Hessiana) es: \\[ \\frac{\\partial^2 (\\mathbf{x}&#39;A\\mathbf{x})}{\\partial \\mathbf{x} \\partial \\mathbf{x}&#39;} = 2A \\] Resumen Derivada de forma lineal: \\(\\frac{\\partial (\\mathbf{a}&#39;\\mathbf{x})}{\\partial \\mathbf{x}} = \\mathbf{a}\\) Derivada de forma cuadrática: \\(\\frac{\\partial (\\mathbf{x}&#39;A\\mathbf{x})}{\\partial \\mathbf{x}} = 2A\\mathbf{x}\\) Matriz Hessiana: \\(\\frac{\\partial^2 (\\mathbf{x}&#39;A\\mathbf{x})}{\\partial \\mathbf{x} \\partial \\mathbf{x}&#39;} = 2A\\) 📘 Preguntas de repaso Sea \\(\\mathbf{i} = (1, 1, \\ldots, 1)&#39;\\) un vector \\(m \\times 1\\) de unos. Calcule \\(\\mathbf{i}&#39;\\mathbf{i}\\). Sean \\(\\mathbf{i} = (1, \\ldots, 1)&#39;\\) y \\(\\mathbf{y} = (y_1, \\ldots, y_m)&#39;\\). Calcule \\(\\mathbf{i}&#39;\\mathbf{y}\\). Demuestre que la media de las observaciones \\(y_1, \\ldots, y_m\\) puede expresarse como: "],["supuestos-de-mco.html", "4 Supuestos de MCO Proceso Generador de Datos Tabla Resumen de Supuestos S1. Linealidad en los Parámetros S2. Exogeneidad Estricta S3. Colinealidad Imperfecta S4. Perturbaciones Esféricas S5. Regresores No Estocásticos S6. Normalidad del Error Glosario de Símbolos 📘 Preguntas de repaso", " 4 Supuestos de MCO Proceso Generador de Datos El modelo de regresión lineal parte de la siguiente estructura: \\[ Y_i = X_i \\beta + \\epsilon_i \\] Donde: - \\(Y_i\\): variable dependiente (observación i) - \\(X_i\\): vector fila con los regresores de la observación i - \\(\\beta\\): vector de parámetros poblacionales - \\(\\epsilon_i\\): error poblacional (componentes no observables) - \\(i = 1, 2, ..., n\\) Esta formulación describe el proceso generador de datos (PGD), base para los supuestos del MCO. Tabla Resumen de Supuestos Supuesto Notación Implicación principal S1. Linealidad en los parámetros \\(y_i = X_i \\beta + \\epsilon_i\\) El modelo es lineal en los parámetros S2. Exogeneidad estricta \\(\\mathbb{E}[\\epsilon_i \\mid X] = 0\\) No hay correlación entre regresores y error S3. Colinealidad imperfecta \\(\\text{Rango}(X) = K\\) No hay multicolinealidad perfecta; modelo identificable S4. Perturbaciones esféricas \\(\\text{Var}(\\epsilon_i \\mid X) = \\sigma^2\\), \\(\\text{Cov}(\\epsilon_i, \\epsilon_j \\mid X) = 0\\) Homocedasticidad y no autocorrelación S5. Regresores no estocásticos \\(X\\) es fija en repetidas muestras Simplifica demostraciones teóricas S6. Normalidad \\(\\epsilon \\mid X \\sim \\mathcal{N}(0, \\sigma^2 I)\\) Solo necesaria para inferencia exacta S1. Linealidad en los Parámetros El valor esperado de \\(y\\) está relacionado linealmente con los regresores: \\[ \\mathbb{E}[Y_i \\mid X_i] = X_i \\beta \\] Esto permite distintas formas funcionales (lineales en parámetros): Lineal: \\(y_i = \\beta_1 + \\beta_2 x_i + \\epsilon_i\\) Log-log: \\(\\log(y_i) = \\beta_1 + \\beta_2 \\log(x_i) + \\epsilon_i\\) Log-lineal: \\(\\log(y_i) = \\beta_1 + \\beta_2 x_i + \\epsilon_i\\) Lineal-log: \\(y_i = \\beta_1 + \\beta_2 \\log(x_i) + \\epsilon_i\\) Cuadrático: \\(y_i = \\beta_1 + \\beta_2 x_i + \\beta_3 x_i^2 + \\epsilon_i\\) Interactuado: \\(y_i = \\beta_1 + \\beta_2 x_1 + \\beta_3 x_2 + \\beta_4(x_1 x_2) + \\epsilon_i\\) S2. Exogeneidad Estricta \\[ \\mathbb{E}[\\epsilon_i \\mid X] = 0 \\] Esto implica que no existe relación sistemática entre los regresores y el término de error. Ejemplos: \\(\\mathbb{E}[u \\mid X = 1] = 0\\) \\(\\mathbb{E}[u \\mid X_2 = \\text{Mujer}] = 0\\) Demostración (Ley de la esperanza iterada): \\[ \\mathbb{E}[\\epsilon_i] = \\mathbb{E}\\left[ \\mathbb{E}[\\epsilon_i \\mid X] \\right] = \\mathbb{E}[0] = 0 \\] Equivalencia: Si \\(\\mathbb{E}[\\epsilon_i \\mid X] = 0\\), entonces: \\[ \\text{Cov}(\\epsilon_i, X_j) = 0 \\quad \\forall j \\] Pero qué quiere decir? Una forma de pensar en esta definición es: Para cualquier valor de \\(X\\), el valor esperado de los residuos debe ser igual a cero E.g., \\(\\mathop{E}\\left[ u \\mid X=1 \\right]=0\\) and \\(\\mathop{E}\\left[ u \\mid X=100 \\right]=0\\) E.g., \\(\\mathop{E}\\left[ u \\mid X_2=\\text{Mujer} \\right]=0\\) and \\(\\mathop{E}\\left[ u \\mid X_2=\\text{Hombre} \\right]=0\\) Note: \\(\\mathop{E}\\left[ u \\mid X \\right]=0\\) es más restrictivo que \\(\\mathop{E}\\left[ u \\right]=0\\) Graficamente… Exogeneidad Estricta se Incumple, i.e., \\(\\mathop{E}\\left[ \\epsilon \\mid X \\right] \\neq 0\\) S3. Colinealidad Imperfecta \\[ \\text{Rango}(X) = K \\] Para que el modelo esté identificado, debe cumplirse que el número de observaciones sea mayor que el número de regresores: \\(n &gt; K\\). Violaciones comunes: Regresor constante: \\(X_j = c\\) Dos variables idénticas: \\(X_j = X_k\\) Combinación lineal exacta: \\(X_3 = X_1 + X_2\\) Trampa de las variables binarias Ejemplo de matriz con rango 3: \\[ A = \\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 3 &amp; 5 &amp; 7 \\\\ 4 &amp; 6 &amp; 5 \\\\ \\end{bmatrix} \\quad \\Rightarrow \\text{Rango}(A) = 3 \\] Ejemplo de matriz con rango menor a 3: \\[ B = \\begin{bmatrix} 1 &amp; 3 &amp; 1 \\\\ 3 &amp; 8 &amp; 2 \\\\ 2 &amp; 9 &amp; 5 \\\\ \\end{bmatrix} \\quad \\Rightarrow \\text{Rango}(B) \\neq 3 \\] La tercera columna de \\(B\\) es combinación lineal de las otras dos: \\(C_3 = -2 \\cdot C_1 + C_2\\) Wooldridge (2003) aclara que este supuesto permite que los regresores estén correlacionados, siempre que no haya una relación lineal exacta entre ellos. S4. Perturbaciones Esféricas Este supuesto se compone de dos condiciones: 🔹 Homocedasticidad \\[ \\text{Var}(\\epsilon_i \\mid X) = \\sigma^2 \\quad \\forall i \\] La dispersión del término de error es constante para todos los individuos. Esto significa que la varianza de los errores no depende de los regresores. 🔹 No autocorrelación \\[ \\text{Cov}(\\epsilon_i, \\epsilon_j \\mid X) = 0 \\quad \\text{para } i \\neq j \\] Los errores no están correlacionados entre sí. Es especialmente relevante en series de tiempo, pero también puede violarse en datos de corte transversal (e.g., por correlación espacial). 🔸 Implicación conjunta Cuando se cumplen homocedasticidad y no autocorrelación: \\[ \\text{Var}(\\epsilon \\mid X) = \\sigma^2 I \\] La matriz de varianzas-covarianzas de los errores es escalar y diagonal, también llamada matriz esférica. 🧠 Derivación paso a paso {-} \\[ \\text{Var}(\\epsilon \\mid X) = \\mathbb{E}[\\epsilon \\epsilon&#39; \\mid X] - \\mathbb{E}[\\epsilon \\mid X] \\cdot \\mathbb{E}[\\epsilon&#39; \\mid X] \\] Por el supuesto de exogeneidad estricta (S2), sabemos que: \\[ \\mathbb{E}[\\epsilon \\mid X] = 0 \\quad \\Rightarrow \\quad \\text{Var}(\\epsilon \\mid X) = \\mathbb{E}[\\epsilon \\epsilon&#39; \\mid X] \\] Entonces, la matriz resultante es: \\[ \\text{Var}(\\epsilon \\mid X) = \\begin{bmatrix} \\mathbb{E}[\\epsilon_1^2 \\mid X] &amp; \\mathbb{E}[\\epsilon_1 \\epsilon_2 \\mid X] &amp; \\cdots &amp; \\mathbb{E}[\\epsilon_1 \\epsilon_n \\mid X] \\\\ \\mathbb{E}[\\epsilon_2 \\epsilon_1 \\mid X] &amp; \\mathbb{E}[\\epsilon_2^2 \\mid X] &amp; \\cdots &amp; \\mathbb{E}[\\epsilon_2 \\epsilon_n \\mid X] \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\mathbb{E}[\\epsilon_n \\epsilon_1 \\mid X] &amp; \\mathbb{E}[\\epsilon_n \\epsilon_2 \\mid X] &amp; \\cdots &amp; \\mathbb{E}[\\epsilon_n^2 \\mid X] \\end{bmatrix} \\] Aplicando los supuestos: \\(\\text{Var}(\\epsilon_i \\mid X) = \\sigma^2\\) \\(\\text{Cov}(\\epsilon_i, \\epsilon_j \\mid X) = 0\\) para \\(i \\neq j\\) \\[ \\Rightarrow \\text{Var}(\\epsilon \\mid X) = \\begin{bmatrix} \\sigma^2 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\sigma^2 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; \\sigma^2 \\end{bmatrix} = \\sigma^2 I \\] Este supuesto es necesario para garantizar la eficiencia del estimador MCO bajo los supuestos clásicos (Teorema de Gauss-Markov). S5. Regresores No Estocásticos Este supuesto establece que la matriz de regresores \\(X\\) no es aleatoria: sus valores permanecen fijos en repeticiones del experimento o entre muestras. \\[ X = \\text{constante} \\quad \\text{(no varía entre muestras)} \\] 🔹 ¿Qué significa? Aunque en la práctica \\(X\\) proviene de una muestra aleatoria, asumir que es no estocástica permite tratarlo como fijo en la teoría. Esto implica que cualquier inferencia o estimación se condiciona sobre \\(X\\). ✅ Ventajas teóricas Simplifica la demostración de propiedades como insesgamiento y varianza mínima. Permite eliminar la distinción entre: valor esperado condicional: \\(\\mathbb{E}[\\hat{\\beta} \\mid X]\\) y valor esperado incondicional: \\(\\mathbb{E}[\\hat{\\beta}]\\) ⚠️ En la práctica… Este supuesto rara vez se cumple literalmente, ya que \\(X\\) normalmente proviene de una muestra aleatoria. Sin embargo, es común en teoría clásica porque: No afecta la validez del MCO si se asume que \\(X\\) es independiente de \\(\\epsilon\\). Se puede relajar en contextos de modelos más generales (paneles, variables instrumentales, etc.). En modelos con regresores estocásticos, se requiere en cambio que \\(\\mathbb{E}[\\epsilon \\mid X] = 0\\), lo que recupera el supuesto de exogeneidad estricta (S2). S6. Normalidad del Error \\[ \\epsilon \\mid X \\sim \\mathcal{N}(0, \\sigma^2 I) \\] Este supuesto establece que los errores, condicionales a los regresores, siguen una distribución normal multivariada con media cero y matriz de varianza-covarianza esférica \\(\\sigma^2 I\\). 🎯 ¿Para qué sirve? Este supuesto no es necesario para que el estimador de Mínimos Cuadrados Ordinarios (MCO) sea: Insesgado (S2 ya garantiza eso), Eficiente entre estimadores lineales insesgados (por el Teorema de Gauss-Markov). Sin embargo, sí es crucial para que se cumpla la distribución exacta de ciertos estadísticos en muestras pequeñas. ✅ Aplicaciones de la normalidad: Validez de las pruebas t para significancia individual. Validez de las pruebas F para restricciones conjuntas. Construcción exacta de intervalos de confianza para \\(\\beta\\). 🧠 ¿Qué pasa en muestras grandes? Gracias al Teorema Central del Límite y **La Ley de los Grandes Números*, incluso si \\(\\epsilon\\) no es normal, el estimador \\(\\hat{\\beta}\\) tenderá a seguir una distribución normal asintótica: \\[ \\hat{\\beta} \\overset{approx}{\\sim} \\mathcal{N}\\left(\\beta, \\sigma^2 (X&#39;X)^{-1}\\right) \\] Por eso, la normalidad puede relajarse si \\(n\\) es suficientemente grande. Glosario de Símbolos Símbolo Significado \\(Y_i\\) Variable dependiente \\(X_{ij}\\) Regresor j para observación i \\(\\beta_j\\) Parámetro poblacional \\(\\epsilon_i\\) Error poblacional \\(n\\) Número de observaciones \\(k\\) Número de regresores (sin constante) 📘 Preguntas de repaso 📘 1. Conceptuales Defina brevemente los siguientes términos: Econometría teórica Econometría aplicada ¿Qué papel juega cada uno de los seis supuestos del modelo clásico de regresión lineal en garantizar las propiedades del estimador de MCO? 🧮 2. Clasificación de modelos Clasifique los siguientes modelos como lineales en parámetros o no lineales: \\(y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\\) \\(\\log(y_i) = \\beta_0 + \\beta_1 \\log(x_i) + \\epsilon_i\\) \\(y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\epsilon_i\\) \\(y_i = \\frac{\\beta_0}{1 + e^{-\\beta_1 x_i}} + \\epsilon_i\\) \\(y_i = \\alpha + \\theta^{x_i} + \\epsilon_i\\) 📏 3. Interpretación de la pendiente Interprete el coeficiente \\(\\beta_1\\) en los siguientes modelos de regresión lineal simple: \\(y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\\) \\(\\log(y_i) = \\beta_0 + \\beta_1 x_i + \\epsilon_i\\) \\(y_i = \\beta_0 + \\beta_1 \\log(x_i) + \\epsilon_i\\) \\(\\log(y_i) = \\beta_0 + \\beta_1 \\log(x_i) + \\epsilon_i\\) En cada caso, explique qué representa un aumento marginal en \\(x_i\\), y si los efectos son absolutos, porcentuales o elásticos. 🎥 Recursos audiovisuales ¿Qué hacen los economistas? (Video 1) An Uneven Paying Field (Video 2) "],["regresión-por-mínimos-cuadrados-ordinarios-mco.html", "5 Regresión por Mínimos Cuadrados Ordinarios (MCO) Modelo ¿Qué hace MCO? ¿Cómo se encuentra el vector \\(\\hat{\\beta}\\)? ¿Es un mínimo? Interpretación en términos de contraparte muestral Supuestos clave empleados hasta acá Diferencia entre la regresión simple y la regresión múltiple Apéndice Regresión múltiple Estimador de MCO en R empleando matrices Estimador de MCO en Stata usando MATA Estimador de MCO en Python usando NumPy", " 5 Regresión por Mínimos Cuadrados Ordinarios (MCO) Modelo Antes de lanzarnos a minimizar errores, hagamos una pausa para ver cómo funciona esto con álgebra lineal, que es el corazón de MCO. Tenemos una regresión con n observaciones y k variables explicativas (incluyendo la constante). Entonces: \\(y\\): vector de resultados observados (\\(n \\times 1\\)) \\(X\\): matriz de variables explicativas (\\(n \\times k\\)) \\(\\beta\\): vector de coeficientes (\\(k \\times 1\\)) \\(\\varepsilon\\): vector de errores aleatorios (\\(n \\times 1\\)) El modelo lineal es: \\[ y = X\\beta + \\varepsilon \\] En notación matricial, esto se expresa como: \\[ \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix} = \\begin{bmatrix} 1 &amp; x_{11} &amp; \\cdots &amp; x_{1k} \\\\ 1 &amp; x_{21} &amp; \\cdots &amp; x_{2k} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 1 &amp; x_{n1} &amp; \\cdots &amp; x_{nk} \\end{bmatrix} \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_k \\end{bmatrix} + \\begin{bmatrix} \\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\vdots \\\\ \\varepsilon_n \\end{bmatrix} \\] ¿Qué hace MCO? La regresión por Mínimos Cuadrados Ordinarios (MCO) es una de las herramientas más fundamentales y utilizadas en econometría. Su objetivo es simple pero poderoso: encontrar la mejor recta (o hiperplano) que se ajusta a los datos observados, minimizando los errores que cometemos al predecir. ¿Qué quiere decir “minimizar errores”? Supongamos que tenemos una relación entre una variable dependiente \\(y\\) y un conjunto de variables explicativas \\(X\\). Queremos encontrar un vector de parámetros \\(\\beta\\) que explique esa relación lo mejor posible. El problema es que los datos reales no siguen perfectamente la recta que proponemos. Siempre hay un error o residuo que capta lo que no podemos explicar. Pero aunque no conocemos los verdaderos errores (denotados por \\(\\varepsilon\\)), sí podemos calcular cuánto nos estamos equivocando si asumimos un valor hipotético de \\(\\beta\\), al que llamaremos \\(\\tilde{\\beta}\\). El error estimado para cada observación \\(i\\) sería: \\[ \\tilde{\\epsilon}_i = y_i - X_i \\tilde{\\beta} \\] Y si elevamos estos errores al cuadrado (para que los negativos no se cancelen con los positivos) y los sumamos para todas las observaciones, obtenemos lo que llamamos la Suma de los Residuos al Cuadrado (SRC) es: \\[ SRC(\\tilde{\\beta}) = \\sum_{i=1}^n (y_i - X_i \\tilde{\\beta})^2 \\] En notación matricial: \\[ SRC(\\tilde{\\beta}) = (y - X\\tilde{\\beta})&#39;(y - X\\tilde{\\beta}) \\] La idea de MCO es encontrar un valor de \\(\\tilde{\\beta}\\) que minimice esta suma de residuos al cuadrado. En otras palabras, queremos encontrar el mejor ajuste posible a nuestros datos. En la siguiente grafica veremos cómo se ve esta función objetivo y cómo el valor óptimo de \\(\\tilde{\\beta}\\) se encuentra en el mínimo de la curva. Cuando tenemos más de una variable explicativa, MCO sigue buscando el punto que minimiza la suma de los residuos al cuadrado (SRC), pero ahora en una superficie 3D. En el gráfico a continuación, mostramos cómo la SRC varía con diferentes combinaciones de valores hipotéticos para \\(\\tilde{\\beta}_1\\) y \\(\\tilde{\\beta}_2\\). La superficie muestra los errores para cada combinación, y el mínimo (el punto más bajo) representa los valores estimados \\((\\hat{\\beta}_1, \\hat{\\beta}_2)\\). ¿Cómo se encuentra el vector \\(\\hat{\\beta}\\)? Queremos minimizar la SRC: \\[ \\hat{\\beta} = \\underset{\\tilde{\\beta}}{\\arg\\min} \\; (y - X\\tilde{\\beta})&#39;(y - X\\tilde{\\beta}) \\] Expandiendo: \\[ SRC(\\tilde{\\beta}) = y&#39;y - 2\\tilde{\\beta}&#39;X&#39;y + \\tilde{\\beta}&#39;X&#39;X\\tilde{\\beta} \\] Para minimizar esta función deerivamos con respecto a \\(\\tilde{\\beta}\\): \\[ \\frac{\\partial SRC}{\\partial \\tilde{\\beta}} = -2X&#39;y + 2X&#39;X\\tilde{\\beta} = 0 \\] Reordenando, llegamos a las Ecuaciones normales de MCO: \\[ X&#39;X \\hat{\\beta} = X&#39;y \\] Si la matriz \\(X&#39;X\\) es invertible (lo que requiere, por ejemplo, que no haya multicolinealidad perfecta), entonces podemos despejar \\(\\hat{\\beta}\\) directamente \\[ \\hat{\\beta} = (X&#39;X)^{-1}X&#39;y \\] ¿Es un mínimo? Sí, y lo podemos verificar con la segunda derivada (matriz Hessiana) es: \\[ \\frac{\\partial^2 SRC}{\\partial \\tilde{\\beta} \\partial \\tilde{\\beta}&#39;} = 2X&#39;X \\] Como \\(X&#39;X\\) es semidefinida positiva (o positiva definida si no hay colinealidad), eso nos garantiza que el punto que encontramos con la primera derivada es en efecto un mínimo. 🧠 Recordatorio: Para funciones cuadráticas, si la matriz que acompaña al término cuadrático (en este caso \\(X&#39;X\\)) es positiva definida, entonces el punto crítico que encontramos resolviendo la condición de primer orden es un mínimo global. 🧠 ¿Qué significa que una matriz sea semidefinida positiva? Una matriz simétrica \\(A\\) es: Positiva semidefinida (PSD) si para cualquier vector no nulo \\(z\\), se cumple: \\[ z&#39;Az \\geq 0 \\] Positiva definida (PD) si para cualquier vector no nulo \\(z\\), se cumple: \\[ z&#39;Az &gt; 0 \\] En el contexto de mínimos cuadrados, la matriz \\(X&#39;X\\) es siempre simétrica y al menos semidefinida positiva. Será positiva definida si las columnas de \\(X\\) no tienen colinealidad perfecta (es decir, no hay combinación lineal exacta entre las variables). ¿Y por qué importa esto? Porque si una función cuadrática tiene la forma: \\[ f(\\beta) = \\beta&#39;A\\beta + b&#39;\\beta + c \\] entonces \\(f(\\beta)\\) tiene un mínimo en el punto donde su derivada es cero si \\(A\\) es positiva definida (o semidefinida positiva en algunos casos). Esto es justo lo que ocurre en MCO. 📌 En otras palabras: el hecho de que \\(X&#39;X\\) sea positiva definida es lo que garantiza que el estimador \\(\\hat{\\beta}\\) minimiza la suma de los residuos al cuadrado, y no la maximiza o genera un punto de silla. Interpretación en términos de contraparte muestral Podemos escribir: \\[ \\hat{\\beta} = \\left( \\frac{X&#39;X}{n} \\right)^{-1} \\left( \\frac{X&#39;y}{n} \\right) \\] Esto sugiere que estamos usando promedios muestrales. Bajo ciertas condiciones: \\[ \\hat{\\beta} \\to E[X&#39;X]^{-1}E[X&#39;y] \\] Supuestos clave empleados hasta acá S1: Linealidad en los parámetros S2: \\(E[\\varepsilon_i \\mid X_i] = 0\\) S3: \\(X&#39;X\\) invertible (no colinealidad perfecta) Diferencia entre la regresión simple y la regresión múltiple Vamos a ver diferentes casos de regresión, desde la más simple hasta la más compleja, para entender la anatomía de la regresión y cómo MCO se adapta a cada uno. Regresión sin variables explicativas En este caso, el modelo es simplemente: \\[ y = \\beta + \\varepsilon \\] Aquí, \\(\\beta\\) es el único parámetro a estimar. El estimador de MCO es simplemente el promedio de \\(y\\): \\[ \\hat{\\beta} = \\frac{1}{n} \\sum_{i=1}^{n} y_i \\] Regresión simple con una variable explicativa En este caso, el modelo es: \\[ y = \\beta_0 + \\beta_1 x + \\varepsilon \\] Aquí, tenemos dos parámetros a estimar: \\(\\beta_0\\) (intercepto) y \\(\\beta_1\\) (pendiente). El estimador de MCO se calcula como: \\[ \\hat{\\beta} = (X&#39;X)^{-1}X&#39;y \\] 📝 Pausa Demostrar que el estimador de MCO para \\(\\beta_0\\) y \\(\\beta_1\\) en la regresión simple son: \\[ \\hat{\\beta}_1 = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2} \\] \\[ \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x} \\] Debes partir de la formulación matricial de MCO y aplicar álgebra paso a paso. 💪 Recuerda las siguientes propiedades de la sumatoria Promedio y suma Si \\[ \\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} X_i \\quad \\Rightarrow \\quad n \\bar{X} = \\sum_{i=1}^{n} X_i \\] Suma de cuadrados centrados \\[ \\sum_{i=1}^{n} (X_i - \\bar{X})^2 = \\sum_{i=1}^{n} X_i^2 - n \\bar{X}^2 \\] Demostración: \\[ \\sum_{i=1}^{n} (X_i - \\bar{X})^2 = \\sum X_i^2 - 2 \\bar{X} \\sum X_i + n \\bar{X}^2 \\] \\[ = \\sum X_i^2 - 2n \\bar{X}^2 + n \\bar{X}^2 = \\sum X_i^2 - n \\bar{X}^2 \\] Covarianza muestral \\[ \\sum_{i=1}^{n} (X_i - \\bar{X})(Y_i - \\bar{Y}) = \\sum X_i (Y_i - \\bar{Y}) - \\bar{X} \\sum (Y_i - \\bar{Y}) \\] \\[ = \\sum X_i Y_i - \\bar{Y} \\sum X_i - \\bar{X} \\cdot 0 = \\sum X_i Y_i - n \\bar{Y} \\bar{X} = \\sum X_i (Y_i - \\bar{Y}) \\] Regresión múltiple con múltiples variables explicativas En este caso, el modelo es: \\[ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_k x_k + \\varepsilon \\] Aquí, tenemos \\(k+1\\) parámetros a estimar. El estimador de MCO sigue siendo: \\[ \\hat{\\beta} = (X&#39;X)^{-1}X&#39;y \\] En el siguiente capítulo entenederemos mejor la anatomía de la regresión múltiple para entender perfectamente ¿cuál es la diferencia entre la regresión simple y la múltiple? Apéndice Álgebra… mucha álgebra Partimos del modelo de regresión simple en forma matricial: \\[ y = X\\beta + \\varepsilon \\] Donde: \\(y\\) es un vector columna de dimensión \\(n \\times 1\\), \\(X\\) es una matriz \\(n \\times 2\\), con una columna de unos y una columna con la variable explicativa \\(X_1\\), es decir: \\(X = [1 \\;\\; X_1]\\), \\(\\varepsilon\\) es el vector de perturbaciones, \\(\\beta\\) es un vector \\(2 \\times 1\\) con los parámetros a estimar. El estimador de MCO es: \\[ \\begin{bmatrix} \\hat{\\beta}_0 \\\\ \\hat{\\beta}_1 \\end{bmatrix} = \\underbrace{(X&#39;X)^{-1}}_a \\cdot \\underbrace{X&#39;y}_b \\] Cálculo de \\(X&#39;X\\) \\[ X&#39;X = \\begin{bmatrix} 1 &amp; 1 &amp; \\cdots &amp; 1 \\\\ X_{11} &amp; X_{12} &amp; \\cdots &amp; X_{1n} \\end{bmatrix} \\begin{bmatrix} 1 &amp; X_{11} \\\\ 1 &amp; X_{12} \\\\ \\vdots &amp; \\vdots \\\\ 1 &amp; X_{1n} \\end{bmatrix} = \\begin{bmatrix} n &amp; \\sum X_{1i} \\\\ \\sum X_{1i} &amp; \\sum X_{1i}^2 \\end{bmatrix} \\] Su inversa es: \\[ (X&#39;X)^{-1} = \\frac{1}{n \\sum X_{1i}^2 - (\\sum X_{1i})^2} \\begin{bmatrix} \\sum X_{1i}^2 &amp; -\\sum X_{1i} \\\\ -\\sum X_{1i} &amp; n \\end{bmatrix} \\] 5.0.1 Cálculo de \\(X&#39;y\\) \\[ X&#39;y = \\begin{bmatrix} 1 &amp; 1 &amp; \\cdots &amp; 1 \\\\ X_{11} &amp; X_{12} &amp; \\cdots &amp; X_{1n} \\end{bmatrix} \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix} = \\begin{bmatrix} \\sum y_i \\\\ \\sum X_{1i} y_i \\end{bmatrix} \\] Producto final Sustituyendo en la fórmula del estimador: \\[ \\begin{aligned} \\begin{bmatrix} \\hat{\\beta}_0 \\\\ \\hat{\\beta}_1 \\end{bmatrix} &amp;= \\frac{1}{n \\sum X_{1i}^2 - (\\sum X_{1i})^2} \\begin{bmatrix} \\sum X_{1i}^2 &amp; -\\sum X_{1i} \\\\ -\\sum X_{1i} &amp; n \\end{bmatrix} \\begin{bmatrix} \\sum y_i \\\\ \\sum X_{1i} y_i \\end{bmatrix} \\\\ &amp;= \\frac{1}{n \\sum X_{1i}^2 - (\\sum X_{1i})^2} \\begin{bmatrix} \\sum X_{1i}^2 \\sum y_i - \\sum X_{1i} \\sum X_{1i} y_i \\\\ - \\sum X_{1i} \\sum y_i + n \\sum X_{1i} y_i \\end{bmatrix} \\end{aligned} \\] Estimador de \\(\\hat{\\beta}_1\\) \\[ \\begin{aligned} \\hat{\\beta}_1 &amp;= \\frac{-\\sum X_{1i} \\sum y_i + n \\sum X_{1i} y_i}{n \\sum X_{1i}^2 - (\\sum X_{1i})^2} \\\\ &amp;= \\frac{\\sum (X_{1i} - \\bar{X}_1)(y_i - \\bar{y})}{\\sum (X_{1i} - \\bar{X}_1)^2} \\end{aligned} \\] Estimador de \\(\\hat{\\beta}_0\\) \\[ \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{X}_1 \\] Regresión múltiple La matriz \\(X&#39;X\\) contiene la covarianza entre las variables explicativas. Si las variables no están correlacionadas, no se necesita regresión múltiple. Asumimos: \\(\\sum X_{ji} = 0\\) para todas las variables (centradas en cero) \\(\\sum X_{ji} X_{hi} = 0\\) para todo \\(j \\ne h\\) (no correlación entre regresores) Entonces \\(X&#39;X\\) se convierte en: \\[ X&#39;X = \\begin{bmatrix} n &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\sum X_{1i}^2 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; \\sum X_{ki}^2 \\end{bmatrix} \\] Y su inversa: \\[ (X&#39;X)^{-1} = \\begin{bmatrix} 1/n &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; 1/\\sum X_{1i}^2 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; 1/\\sum X_{ki}^2 \\end{bmatrix} \\] El estimador de MCO en este caso es: \\[ \\hat{\\beta} = \\begin{bmatrix} \\frac{\\sum y}{n} \\\\ \\frac{\\sum X_1 y}{\\sum X_1^2} \\\\ \\vdots \\\\ \\frac{\\sum X_k y}{\\sum X_k^2} \\end{bmatrix} \\] Conclusión: si las variables explicativas están centradas y no están correlacionadas entre sí, entonces el estimador de MCO para cada coeficiente coincide con el estimador de regresión simple. Solo cuando hay correlación entre regresores es necesaria la regresión múltiple para obtener efectos parciales. Estimador de MCO en R empleando matrices # Simular datos set.seed(123) n &lt;- 100 x &lt;- runif(n, 0, 10) epsilon &lt;- rnorm(n, 0, 1.5) y &lt;- 2 + 0.7 * x + epsilon # Crear matrices X &lt;- cbind(1, x) # matriz de diseño con constante Y &lt;- matrix(y, ncol = 1) # Estimador de MCO XtX_inv &lt;- solve(t(X) %*% X) XtY &lt;- t(X) %*% Y b_hat &lt;- XtX_inv %*% XtY # Valores ajustados y residuos Y_hat &lt;- X %*% b_hat e &lt;- Y - Y_hat # Varianza del error y errores estándar n &lt;- nrow(X) k &lt;- ncol(X) s2 &lt;- sum(e^2) / (n - k) var_cov_matrix &lt;- s2 * XtX_inv se &lt;- sqrt(diag(var_cov_matrix)) # Resultados b_hat #&gt; [,1] #&gt; 1.9865602 #&gt; x 0.6865253 se #&gt; x #&gt; 0.29408249 0.05127318 Estimador de MCO en Stata usando MATA clear all set more off sysuse auto, clear * Generamos la constante gen cons = 1 * Entramos a MATA mata // Creamos las matrices X y Y st_view(Y=., ., &quot;price&quot;) st_view(X=., ., (&quot;cons&quot;, &quot;weight&quot;)) n = rows(X) k = cols(X) df = n - k // Estimador de MCO b = invsym(X&#39;X)*X&#39;Y // Valores ajustados y residuos Y_hat = X*b e = Y - Y_hat // Suma de residuos al cuadrado SRC = e&#39;e // Varianza del error s2 = SRC / df s = sqrt(s2) // Matriz de varianzas y covarianzas V = s2 * invsym(X&#39;X) se = sqrt(diagonal(V)) // Resultados b se end * Comparación con comando base reg price weight Estimador de MCO en Python usando NumPy # Estimador de MCO en Python usando matrices (numpy) import numpy as np # Simular datos np.random.seed(123) n = 100 x = np.random.uniform(0, 10, size=n) epsilon = np.random.normal(0, 1.5, size=n) y = 2 + 0.7 * x + epsilon # Crear matriz X con constante X = np.column_stack((np.ones(n), x)) # n x 2 Y = y.reshape(-1, 1) # n x 1 # Estimador MCO: beta_hat = (X&#39;X)^(-1) X&#39;Y XtX_inv = np.linalg.inv(X.T @ X) XtY = X.T @ Y beta_hat = XtX_inv @ XtY # Valores ajustados y residuos Y_hat = X @ beta_hat e = Y - Y_hat # Varianza del error k = X.shape[1] s2 = (e.T @ e) / (n - k) var_cov_matrix = s2[0, 0] * XtX_inv se = np.sqrt(np.diag(var_cov_matrix)) # Resultados print(&quot;Coeficientes (beta_hat):&quot;) print(beta_hat.flatten()) print(&quot;\\nErrores estándar:&quot;) print(se) import matplotlib.pyplot as plt plt.scatter(x, y, label=&#39;Datos&#39;, alpha=0.6) plt.plot(x, Y_hat, color=&#39;red&#39;, label=&#39;Ajuste MCO&#39;) plt.xlabel(&#39;x&#39;) plt.ylabel(&#39;y&#39;) plt.title(&#39;Regresión lineal por MCO&#39;) plt.legend() plt.grid(True) plt.show() 📘 Preguntas de repaso A continuación encontrarás algunas preguntas conceptuales y computacionales para reforzar lo aprendido sobre el estimador de Mínimos Cuadrados Ordinarios (MCO). Puedes intentar resolverlas en papel o programando en R, Stata o Python. ✍️ Interpretación de coeficientes Suponga que ha estimado la siguiente regresión: \\[ \\widehat{\\text{salario}} = \\beta_0 + \\beta_1 \\cdot \\text{educación} \\] donde: salario = salario por hora (en pesos) educación = años de educación completados Para cada uno de los siguientes coeficientes estimados, interprete el resultado en términos económicos: \\(\\hat{\\beta}_1 = 2000\\) \\(\\hat{\\beta}_1 = 0\\) \\(\\hat{\\beta}_1 = -500\\) \\(\\hat{\\beta}_1 = 0.15\\) \\(\\hat{\\beta}_1 = 10\\) \\(\\hat{\\beta}_0 = 2500\\) 🧠 Álgebra y transformaciones ¿Qué ocurre con el estimador de MCO si multiplicamos la variable dependiente por una constante \\(c\\)? Demuestre usando la expresión matricial \\(\\hat{\\beta} = (X&#39;X)^{-1} X&#39;y\\). ¿Qué ocurre si multiplicamos una de las variables explicativas por una constante \\(c\\)? ¿Qué sucede con el coeficiente estimado y con el resto de los coeficientes? Si a la variable dependiente se le suma una constante, ¿qué ocurre con los estimadores? Si todas las variables explicativas tienen media cero, ¿cuál es el valor estimado del intercepto? ¿Por qué? 💻 Preguntas de programación Implemente una función en R que reciba dos vectores \\(x\\) e \\(y\\), y devuelva \\(\\hat{\\beta}_0\\), \\(\\hat{\\beta}_1\\) y los errores estándar, usando solo álgebra matricial. En Stata, estime una regresión por MCO con reg y luego reproduzca todos los pasos usando mata. Compare los resultados y verifique que los residuos sean iguales. Escriba una función en Python que tome \\(X\\) e \\(Y\\) como matrices de NumPy y devuelva: \\(\\hat{\\beta}\\), la matriz de varianza-covarianza, los errores estándar, y el \\(R^2\\). 📚 Para reflexionar ¿Por qué es importante que la matriz \\(X&#39;X\\) sea positiva definida? ¿Qué ocurre si no lo es? ¿Qué implicaciones tiene usar variables altamente correlacionadas en un modelo de regresión múltiple? ¿Cómo lo podrías detectar? "],["anatomía-de-la-regresión-múltiple.html", "6 Anatomía de la Regresión Múltiple Matrices de Proyección Teorema de Frisch-Waugh-Lovell (FWL) 🎯 Notación y Motivación ❓¿Cuál es el problema que resuelve el FWL? ✨ Paso a paso del Teorema de Frisch-Waugh-Lovell (FWL) 🧩 Paso 1: Proyectar \\(y\\) sobre \\(X_s\\) y obtener los residuos 🧩 Paso 2: Proyectar \\(X_r\\) sobre \\(X_s\\) y obtener los residuos 🧩 Paso 3: Regresar \\(\\tilde{y}\\) sobre \\(\\tilde{X}_r\\) ✅ Interpretación final 📦 Conclusión Demostración Formal 🧪 Ejemplo práctico del Teorema de Frisch-Waugh-Lovell en Stata, R y Python", " 6 Anatomía de la Regresión Múltiple Matrices de Proyección Supongamos que estamos estimando una regresión lineal múltiple del tipo: \\[ y = X\\beta + \\varepsilon \\] donde: \\(y \\in \\mathbb{R}^n\\) es el vector de la variable dependiente \\(X \\in \\mathbb{R}^{n \\times k}\\) es la matriz de regresores de rango completo \\(\\beta \\in \\mathbb{R}^k\\) es el vector de parámetros La estimación de mínimos cuadrados se basa en proyectar el vector \\(y\\) sobre el espacio columna de \\(X\\), denotado \\(\\mathcal{C}(X)\\). Esta proyección busca encontrar el punto más cercano en ese subespacio a \\(y\\), minimizando la suma de los residuos al cuadrado. Esta proyección se logra mediante la matriz de proyección: \\[ \\mathbf{P}_X = X(X&#39;X)^{-1}X&#39; \\] Y su complemento ortogonal (que proyecta sobre el espacio ortogonal a \\(\\mathcal{C}(X)\\)) es: \\[ \\mathbf{M}_X = I - \\mathbf{P}_X \\] \\[ \\hat{y} = \\mathbf{P}_X y \\quad \\text{y} \\quad \\hat{\\varepsilon} = \\mathbf{M}_X y \\] 🔍 Intuición geométrica \\(y\\): vector observado \\(\\hat{y} = \\mathbf{P}_X y\\): predicción, o “sombra” de \\(y\\) sobre el plano generado por \\(X\\) \\(\\hat{\\varepsilon} = y - \\hat{y}\\): residuo, perpendicular al plano El estimador de MCO se obtiene al minimizar: \\[ \\min_{\\beta} (y - X\\beta)&#39;(y - X\\beta) \\] La solución es: \\[ \\hat{\\beta} = (X&#39;X)^{-1}X&#39;y \\] 📐 Propiedades algebraicas clave Propiedad \\(\\mathbf{P}_X\\) \\(\\mathbf{M}_X\\) Simetría \\(\\mathbf{P}_X&#39; = \\mathbf{P}_X\\) \\(\\mathbf{M}_X&#39; = \\mathbf{M}_X\\) Idempotencia \\(\\mathbf{P}_X^2 = \\mathbf{P}_X\\) \\(\\mathbf{M}_X^2 = \\mathbf{M}_X\\) Ortogonalidad \\(\\mathbf{P}_X \\mathbf{M}_X = 0\\) \\(\\hat{y} \\perp \\hat{\\varepsilon}\\) Estas propiedades nos permitirán probar resultados como: ortogonalidad entre predicción y residuos, descomposición de la varianza total, y derivar el estimador de varianza del error. ✨ Visualización tridimensional de la proyección La figura muestra cómo el vector \\(y\\) se proyecta sobre el espacio generado por las columnas de \\(X\\). La diferencia \\(y - P_X y\\) es ortogonal al plano y corresponde a los residuos. Teorema de Frisch-Waugh-Lovell (FWL) El Teorema de Frisch-Waugh-Lovell (FWL) es uno de los resultados más importantes de la econometría, porque nos permite entender qué significa realmente el coeficiente de una variable en una regresión múltiple. FWL nos dice que, si estamos interesados en el efecto de una variable \\(X_r\\) sobre \\(y\\), controlando por otras variables \\(X_s\\), podemos obtener exactamente el mismo coeficiente de una manera alternativa, usando proyecciones. Esto es extremadamente útil porque: Da una interpretación clara del coeficiente como un efecto “depurado”. Permite construir regresiones parciales paso a paso. Ayuda a entender la mecánica interna de los modelos de regresión múltiples. 🎯 Notación y Motivación Considera el siguiente modelo de regresión lineal: \\[ y = X_r \\beta_r + X_s \\beta_s + \\varepsilon \\] donde: \\(y \\in \\mathbb{R}^{n \\times 1}\\) es la variable dependiente (por ejemplo, salario). \\(X_r \\in \\mathbb{R}^{n \\times 1}\\) es la variable de interés (por ejemplo, educación). \\(X_s \\in \\mathbb{R}^{n \\times k}\\) es el conjunto de variables de control (por ejemplo, experiencia, edad, género, etc.). \\(\\varepsilon \\in \\mathbb{R}^{n \\times 1}\\) es el error. Esta forma también puede escribirse más compactamente así: \\[ y = [X_r \\quad X_s] \\begin{bmatrix} \\beta_r \\\\ \\beta_s \\end{bmatrix} + \\varepsilon \\] Donde la matriz \\(X = [X_r \\quad X_s] \\in \\mathbb{R}^{n \\times (k+1)}\\) contiene todas las variables explicativas, y el vector de parámetros \\(\\beta \\in \\mathbb{R}^{k+1}\\) contiene sus coeficientes respectivos. ❓¿Cuál es el problema que resuelve el FWL? Imagina que quieres saber el efecto de la educación sobre el salario, pero sabes que hay muchas otras variables (edad, experiencia, género, etc.) que también afectan el salario. Entonces haces una regresión múltiple, y obtienes el coeficiente de educación controlando por todo lo demás. El teorema FWL te dice: “Ese coeficiente se puede obtener en tres pasos, sin necesidad de correr la regresión completa.” Y lo mejor: el resultado será exactamente igual. Eso es lo que veremos a continuación en el paso a paso. ✨ Paso a paso del Teorema de Frisch-Waugh-Lovell (FWL) El Teorema de FWL nos dice que podemos obtener el coeficiente \\(\\beta_r\\) de una regresión múltiple: \\[ y = X_r \\beta_r + X_s \\beta_s + \\varepsilon \\] realizando tres regresiones parciales, sin necesidad de incluir todos los regresores al mismo tiempo. A continuación explicamos cada paso con todo el detalle necesario. 🧩 Paso 1: Proyectar \\(y\\) sobre \\(X_s\\) y obtener los residuos Primero, eliminamos de \\(y\\) la parte que puede ser explicada por los controles \\(X_s\\). Esto se hace regresando \\(y\\) sobre \\(X_s\\) y guardando los residuos. Es decir: \\[ \\tilde{y} = M_s y = (I - P_s)y \\] donde: \\(P_s = X_s (X_s&#39;X_s)^{-1} X_s&#39;\\) es la matriz de proyección sobre el espacio generado por los controles \\(X_s\\). \\(M_s = I - P_s\\) es la matriz de residuos o complemento ortogonal: elimina todo lo que esté explicado por \\(X_s\\). 🔍 ¿Qué significa esto? Estamos “limpiando” a \\(y\\), quitándole la parte que se debe a los controles. El resultado \\(\\tilde{y}\\) representa la parte de \\(y\\) que es ortogonal a los controles. 📐 Interpretación geométrica: proyectamos \\(y\\) sobre el subespacio generado por \\(X_s\\) y nos quedamos con la componente perpendicular. 🧩 Paso 2: Proyectar \\(X_r\\) sobre \\(X_s\\) y obtener los residuos Ahora hacemos lo mismo con la variable de interés \\(X_r\\): le quitamos la parte que puede explicarse con los controles. \\[ \\tilde{X}_r = M_s X_r = (I - P_s)X_r \\] 🔍 ¿Qué significa esto? Estamos “depurando” a \\(X_r\\), eliminando cualquier relación lineal con los controles. \\(\\tilde{X}_r\\) es la parte de \\(X_r\\) que no se puede predecir con \\(X_s\\). 📐 Interpretación geométrica: proyectamos \\(X_r\\) sobre \\(\\mathcal{C}(X_s)\\) y guardamos el residuo, que es ortogonal a ese subespacio. 🧩 Paso 3: Regresar \\(\\tilde{y}\\) sobre \\(\\tilde{X}_r\\) Finalmente, estimamos el coeficiente que relaciona las dos variables “limpias” o depuradas: \\[ \\tilde{y} = \\tilde{X}_r \\cdot \\gamma + u \\] El estimador de MCO de esta regresión es: \\[ \\hat{\\gamma} = (\\tilde{X}_r&#39;\\tilde{X}_r)^{-1} \\tilde{X}_r&#39; \\tilde{y} \\] 🔔 ¡Sorpresa! Este estimador es exactamente igual a: \\[ \\hat{\\beta}_r \\quad \\text{(el coeficiente de \\( X_r \\) en la regresión completa)} \\] ✅ Interpretación final Este resultado nos dice que: El efecto de \\(X_r\\) sobre \\(y\\), controlando por \\(X_s\\), es igual al efecto de la parte de \\(X_r\\) que no se relaciona con \\(X_s\\) sobre la parte de \\(y\\) que tampoco se relaciona con \\(X_s\\). En palabras simples: es una regresión entre los residuos. 📦 Conclusión Este teorema tiene implicaciones profundas: Muestra que controlar por variables equivale a quitarles su efecto tanto a la variable explicativa como a la dependiente, y luego ver cómo se relacionan esas partes “limpias”. Es la base para entender técnicas como control por regresión parcial, y también para desarrollar intuiciones sobre variables instrumentales, efectos marginales y más. Demostración Formal Usando álgebra matricial: \\[ \\hat{\\beta}_r = (X_r&#39;M_s X_r)^{-1} X_r&#39;M_s y \\] Esto se deduce de la forma general del estimador de MCO: \\[ \\hat{\\beta} = (X&#39;X)^{-1}X&#39;y \\] pero aplicado al modelo reducido, en el que \\(y\\) y \\(X_r\\) han sido “limpiados” de \\(X_s\\). 🧪 Ejemplo práctico del Teorema de Frisch-Waugh-Lovell en Stata, R y Python A continuación presentamos una simulación sencilla para ilustrar el Teorema de Frisch-Waugh-Lovell (FWL) y mostrar cómo cambia el coeficiente estimado de una variable de interés dependiendo de la correlación con los controles. También implementamos paso a paso la construcción del estimador usando residuos. 6.0.1 🔵 Código en Stata clear set seed 6981473 set obs 1000 * Variable de interés Xr gen altura = runiform()*30+150 replace altura= round(altura) label var altura &quot;altura&quot; * Crear Xs con correlación positiva con Xr gen ingreso_hh = rnormal() + 5*altura * Variable dependiente Y gen salario = 1 + 2*altura + 5*ingreso_hh + rnormal() * CASO 1: cov(altura, ingreso_hh)&gt;0 reg salario altura reg salario altura ingreso_hh * CASO 2: cov(altura, ingreso_hh)=0 gen ingreso_hh2 = rnormal() gen salario2 = 1 + 2*altura + 5*ingreso_hh2 + rnormal() reg salario2 altura reg salario2 altura ingreso_hh2 * CASO 3: cov(salario, ingreso_hh)=0 gen ingreso_hh3 = rnormal() + 3*altura gen salario3 = 1 + 2*altura + 0*ingreso_hh3 + rnormal() reg salario3 altura reg salario3 altura ingreso_hh3 * Teorema de FWL paso a paso reg salario ingreso_hh predict My, res reg altura ingreso_hh predict MXr, res reg salario altura ingreso_hh reg My MXr * Comparar coeficientes di _b[MXr] _b[altura] 6.0.2 🟢 Código en R library(tidyverse) set.seed(6981473) n &lt;- 1000 # Variable de interés altura &lt;- runif(n, 0, 30) + 150 altura &lt;- round(altura) # Control correlacionado con altura ingreso_hh &lt;- rnorm(n) + 5 * altura # Variable dependiente salario &lt;- 1 + 2 * altura + 5 * ingreso_hh + rnorm(n) df &lt;- tibble(altura, ingreso_hh, salario) # CASO 1: cov(altura, ingreso_hh)&gt;0 summary(lm(salario ~ altura, data = df)) summary(lm(salario ~ altura + ingreso_hh, data = df)) # CASO 2: cov(altura, ingreso_hh)=0 df$ingreso_hh2 &lt;- rnorm(n) df$salario2 &lt;- 1 + 2 * df$altura + 5 * df$ingreso_hh2 + rnorm(n) summary(lm(salario2 ~ altura, data = df)) summary(lm(salario2 ~ altura + ingreso_hh2, data = df)) # CASO 3: cov(y, ingreso_hh)=0 df$ingreso_hh3 &lt;- rnorm(n) + 3 * df$altura df$salario3 &lt;- 1 + 2 * df$altura + 0 * df$ingreso_hh3 + rnorm(n) summary(lm(salario3 ~ altura, data = df)) summary(lm(salario3 ~ altura + ingreso_hh3, data = df)) # FWL paso a paso modelo_y &lt;- lm(salario ~ ingreso_hh, data = df) df$My &lt;- resid(modelo_y) modelo_xr &lt;- lm(altura ~ ingreso_hh, data = df) df$MXr &lt;- resid(modelo_xr) summary(lm(My ~ MXr, data = df)) # Igual al coef. de altura summary(lm(salario ~ altura + ingreso_hh, data = df)) # Verificación # Comparar coeficientes cat(&quot;Coeficiente de altura (residuos):&quot;, coef(lm(My ~ MXr, data = df))[&quot;MXr&quot;], &quot;\\n&quot;) cat(&quot;Coeficiente de altura (modelo completo):&quot;, coef(lm(salario ~ altura + ingreso_hh, data = df))[&quot;altura&quot;], &quot;\\n&quot;) 6.0.3 🔴 Código en Python import numpy as np import pandas as pd import statsmodels.api as sm import statsmodels.formula.api as smf np.random.seed(6981473) n = 1000 # Variable de interés altura = np.round(np.random.uniform(0, 30, n) + 150) # Control correlacionado ingreso_hh = np.random.normal(0, 1, n) + 5 * altura # Variable dependiente salario = 1 + 2 * altura + 5 * ingreso_hh + np.random.normal(0, 1, n) df = pd.DataFrame({ &#39;altura&#39;: altura, &#39;ingreso_hh&#39;: ingreso_hh, &#39;salario&#39;: salario }) # CASO 1 print(sm.OLS.from_formula(&#39;salario ~ altura&#39;, data=df).fit().summary()) print(sm.OLS.from_formula(&#39;salario ~ altura + ingreso_hh&#39;, data=df).fit().summary()) # CASO 2 df[&#39;ingreso_hh2&#39;] = np.random.normal(0, 1, n) df[&#39;salario2&#39;] = 1 + 2 * df[&#39;altura&#39;] + 5 * df[&#39;ingreso_hh2&#39;] + np.random.normal(0, 1, n) print(sm.OLS.from_formula(&#39;salario2 ~ altura&#39;, data=df).fit().summary()) print(sm.OLS.from_formula(&#39;salario2 ~ altura + ingreso_hh2&#39;, data=df).fit().summary()) # CASO 3 df[&#39;ingreso_hh3&#39;] = np.random.normal(0, 1, n) + 3 * df[&#39;altura&#39;] df[&#39;salario3&#39;] = 1 + 2 * df[&#39;altura&#39;] + np.random.normal(0, 1, n) print(sm.OLS.from_formula(&#39;salario3 ~ altura&#39;, data=df).fit().summary()) print(sm.OLS.from_formula(&#39;salario3 ~ altura + ingreso_hh3&#39;, data=df).fit().summary()) # FWL paso a paso modelo_y = sm.OLS.from_formula(&#39;salario ~ ingreso_hh&#39;, data=df).fit() df[&#39;My&#39;] = modelo_y.resid modelo_xr = sm.OLS.from_formula(&#39;altura ~ ingreso_hh&#39;, data=df).fit() df[&#39;MXr&#39;] = modelo_xr.resid print(sm.OLS.from_formula(&#39;My ~ MXr&#39;, data=df).fit().summary()) # Verificación print(sm.OLS.from_formula(&#39;salario ~ altura + ingreso_hh&#39;, data=df).fit().summary()) # Comparar coeficientes print(&quot;Coeficiente de altura (residuos):&quot;, sm.OLS.from_formula(&#39;My ~ MXr&#39;, data=df).fit().params[&#39;MXr&#39;]) print(&quot;Coeficiente de altura (modelo completo):&quot;, sm.OLS.from_formula(&#39;salario ~ altura + ingreso_hh&#39;, data=df).fit().params[&#39;altura&#39;]) 📘 Preguntas de repaso ¿Qué significa que el coeficiente de una variable en una regresión múltiple sea “depurado”? 📌 Preguntas sobre FWL y matrices de proyección Sea \\(y \\in \\mathbb{R}^{n \\times 1}\\), \\(X \\in \\mathbb{R}^{n \\times k}\\), y \\(D \\in \\mathbb{R}^{n \\times 1}\\) una variable binaria tal que \\(D_i = 1\\) solo para una observación \\(i\\), y \\(D_j = 0\\) para \\(j \\neq i\\). Use los pasos del teorema de Frisch-Waugh-Lovell para demostrar que el coeficiente estimado de \\(D\\) representa la diferencia entre la observación \\(i\\) y la predicción para esa observación basada en el resto de la muestra. ¿Qué ocurre con la matriz de proyección \\(P_D\\)? ¿Qué dimensión tiene y cómo se interpreta cuando solo proyecta sobre una observación? ¿Qué ocurre con la matriz de aniquilación \\(M_D = I - P_D\\)? ¿Qué efecto tiene sobre el resto del vector \\(y\\)? Use esta información para demostrar que al incluir \\(D\\) en la regresión, se está excluyendo efectivamente la observación \\(i\\) del resto del modelo. Es decir, la estimación de los coeficientes asociados a \\(X\\) se hace como si se eliminara la observación \\(i\\). Repita el análisis anterior, pero ahora asuma que la variable \\(D\\) es una constante. ¿Qué ocurre con las matrices de proyección y aniquilación? ¿Qué interpretación tiene proyectar sobre una constante? Suponga ahora que \\(X\\) es una dummy igual a 1 si el individuo es hombre, y que \\(D\\) es una dummy igual a 1 si el individuo es mujer. ¿Qué ocurre en este caso? ¿Cómo se interpretan los coeficientes al incluir ambas dummies en la regresión? 📘 Preguntas sobre modelos con variables binarias y constantes Se desea estudiar el número de horas de lectura diaria \\(Y\\) como función del nivel educativo. Se tiene una muestra de \\(N\\) individuos clasificados en tres grupos: Grupo 1: estudios superiores Grupo 2: estudios medios Grupo 3: estudios bajos Se definen tres variables binarias \\(D_1, D_2, D_3\\), donde: \\[ D_j = \\begin{cases} 1 &amp; \\text{si el individuo pertenece al grupo } j \\\\ 0 &amp; \\text{en caso contrario} \\end{cases} \\] Al estimar el siguiente modelo: \\[ Y = 10 D_1 + 5 D_2 + 2 D_3 + u \\] Demuestre matemáticamente que las medias condicionales de horas de lectura son 10, 5 y 2 para cada grupo. ¿Qué sucede si se incluye una constante en este modelo? ¿Qué problema empírico surge? Proponga una solución (por ejemplo, eliminar una de las dummies para evitar colinealidad perfecta). 🧮 Preguntas sobre regresiones simples con constantes y dummies Si se estima una regresión de \\(Y\\) contra una constante y \\(D_1\\), ¿cuál es el intercepto y cuál es el coeficiente de \\(D_1\\)? Interprételos. Si se estima una regresión de \\(Y\\) contra una constante y \\(D_2\\), ¿qué coeficiente acompaña a \\(D_2\\)? ¿Cómo cambia la interpretación con respecto al caso anterior? 📊 Pregunta sobre FWL y álgebra matricial Considere el siguiente modelo lineal sin constante: \\[ Y = X \\beta + u \\] donde \\(Y \\in \\mathbb{R}^{n \\times 1}\\), \\(X \\in \\mathbb{R}^{n \\times k}\\), y \\(u \\in \\mathbb{R}^{n \\times 1}\\). Suponga además que desea controlar por un conjunto adicional de variables \\(Z \\in \\mathbb{R}^{n \\times m}\\). Defina \\(\\tilde{Y} = M_Z Y\\) y \\(\\tilde{X} = M_Z X\\), donde \\(M_Z = I - P_Z\\) y \\(P_Z = Z(Z&#39;Z)^{-1}Z&#39;\\). Explique con detalle qué representan estas transformaciones. ¿Qué parte de \\(Y\\) y de \\(X\\) están conservando? ¿Qué parte están eliminando? Demuestre que el vector de coeficientes \\(\\hat{\\beta}\\) para \\(X\\), en la regresión de \\(Y\\) sobre \\(X\\) y \\(Z\\), puede obtenerse a partir de la siguiente expresión: \\[ \\hat{\\beta} = (\\tilde{X}&#39; \\tilde{X})^{-1} \\tilde{X}&#39; \\tilde{Y} \\] ¿Bajo qué condiciones es válida esta expresión? ¿Qué sucede si \\(\\tilde{X}&#39; \\tilde{X}\\) no es invertible? Explique cómo se interpreta este resultado en términos del Teorema de Frisch-Waugh-Lovell. "]]
