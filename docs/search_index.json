[["index.html", "Álgebra Matricial para Econometría Capítulo 2: Regresión lineal 1 Información general Descripción del curso Material bibliográfico Evaluación Programa semanal Recursos adicionales Inclusión Integridad académica", " Álgebra Matricial para Econometría Capítulo 2: Regresión lineal Ana María Díaz 15 July 2025 1 Información general Campo Detalle Curso Econometría Avanzada (Cod 1420) Docente Ana María Díaz Oficina / Atención Séptimo piso Edificio 20 | Lunes 9–11 a.m. (o por Teams) Sitio web http://adiazescobar.com Correo a.diaze@javeriana.edu.co Prerequisito Econometría I Horario de clase Martes y Jueves 11:00–13:00 | Salones 67‑208 y 67‑314 Monitor Miguel Ángel Cortés — horarios y oficina por definir Descripción del curso El objetivo principal es proporcionar herramientas para el análisis econométrico de datos de corte transversal, series de tiempo y panel. Se revisa el modelo clásico de regresión lineal, las consecuencias de violar sus supuestos, modelos para variables dependientes discretas o limitadas y técnicas básicas de series de tiempo y panel. Al finalizar, el estudiante podrá ejecutar regresiones múltiples, diagnosticar problemas comunes y aplicar soluciones apropiadas. Material bibliográfico Libro obligatorio Verbeek, Marno (2004). A Guide to Modern Econometrics. Wiley. Libros recomendados Greene, William (2003). Econometric Analysis. Prentice Hall. Wooldridge, Jeffrey (2003). Introductory Econometrics: A Modern Approach. Thomson. Wooldridge, Jeffrey (2002). Econometric Analysis of Cross Section and Panel Data. MIT Press. Montenegro, Álvaro (2009). Series de Tiempo. Javegraf, PUJ. Stock, J. &amp; Watson, M. (2006). Introduction to Econometrics. Addison‑Wesley. Hayashi, Fumio (2000). Econometrics. Princeton UP. Angrist, J.D. &amp; Pischke, J.S. (2009). Mostly Harmless Econometrics. Princeton UP. Cameron, A.C. &amp; Trivedi, P.K. (2009). Microeconometrics Using Stata. Stata Press. Stata 11 Time Series Reference Manual. Stata Press. Rosales R. et al. (2010). Fundamentos de Econometría Intermedia. CEDE. Evaluación Porcentaje Actividad 25 % Parcial 1 teórico 25 % Parcial 2 teórico 3 % Talleres en clase 7 % Monitorías 15 % Trabajo final 25 % Examen final +0.5 (para el mejor) Video‑bono examen final Los exámenes son con libro cerrado y sin dispositivos electrónicos. El incumplimiento se sanciona según el reglamento de integridad académica. Programa semanal Semana Tema principal Lecturas clave 1 Supuestos del MCRL Verbeek cap. 1‑2 (oblig.)  | Hayashi cap. 1; Wooldridge cap. 1‑2 (opc.) 2 Regresión simple vs múltiple; Teorema FWL Verbeek cap. 1‑2  | The Stata Journal (2013) 13(1): 92‑106 3 Propiedades de MCO en muestras finitas; Teorema Gauss‑Markov Verbeek cap. 2 4 Inferencia y predicción; Propiedades asintóticas de MCO Verbeek cap. 3 5 Primer parcial — 6 No linealidad; Multicolinealidad Verbeek cap. 3‑4 7 Heterocedasticidad Verbeek cap. 4 8 Endogeneidad: simultaneidad, omitidas, medición Verbeek cap. 5 9 Variables instrumentales, MCO2E, GMM Verbeek cap. 5 10 Modelos LPM, logit y probit Verbeek cap. 7 11 Máximo verosimilitud; DID, RD, duración, cuantílica (opc.) — 12 Semana Santa / Receso — 13 Segundo parcial — 14 Series de tiempo: conceptos básicos Verbeek cap. 8 15 AR, MA y VAR estacionarios Verbeek cap. 9 16 Datos de panel: pooled, between, FE, RE Verbeek cap. 10 17 Examen final — Recursos adicionales Ben Lambert – Econometrics on YouTube Mastering Econometrics (MRU) AEA Journal of Economic Perspectives – Classroom Google Dataset Search Stata Cheat Sheets Seeing Theory – Visual Probability Inclusión Este curso da la bienvenida a personas de todas las edades, orígenes, creencias, etnias, géneros, identidades, orientaciones sexuales y capacidades. Se espera un ambiente respetuoso e inclusivo. Integridad académica La Universidad Javeriana fomenta la honestidad y establece sanciones por fraude o plagio según el reglamento de estudiantes. Cualquier uso no autorizado de materiales durante evaluaciones se considera falta grave. "],["repaso.html", "2 Repaso Construimos una población de juguete ¿Y si mi muestra es mala? ¿Y si mantengo fija la muestra? 📘 Preguntas de repaso", " 2 Repaso ¿Qué estudia la econometría? La econometría es la herramienta que usamos para entender el mundo usando datos. Nos ayuda a responder preguntas como: ¿cuánto gana una persona según su nivel educativo? ¿Cómo influye la experiencia laboral en el salario? ¿Cuál es el impacto de una política pública sobre el empleo? Pero aquí hay un reto importante: casi nunca podemos observar a toda la población. En vez de eso, trabajamos con una muestra. Usamos esta muestra para hacer inferencias sobre cómo funciona el mundo real, ese que no podemos ver completamente. En este capítulo vamos a entender, paso a paso, por qué eso genera incertidumbre —y por qué esa incertidumbre es una parte inevitable (¡y valiosa!) del análisis econométrico. El proceso generador de datos Supongamos que el salario de un individuo, \\(y_i\\), depende de forma lineal de su nivel educativo, \\(x_i\\): \\[ y_i \\;=\\; \\beta_0 \\;+\\; \\beta_1\\,x_i \\;+\\; u_i, \\] donde \\(u_i\\) recoge todo lo que no observamos (habilidad, contactos, suerte…). A esta ecuación la llamaremos Proceso Generador de Datos (PGD) o modelo poblacional. El problema es que no podemos observar \\(u_i\\) porque es un término de error. Ni tenemos acceso a todos los individuos de la población. ¿Qué hacemos entonces? En la práctica, tomamos una muestra aleatoria de individuos y observamos sus salarios y años de educación \\((y_i,\\,x_i)\\), el termino de error \\(u_i\\) permanece oculto. Con una muestra aleatoria de tamaño \\(n\\), estimamos los parámetros \\(\\beta_0\\) y \\(\\beta_1\\) de la siguiente manera: \\[ y_i \\;=\\; \\hat{\\beta}_0 + \\hat{\\beta}_1\\,x_i + e_i, \\qquad \\hat{y}_i \\;=\\; \\hat{\\beta}_0 + \\hat{\\beta}_1\\,x_i, \\] Donde \\(\\hat{\\beta}_0\\) y \\(\\hat{\\beta}_1\\) son los estimadores de los parámetros poblacionales \\(\\beta_0\\) y \\(\\beta_1\\), y \\(e_i\\) es el término de error muestral. A la recta que obtenemos se le llama modelo muestral. La direfencia entre este modelo y el PGD es precisamente lo que genera incertidumbre en nuestras estimaciones. Es decir que tenemos dos fuentes de incertidumbre, la muestra que compone nuestros datos y el término de error \\(u_i\\) que no podemos observar. Para entender todo esto mejor, vamos primero a enfocarnos en la muestra que tenemos y cómo podemos usarla para estimar el PGD. Luego veremos cómo la incertidumbre afecta nuestras estimaciones y por qué es importante. Construimos una población de juguete Para ilustrar el proceso generador de datos y la incertidumbre, vamos a crear una población de juguete. Esta población será un conjunto de 100 individuos con características específicas. Luego tomaremos muestras aleatorias de esta población y realizaremos regresiones para ver cómo se comportan nuestras estimaciones en comparación con el PGD real. Vamos a crear un mundo ficticio con 100 individuos. A cada uno le asignamos: \\(x\\) (años de educación) sigue una normal con media 5 y desviación 1.5. \\(y\\) depende linealmente de \\(x\\) con pendiente 0.5 y un término aleatorio \\(u\\sim N(0,1)\\). La relación verdadera en la población El modelo poblacional que usamos, es decir el PGD, es: \\(y = 3 + 0.5x\\): Así que en promedio los salarios de los individuos aumentan en 0.5 por cada año adicional de educación. Esta es la verdad de nuestra población simulada. Obtenemos que los coeficientes son muy similares a los que usamos para generar la población: \\[ y_i = 2.53 + 0.57 x_i + u_i \\] Esto significa que el modelo poblacional es: \\[ y_i = \\beta_0 + \\beta_1 x_i + u_i \\] Sin embargo, esa linea está fuera de nuestro alcance porque requeriría encuestar a todos los egresados. Podemos estimar la relación entre \\(y\\) y \\(x\\) en una muestra aleatoria de individuos. Comencemos tomando 30 graduados al azar de nuestro grupo de 100 individuos: Estimemos la relación que existe entre \\(y\\) y \\(x\\) en esta muestra de 30 individuos. En la siguiente gráfica, la línea roja es el modelo poblacional y la línea negra discontinua es el modelo muestral. Ahora encontramos unos coeficientes estimados que son diferentes a los del modelo poblacional: PGD Modelo Poblacional \\(y_i = 2.53 + 0.57 x_i + u_i\\) Modelo muestral \\(\\hat{y}_i = 2.36 + 0.61 x_i\\) Tomemos otros 30 individuos al azar de la población y veamos cómo se comporta la regresión. Ahora encontramos los siguientes coeficientes estimados: PGD Modelo Poblacional \\(y_i = 2.53 + 0.57 x_i + u_i\\) Modelo muestral \\(\\hat{y}_i = 2.79 + 0.56 x_i\\) Podemos ver que los coeficientes estimados son diferentes a los del modelo poblacional y también diferentes entre sí. Esto es normal, porque cada muestra aleatoria puede dar lugar a diferentes estimaciones. Tomemos una tercera muestra aleatoria de 30 individuos y veamos cómo se comporta la regresión. Ahora encontramos los siguientes coeficientes estimados: PGD Modelo Poblacional \\(y_i = 2.53 + 0.57 x_i + u_i\\) Modelo muestral \\(\\hat{y}_i = 3.21 + 0.45 x_i\\) Siguen siendo diferentes a los del modelo poblacional y también diferentes entre sí. A veces se parece mucho, a veces no tanto. La razón es simple; cada muestra incluye un conjunto diferente de personas y eso cambia los resultados. Ahora repitamos esto 10,000 veces. Este ejercicio se conoce como Ejercicio de Monte Carlo. Vamos a tomar 10,000 muestras aleatorias de 30 individuos de nuestra población y estimar los coeficientes de regresión para cada muestra. Luego, graficaremos todas las líneas de regresión obtenidas para ver cómo se distribuyen en relación con la línea poblacional. ¿Lo interesante? Aunque cada recta individual es distinta, en promedio todas convergen hacia la recta verdadera (la primera que estimamos). En resumen, en promedio las líneas de regresión se ajustan muy bien a la línea de la población. Sin embargo, las líneas individuales (muestras) pueden desviarse significativamente. Las diferencias entre las muestras individuales y la población generan incertidumbre para el econometrista. 👉 Este resultado es tranquilizador: aunque nuestras estimaciones varían de muestra a muestra, en promedio nos acercamos a la verdad. Esto es lo que se conoce como insesgamiento del estimador MCO. Eso implica que cuando estimamos los coeficientes de regresión, no podemos estar seguros de que nuestros estimadores sean exactamente iguales a los parámetros poblacionales. En cambio, obtenemos estimaciones que son variables aleatorias. En otras palabras, \\(\\hat{\\beta}\\) en sí mismo es una variable aleatoria, dependiente de la muestra aleatoria. Cuando tomamos una muestra y realizamos una regresión, no sabemos si es una muestra ‘buena’ ( \\(\\hat{\\beta}\\) está cerca de \\(\\beta\\)) o una muestra ‘mala’ (nuestra muestra difiere significativamente de la población). Mantener un registro de esta incertidumbre es clave para el análisis econométrico. Nos permite entender la precisión de nuestras estimaciones y cómo podemos mejorar nuestro modelo. ¿Y si mi muestra es mala? ❓ Pregunta del lector: ¿Qué pasa si me toca una muestra mala? ¿Cómo lo sé? ¿Se puede hacer algo para reducir esa incertidumbre? Una muestra mala es una muestra que, por puro azar, no representa bien a la población. Esto puede pasar incluso si tomamos la muestra correctamente. En esos casos, los estimadores como \\(\\hat{\\beta}_1\\) pueden estar lejos de su valor verdadero \\(\\beta_1\\), y nuestras conclusiones podrían ser engañosas. ¿Cómo saber si me tocó una muestra mala? No podemos saberlo con certeza, porque no conocemos la verdad poblacional. Pero hay señales que nos pueden alertar: Errores estándar grandes: indican mucha variabilidad en la estimación. Intervalos de confianza anchos: reflejan gran incertidumbre. Signos o tamaños inesperados en los coeficientes: pueden deberse a una muestra no representativa. Pruebas de diagnóstico del modelo: pueden revelar si los supuestos no se cumplen (residuos no normales, heterocedasticidad, etc.). ¿Se puede reducir la incertidumbre muestral? ¡Sí! Estas son algunas estrategias comunes: Aumentar el tamaño de la muestra (\\(n\\)) Entre más observaciones, más cerca estará \\(\\hat{\\beta}\\) de \\(\\beta\\) (por la ley de los grandes números). Mejorar el diseño muestral Muestreos estratificados, por conglomerados o con pesos pueden hacer las estimaciones más precisas. Controlar por variables relevantes Incluir más covariables reduce la varianza al explicar mejor el comportamiento de \\(y\\). Usar estimadores eficientes o robustos Si hay heterocedasticidad, los errores estándar robustos o el uso de métodos como MCO ponderado pueden mejorar la precisión. ✅ La importancia de la fuente de los datos Una forma muy eficaz de minimizar el riesgo de una muestra sesgada es usar datos de fuentes confiables y con buen diseño muestral. Por ejemplo, confiar en los datos del DANE en Colombia o de institutos nacionales de estadística en otros países es una práctica fundamental. Estas instituciones diseñan cuidadosamente sus encuestas (como la ECH, ENUT o ENDS) para asegurar que sean representativas de la población. Si el muestreo está bien hecho desde el inicio, el margen de error se reduce y nuestras inferencias serán mucho más confiables. 💡 Mensaje clave: La incertidumbre no es un error: es una característica natural del trabajo con datos. Lo importante no es eliminarla, sino medirla bien, comunicarla con claridad y tenerla en cuenta al tomar decisiones. ¿Y si mantengo fija la muestra? Hasta ahora nos enfocamos en la incertidumbre que surge por el azar de la muestra. Ahora exploramos otra fuente igual de importante: la variabilidad del término de error , incluso si la muestra es fija. Hasta ahora vimos que la incertidumbre puede surgir del hecho de que trabajamos con una muestra: cada subconjunto aleatorio de la población genera estimadores ligeramente diferentes. Pero hay una segunda fuente de incertidumbre: el término de error \\(u_i\\). Recordemos el modelo: \\[ y_i = \\beta_0 + \\beta_1 x_i + u_i \\] Aunque tuviéramos toda la población (o una muestra perfecta), no podríamos predecir perfectamente \\(y_i\\) porque el valor de \\(u_i\\) sigue siendo desconocido. El termino de error \\(u_i\\) representa todo lo que influye en \\(y_i\\) pero no está capturado por \\(x_i\\). Por ejemplo, en un modelo donde \\(y_i\\) es el salario y \\(x_i\\) es la educación: Habilidades intrínsecas Redes de contacto Experiencia laboral previa Suerte (¡sí, también cuenta!) Todo eso se concentra en \\(u_i\\), que es no observable, pero no irrelevante. Incluso si estimamos \\(\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i\\) con una muestra fija: Nuestra recta de regresión capta la tendencia promedio. Pero los valores observados de \\(y_i\\) se dispersan alrededor de la recta por culpa de \\(u_i\\). Esto se ve así: Al repetir el proceso de muestreo muchas veces, cada muestra tendrá su propia recta de regresión, pero todas estarán dispersas alrededor de la recta poblacional. Vamos a hacer lo siguiente: Tomamos una sola muestra fija de 30 individuos de la población. Mantenemos sus valores de \\(x_i\\) constantes. Re-generamos el término de error \\(u_i \\sim N(0,1)\\) varias veces. Calculamos nuevos valores de \\(y_i = 3 + 0.5x_i + u_i\\) en cada iteración. Estimamos una regresión para cada muestra simulada. Esto nos permite ver cómo las estimaciones varían únicamente por culpa del término de error, manteniendo fija la muestra. Aquí vemos que, aunque la muestra es fija, las rectas de regresión se dispersan alrededor de la recta poblacional. Esto es porque el término de error \\(u_i\\) introduce variabilidad en los valores de \\(y_i\\). ☝️ Una aclaración importante sobre el insesgamiento Al repetir el proceso de muestreo muchas veces, cada muestra tendrá su propia recta de regresión. Pero ojo: aunque esas rectas se vean “alrededor” de la recta poblacional en nuestras simulaciones, esto no siempre ocurre en la vida real. El hecho de que las estimaciones se agrupen alrededor de los verdaderos valores poblacionales depende de que se cumplan los supeustos de modelo de regresión lineal. Por ejemplo: Si el término de error \\(u_i\\) está correlacionado con \\(x_i\\) (por ejemplo, porque omitimos una variable relevante), entonces nuestro estimador de \\(\\beta_1\\) estará sesgado. Si hay errores de medición, mala especificación del modelo o selección no aleatoria, también se viola el insesgamiento. 🎯 ¿Cuándo es cierto que nuestras estimaciones “se agrupan” alrededor del verdadero \\(\\beta_1\\)? Cuando se cumplen los supuestos del modelo clásico de regresión lineal, en particular exogeneidad estricta o independencia del término de error \\(u_i\\) respecto a las variables explicativas \\(x_i\\). En algunos libros se conoce como esperanza condicional igual cero: \\[ \\mathbb{E}[u_i \\mid x_i] = 0 \\] 🔁 Si este supuestos se cumplen, entonces nuestro estimador de Mínimos Cuadrados Ordinarios (MCO) es insesgado: \\[ \\mathbb{E}[\\hat{\\beta}_1] = \\beta_1 \\] Es decir, si repitiéramos el experimento de muestreo muchas veces, el promedio de nuestras estimaciones convergería al verdadero valor poblacional. Pero si no se cumplen, como veremos más adelante, podemos tener: Estimadores sesgados Estimadores inconsistentes Intervalos de confianza y pruebas de hipótesis inválidos 💬 Entonces, ¿las simulaciones que hicimos son “realistas”? Sí… bajo los supuestos del modelo clásico. En nuestras simulaciones controlamos todo: sabemos exactamente cómo se genera \\(y_i\\), y aseguramos que \\(u_i\\) sea independiente de \\(x_i\\). Por eso nuestras estimaciones tienden a agruparse cerca de la recta poblacional. Pero en el mundo real, los datos no vienen con etiqueta de “supuestos cumplidos”. Por eso uno de los grandes desafíos del análisis econométrico es diagnosticar y justificar si los supuestos se cumplen. Y si no se cumplen, buscar soluciones: variables instrumentales, variables omitidas, diseños cuasiexperimentales, diseños experimentales, etc. 💡 Conclusión clave: El término de error no desaparece, incluso cuando tenemos una muestra grande o bien diseñada. Por eso, cualquier estimación puntual (\\(\\hat{\\beta}_1\\)) debe ir acompañada de una medida de incertidumbre, como el error estándar o un intervalo de confianza. Esto nos prepara para el siguiente paso: la inferencia estadística. 📘 Preguntas de repaso ¿Qué diferencia hay entre el modelo poblacional y el modelo muestral? ¿Cuál de los dos observamos y cuál inferimos? ¿Por qué decimos que el término de error \\(u_i\\) es una fuente de incertidumbre, incluso si la muestra está fija? ¿Qué condiciones deben cumplirse para que el estimador de Mínimos Cuadrados Ordinarios (MCO) sea insesgado? ¿Qué significa que un estimador sea insesgado “en promedio”? ¿Eso garantiza que cualquier muestra nos dará un buen resultado? ¿Qué implicaciones tiene usar una muestra mal diseñada o no representativa? En la simulación Monte Carlo, ¿por qué las rectas de regresión estimadas con diferentes muestras se agrupan alrededor de la recta poblacional? ¿Qué observas cuando repetimos la estimación con una misma muestra, pero re-generamos el término de error? ¿Qué se mantiene constante y qué varía? En tus propias palabras, ¿por qué no podemos predecir perfectamente \\(y_i\\) aunque conozcamos bien \\(x_i\\)? ¿Por qué se llama “Monte Carlo” a este método de simulación? ¿Qué relación tiene con el azar? ¿Qué riesgos implica asumir que los supuestos del modelo clásico se cumplen cuando en realidad no lo hacen? ¿Qué estrategias puedes usar si sospechas que \\(u_i\\) está correlacionado con \\(x_i\\)? Menciona al menos dos. ¿Crees que todas las fuentes oficiales de datos (como el DANE) garantizan muestras perfectamente representativas? ¿Qué condiciones lo permitirían? ✏️ Opcional para práctica adicional: simula tu propia población de juguete con una relación negativa entre \\(x\\) y \\(y\\), y repite el ejercicio de Monte Carlo. ¿Qué cambia? ¿Qué se mantiene? # Paso 1: Crear muestra fija de x set.seed(123) n &lt;- 30 x &lt;- rnorm(n, mean = 5, sd = 1.5) # Paso 2: Definir número de simulaciones B &lt;- 1000 # Paso 3: Simular y almacenar coeficientes library(tibble) library(purrr) library(broom) sim &lt;- map_dfr(1:B, function(i) { u &lt;- rnorm(n, 0, 1) y &lt;- 3 + 0.5 * x + u model &lt;- lm(y ~ x) tidy(model)[2, c(&quot;term&quot;, &quot;estimate&quot;)] }) # Paso 4: Visualizar distribución del estimador library(ggplot2) ggplot(sim, aes(x = estimate)) + geom_histogram(bins = 40, fill = &quot;steelblue&quot;, color = &quot;white&quot;) + geom_vline(xintercept = 0.5, color = &quot;red&quot;, linetype = &quot;dashed&quot;) + labs( title = &quot;Distribución de \\\\(\\\\hat{\\\\beta}_1\\\\) con muestra fija&quot;, x = &quot;Estimaciones de \\\\(\\\\hat{\\\\beta}_1\\\\)&quot;, y = &quot;Frecuencia&quot; ) clear all set seed 123 set obs 30 * Paso 1: Crear muestra fija de x gen x = rnormal(5, 1.5) * Paso 2: Guardar la muestra base tempfile base save `base&#39; * Paso 3: Preparar almacenamiento de resultados tempname resultados postfile `resultados&#39; b1 using results_sim.dta, replace * Paso 4: Simulación Monte Carlo local reps = 1000 forvalues i = 1/`reps&#39; { use `base&#39;, clear gen u = rnormal(0,1) gen y = 3 + 0.5*x + u regress y x post `resultados&#39; (_b[x]) } postclose `resultados&#39; * Paso 5: Usar y graficar los resultados use results_sim.dta, clear histogram b1, width(0.02) normal /// title(&quot;Distribución de coeficientes con muestra fija&quot;) /// xtitle(&quot;Estimaciones de _b[x]&quot;) ytitle(&quot;Frecuencia&quot;) "],["regresión-lineal.html", "3 Regresión lineal 🎯 Objetivo del capítulo 🔍 ¿Qué significa encontrar la “mejor línea”? MCO 📊 Propiedades y supuestos 📘 Preguntas de repaso", " 3 Regresión lineal 🎯 Objetivo del capítulo En este capitulo vamos a: 1. Entender qué es una regresión lineal y cómo se ve gráficamente. 2. Aprender cómo se calcula la mejor línia con mínimos cuadrados ordinarios (MCO) 3. Explorar qué hace un buen estiamdor y cómo evaluarlo 🔍 ¿Qué significa encontrar la “mejor línea”? Antes de hablar de estimaciones, pensemos en cómo se generan los datos: Supondremos que hay un modelo poblacional o proceso generador de datos: \\[ y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i \\] \\(y_i\\): variable dependiente (lo que queremos explicar) \\(x_i\\): variable independiente \\(\\beta_0, \\beta_1\\): parámetros poblacionales \\(\\epsilon_i\\): término de error: todo lo que afecta a \\(y_i\\) y no está en \\(x_i\\) El término \\(\\epsilon_i\\) captura factores no observados, errores de medición, y variación aleatoria. Es fundamental porque incluso si tuviéramos los valores verdaderos de \\(\\beta_0\\) y \\(\\beta_1\\), seguiríamos sin poder predecir perfectamente \\(y_i\\) debido a este componente. En la práctica, estimamos los parámetros a partir de una muestra. Esto nos da una versión estimada del modelo: \\[ \\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i \\] Y calculamos los residuos (errores estimados): \\[ \\hat{\\epsilon}_i = y_i - \\hat{y}_i \\] Queremos encontrar la línea que prediga \\(y_i\\) con la menor cantidad posible de errores. Eso significa minimizar: \\[ \\text{SRC} = \\sum_{i = 1}^{n} \\hat{\\epsilon}_i^2 \\] Esto se conoce como el criterio de mínimos cuadrados. 🎨 Ilustremos esto con un ejemplo visual Creemos unos nuevos datos para ilustrar esto. La linea de regresión es igual a \\(\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x\\) donde _0$ y \\(\\hat{\\beta}_1\\) son los parámetros estimados de la regresión. En este caso, \\(\\hat{\\beta}_0 = 6\\) y \\(\\hat{\\beta}_1 = 0.2\\). Para cada una de las observaciones podemos encontrar el y estimado \\(\\hat{y}_i\\). En la siguiente figura, la línea naranja representa la línea de regresión estimada. Para cada una de las observaciones podemos calcular los errores: \\(\\epsilon_i = y_i - \\hat{y}_i\\), como se observa en el siguiente gráfico. Ahora podemos probar con otras lineas y ver cómo se comportan los errores. En el siguiente grafico, la línea de regresión estimada es \\(\\hat{y} = 3 + 0.2 x\\). Es evidente que los errores estiamdos son más grandes que los errores estimados en el gráfico anterior. Probemos ahora con una línea de regresión estimada que no se ajusta a los datos, \\(\\hat{y} = 10 - 0.8 x\\). En este caso, los errores son aún más grandes. Recuerda que SRC es igual a: \\(\\left(\\sum e_i^2\\right)\\): Errores más grandes reciben penalizaciones más grandes. La estimación de MCO es la combinación de \\(\\hat{\\beta}_0\\) y \\(\\hat{\\beta}_1\\) que minimiza la SRC MCO Formalmente En una regresión lineal simple, el estimador de MCO proviene de escoger \\(\\hat{\\beta}_0\\) y \\(\\hat{\\beta}_1\\) que minimice la suma de residuos al cuadrado (SRC), i.e., \\[ \\min_{\\hat{\\beta}_0,\\, \\hat{\\beta}_1} \\text{SRC} \\] donde \\[ \\text{SRC} = \\sum_{i = 1}^{n} (y_i - \\hat{y}_i)^2 = \\sum_{i = 1}^{n} (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)^2 \\] El estimador de MCO es el valor de \\(\\hat{\\beta}_0\\) y \\(\\hat{\\beta}_1\\) que minimiza la SRC. pero nosotros sabemos que \\(\\text{SRC} = \\sum_i \\tilde{\\epsilon_i}^2\\). Now use the definitions of \\(\\tilde{\\epsilon_i}\\) and \\(\\hat{y}\\). \\[ \\begin{aligned} \\tilde{\\epsilon_i}^2 &amp;= \\left( y_i - \\hat{y}_i \\right)^2 = \\left( y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i \\right)^2 \\\\ &amp;= y_i^2 - 2 y_i \\hat{\\beta}_0 - 2 y_i \\hat{\\beta}_1 x_i + \\hat{\\beta}_0^2 + 2 \\hat{\\beta}_0 \\hat{\\beta}_1 x_i + \\hat{\\beta}_1^2 x_i^2 \\end{aligned} \\] Recordatorio: Minimizar una función multivariada requiere (1) que las primeras derivadas sean iguales a cero (las condiciones de primer orden) y (2) las condiciones de segundo orden (concavidad). Nos estamos acercando. Necesitamos minimizar la SRC. \\[ \\text{SRE} = \\sum_i \\tilde{e_i}^2 = \\sum_i \\left( y_i^2 - 2 y_i \\hat{\\beta}_0 - 2 y_i \\hat{\\beta}_1 x_i + \\hat{\\beta}_0^2 + 2 \\hat{\\beta}_0 \\hat{\\beta}_1 x_i + \\hat{\\beta}_1^2 x_i^2 \\right) \\] For the first-order conditions of minimization, we now take the first derivates of SSE with respect to \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\). \\[ \\begin{aligned} \\dfrac{\\partial \\text{SRC}}{\\partial \\hat{\\beta}_0} &amp;= \\sum_i \\left( 2 \\hat{\\beta}_0 + 2 \\hat{\\beta}_1 x_i - 2 y_i \\right) = 2n \\hat{\\beta}_0 + 2 \\hat{\\beta}_1 \\sum_i x_i - 2 \\sum_i y_i \\\\ &amp;= 2n \\hat{\\beta}_0 + 2n \\hat{\\beta}_1 \\overline{x} - 2n \\overline{y} \\end{aligned} \\] donde \\(\\overline{x} = \\frac{\\sum x_i}{n}\\) y \\(\\overline{y} = \\frac{\\sum y_i}{n}\\) son medias muestrales de \\(x\\) y \\(y\\) (de tamaño \\(n\\)). Las condiciones de primer orden establecen que las derivadas deben ser iguales a cero: \\[ \\dfrac{\\partial \\text{SSE}}{\\partial \\hat{\\beta}_0} = 2n \\hat{\\beta}_0 + 2n \\hat{\\beta}_1 \\overline{x} - 2n \\overline{y} = 0 \\] Lo que implica \\[ \\hat{\\beta}_0 = \\overline{y} - \\hat{\\beta}_1 \\overline{x} \\] Ahora para \\(\\hat{\\beta}_1\\). Tomemos la derivada de la SRC con respecto a \\(\\hat{\\beta}_1\\) \\[ \\begin{aligned} \\dfrac{\\partial \\text{SSE}}{\\partial \\hat{\\beta}_1} &amp;= \\sum_i \\left( 2 \\hat{\\beta}_0 x_i + 2 \\hat{\\beta}_1 x_i^2 - 2 y_i x_i \\right) = 2 \\hat{\\beta}_0 \\sum_i x_i + 2 \\hat{\\beta}_1 \\sum_i x_i^2 - 2 \\sum_i y_i x_i \\\\ &amp;= 2n \\hat{\\beta}_0 \\overline{x} + 2 \\hat{\\beta}_1 \\sum_i x_i^2 - 2 \\sum_i y_i x_i \\end{aligned} \\] Igualarlo a cero \\[ \\dfrac{\\partial \\text{SSE}}{\\partial \\hat{\\beta}_1} = 2n \\hat{\\beta}_0 \\overline{x} + 2 \\hat{\\beta}_1 \\sum_i x_i^2 - 2 \\sum_i y_i x_i = 0 \\] y reemplazarlo \\(\\hat{\\beta}_0\\), i.e., \\(\\hat{\\beta}_0 = \\overline{y} - \\hat{\\beta}_1 \\overline{x}\\). Thus, \\[ 2n \\left(\\overline{y} - \\hat{\\beta}_1 \\overline{x}\\right) \\overline{x} + 2 \\hat{\\beta}_1 \\sum_i x_i^2 - 2 \\sum_i y_i x_i = 0 \\] Continuando \\[ 2n \\left(\\overline{y} - \\hat{\\beta}_1 \\overline{x}\\right) \\overline{x} + 2 \\hat{\\beta}_1 \\sum_i x_i^2 - 2 \\sum_i y_i x_i = 0 \\] \\[ 2n \\overline{y}\\,\\overline{x} - 2n \\hat{\\beta}_1 \\overline{x}^2 + 2 \\hat{\\beta}_1 \\sum_i x_i^2 - 2 \\sum_i y_i x_i = 0 \\] \\[ \\implies 2 \\hat{\\beta}_1 \\left( \\sum_i x_i^2 - n \\overline{x}^2 \\right) = 2 \\sum_i y_i x_i - 2n \\overline{y}\\,\\overline{x} \\] \\[ \\implies \\hat{\\beta}_1 = \\dfrac{\\sum_i y_i x_i - 2n \\overline{y}\\,\\overline{x}}{\\sum_i x_i^2 - n \\overline{x}^2} = \\dfrac{\\sum_i (x_i - \\overline{x})(y_i - \\overline{y})}{\\sum_i (x_i - \\overline{x})^2} \\] LISTOO! Ahora tenemos nuestros lindos estimadores \\[ \\hat{\\beta}_1 = \\dfrac{\\sum_i (x_i - \\overline{x})(y_i - \\overline{y})}{\\sum_i (x_i - \\overline{x})^2} \\] and the intercept \\[ \\hat{\\beta}_0 = \\overline{y} - \\hat{\\beta}_1 \\overline{x} \\] Ya sabes de dónde proviene la parte de mínimos cuadrados en el término “mínimos cuadrados ordinarios”. 🎊 Ahora pasamos a las propiedades (implícitas) de los Mínimos Cuadrados Ordinarios (MCO / OLS). 📊 Propiedades y supuestos ¿Qué hace a un buen estimador? Antes de hablar de propiedades del estimador de MCO, recordemos algunas herramientas fundamentales de estadística. 3.0.1 📈 Repaso: Funciones de densidad Las funciones de densidad de probabilidad (FDP, o PDF en inglés) describen la probabilidad de que una variable aleatoria continua tome valores dentro de un intervalo dado. La probabilidad total bajo la curva es 1. Ejemplo: para una variable normal estándar, la probabilidad de que tome un valor entre -2 y 0 es: \\[ \\mathop{\\text{P}}\\left(-2 \\leq X \\leq 0\\right) = 0.48 \\] Otro ejemplo clásico es la probabilidad de que una variable aleatoria normal estándar tome un valor entre -1.96 y 1.96: \\(\\mathop{\\text{P}}\\left(-1.96 \\leq X \\leq 1.96\\right) = 0.95\\) O la probabilidad de que una variable aleatoria normal estándar tome un valor mayor a 2: \\(\\mathop{\\text{P}}\\left(X &gt; 2\\right) = 0.023\\) 🤔 ¿Qué propiedades buscamos en un estimador? Imaginemos que intentamos estimar un parámetro verdadero \\(\\beta\\), y tenemos tres métodos distintos. Cada uno produce una distribución diferente para \\(\\hat{\\beta}\\). Pregunta: ¿Qué propiedades podrían ser importantes para un estimador? Propiedad 1. Insesgamiento Es decir, si repitiéramos el experimento muchas veces, ¿el estimador tiende a acercarse al valor verdadero del parámetro que estamos tratando de estimar? El sesgo mide si el estimador se acerca al valor real en promedio: 🧪 ¿Qué significa “repetir el experimento”? En este contexto, repetir el experimento puede entenderse de tres formas, todas válidas para pensar en la incertidumbre de un estimador: Cambiar la muestra: imaginar que tomamos muchas muestras aleatorias distintas de la población. Mantener fija la muestra, pero cambiar los errores: incluso si los valores de \\(x_i\\) no cambian, los valores de \\(y_i\\) pueden variar si asumimos que los errores \\(\\epsilon_i\\) son aleatorios. Recuerda que \\(y_i\\) sigue un proceso generador de datos subyacente. Cambiar ambos simultáneamente: es el caso más común en simulaciones — se sortean tanto los \\(x_i\\) como los \\(\\epsilon_i\\). En cualquiera de los tres escenarios, obtendríamos distintos valores de \\(\\hat{\\beta}\\). Eso nos permite construir una distribución muestral del estimador y analizar propiedades como el sesgo. ⚠️ Importante: cuando hablamos de “repetir el experimento”, no queremos decir que volvamos a observar a las mismas personas varias veces con diferentes valores de \\(x\\) (por ejemplo, dándoles distintos años de educación). Lo que estamos haciendo es imaginar escenarios hipotéticos en los que la muestra o los errores cambian, y ver cómo eso afecta al estimador. Estos experimentos no se pueden realizar en la realidad con una misma persona, pero sí los podemos simular por computadora o analizar teóricamente. Más formalmente: ¿La media de la distribución del estimador es igual al parámetro que estima? En promedio (después de muchas repeticiones), ¿el estimador tiende hacia el valor correcto? Más formalmente: ¿La media de la distribución del estimador es igual al parámetro que estima? \\[ \\mathop{\\text{Sesgo}}_\\beta \\left( \\hat{\\beta} \\right) = \\mathop{\\boldsymbol{E}}\\left[ \\hat{\\beta} \\right] - \\beta \\] Estimador Insesagado: \\(\\mathop{\\boldsymbol{E}}\\left[ \\hat{\\beta} \\right] = \\beta\\) Estimador Sesagado: \\(\\mathop{\\boldsymbol{E}}\\left[ \\hat{\\beta} \\right] \\neq \\beta\\) Propiedad 2: Varianza También queremos que nuestras estimaciones no varíen demasiado de una muestra a otra. En otras palabras: queremos un estimador que sea estable, no que en cada muestra nos dé un valor completamente distinto. La varianza mide cuánta variación hay en las estimaciones \\(\\hat{\\beta}\\) que obtenemos al repetir el experimento (cambiando la muestra, los errores, o ambos): \\[ \\text{Var} \\left( \\hat{\\beta} \\right) = \\mathbb{E} \\left[ \\left( \\hat{\\beta} - \\mathbb{E}[\\hat{\\beta}] \\right)^2 \\right] \\] Un estimador con menor varianza produce resultados más consistentes entre muestras. Esto lo hace más confiable, incluso si no es perfecto. 🎯 Queremos que nuestras estimaciones estén “concentradas” cerca del valor esperado, no dispersas como tiros al aire. Veamos un ejemplo visual de cómo la varianza afecta a las distribuciones de los estimadores. La curva rosada representa un estimador con baja varianza: la mayoría de los valores de \\(\\hat{\\beta}\\) están cerca de \\(\\beta\\). Mientras que la curva gris oscuro representa un estimador con alta varianza: sus valores están más dispersos. A igualdad de sesgo, preferimos el estimador que tenga menor varianza. 🎯 El trade-off: sesgo vs. varianza Hasta ahora hablamos del sesgo y de la varianza por separado. Pero muchas veces, mejorar uno implica empeorar el otro. Esto se conoce como el ** trade-off entre sesgo y varianza**. ¿Deberíamos aceptar un poco de sesgo si eso nos permite reducir mucho la varianza? En econometría, solemos preferir estimadores insesgados (o al menos consistentes), porque valoramos la interpretación causal y teórica de los parámetros. Pero en otras disciplinas, como el aprendizaje automático o la predicción estadística, es común aceptar un pequeño sesgo si con ello se logra una gran reducción en la varianza y, en consecuencia, una mejor predicción promedio. veámos esta idea: Propiedad 3: Consistencia La consistencia es una propiedad clave que nos dice qué pasa con el estimador cuando la muestra es cada vez más grande. Intuitivamente, un estimador es consistente si, al aumentar el tamaño de la muestra, sus valores se acercan cada vez más al valor verdadero del parámetro \\(\\beta\\). Esto nos da confianza de que, con datos suficientes, estaremos muy cerca del valor correcto. Formalmente, un estimador \\(\\hat{\\beta}\\) es consistente si: \\[ \\hat{\\beta} \\xrightarrow{p} \\beta \\quad \\text{cuando } n \\to \\infty \\] Esto se lee como: “\\(\\hat{\\beta}\\) converge en probabilidad a \\(\\beta\\)”. Es decir, la probabilidad de que \\(\\hat{\\beta}\\) se aleje mucho de \\(\\beta\\) se hace cada vez más pequeña a medida que usamos muestras más grandes. Veámos lo que ocurre cuando la muestra crece: La curva naranja representa una estimación con mucha incertidumbre (muestra pequeña). La curva gris oscura representa una muestra de tamaño mediano. Mientras que la curva rosada muestra cómo la estimación se concentra alrededor de \\(\\beta\\) con una muestra grande. 🎯 Un estimador consistente se “afina” con más datos: no solo mejora su varianza, sino que tiende a decir la verdad. Propiedad 4: Eficiencia La eficiencia combina las ideas de sesgo y varianza. Entre todos los estimadores insesgados, el más eficiente es aquel que tiene la menor varianza posible. Es decir, si dos estimadores son igual de “correctos en promedio”, preferimos el que sea más estable. La eficiencia no se refiere a un único estimador, sino a una comparación entre estimadores. Formalmente, un estimador \\(\\hat{\\beta}\\) es eficiente si: \\[ \\text{Var}(\\hat{\\beta}) \\leq \\text{Var}(\\hat{\\beta}&#39;) \\] para cualquier otro estimador \\(\\hat{\\beta}&#39;\\) que también sea insesgado. Esto significa que ningún otro estimador insesgado tiene una varianza menor que \\(\\hat{\\beta}\\). 💡 En el contexto de mínimos cuadrados ordinarios (MCO), cuando se cumplen ciertos supuestos (los del teorema de Gauss-Markov), el estimador \\(\\hat{\\beta}_{\\text{MCO}}\\) es el Mejor Estimador Lineal Insesgado, también conocido como MELI: ✔️ Mejor → tiene la menor varianza ✔️ Estimador Lineal → combinación lineal de los datos ✔️ Insesgado → \\(\\mathbb{E}[\\hat{\\beta}] = \\beta\\) 🎓 En resumen, un estimador eficiente es tan preciso como permite la información disponible en los datos, sin sacrificar insesgamiento. Resumen de las propiedades Propiedad Descripción Insesgamiento El estimador no se aleja sistemáticamente del valor verdadero. Varianza El estimador tiene poca variación entre muestras. Consistencia A medida que aumenta el tamaño de la muestra, el estimador converge al valor verdadero. Eficiencia El estimador tiene la menor varianza posible entre todos los estimadores insesgados. 🧠 Nota de cierre: cómo interpretar cada propiedad Cada propiedad que vimos tiene un enfoque ligeramente distinto sobre cómo pensar la incertidumbre: Sesgo: ¿En promedio (tras repetir el experimento), el estimador acierta? Varianza: ¿Qué tanto cambia el estimador de una muestra a otra? Consistencia: ¿El estimador se acerca al valor verdadero si usamos una muestra más grande del mismo experimento? Eficiencia: ¿Este estimador es mejor (más preciso) que otros estimadores insesgados disponibles? 🔁 Las primeras dos propiedades (sesgo y varianza) se entienden a través de repeticiones hipotéticas del experimento. 📈 La consistencia se analiza observando lo que ocurre cuando crece el tamaño muestral. ⚖️ La eficiencia es una comparación entre estimadores, dado que todos sean insesgados. Estas ideas son fundamentales para entender cómo evaluar y justificar un estimador en econometría. 📘 Preguntas de repaso Verdadero o falso (V/F) Un estimador puede ser insesgado pero tener alta varianza. (V/F) La consistencia se refiere a repetir el experimento muchas veces. (V/F) Un estimador eficiente siempre es consistente. (V/F) Si un estimador es insesgado y eficiente, no puede ser mejorado bajo los supuestos del modelo. Selección múltiple {-} ¿Cuál de las siguientes afirmaciones es correcta respecto a la eficiencia? A. Es una propiedad absoluta de un estimador. B. Se refiere a qué tan cerca está \\(\\hat{\\beta}\\) del promedio de los datos. C. Compara la varianza entre estimadores insesgados. D. Es sinónimo de consistencia. ¿Qué pasa con un estimador consistente cuando el tamaño muestral crece? A. Se vuelve insesgado automáticamente. B. Se aleja del valor verdadero. C. Su varianza se hace infinita. D. Se aproxima al valor verdadero con alta probabilidad. Respuesta abierta Explica con tus palabras qué significa que un estimador sea insesgado. ¿Por qué esta propiedad es importante en econometría?** ¿Por qué puede ser útil, en algunos contextos, aceptar un estimador sesgado? Da un ejemplo donde podría ser preferible.** ¿En qué se diferencia el concepto de varianza del de eficiencia? ¿Pueden dos estimadores tener la misma varianza pero distinta eficiencia?** Supón que tienes dos estimadores: - A es insesgado pero tiene alta varianza. - B tiene un pequeño sesgo pero varianza muy baja. ¿Cuál elegirías para un problema donde la prioridad es predecir bien el valor de \\(y\\)? ¿Cambiaría tu respuesta si el objetivo fuera estimar un efecto causal? Justifica tu elección. "],["álgebra-matricial-para-econometría.html", "4 Álgebra Matricial para Econometría Matrices Vectores Operaciones con matrices Determinantes Matriz inversa Rango de una matriz Sistemas de ecuaciones lineales Matrices cuadradas especiales Derivadas de una función multidimensional 📘 Preguntas de repaso", " 4 Álgebra Matricial para Econometría Antes de introducir los supuestos fundamentales del modelo de regresión lineal, es importante repasar algunos conceptos clave del álgebra matricial. Este lenguaje permite expresar de forma compacta y elegante muchos de los resultados econométricos, facilitando la comprensión de los modelos lineales y sus propiedades. Matrices Una matriz \\(A \\in \\mathbb{R}^{m \\times n}\\) es un conjunto de elementos \\(a_{ij}\\), donde \\(i = 1, \\ldots, m\\) (filas) y \\(j = 1, \\ldots, n\\) (columnas), organizados de la siguiente manera: \\[ A = \\begin{pmatrix} a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1n} \\\\ a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{m1} &amp; a_{m2} &amp; \\cdots &amp; a_{mn} \\end{pmatrix} \\] La dimensión o orden de la matriz es \\(m \\times n\\), lo que indica que tiene \\(m\\) filas y \\(n\\) columnas. Cuando \\(m = n\\), se dice que la matriz es cuadrada; si \\(m \\neq n\\), es una matriz rectangular. Las matrices se representan con letras mayúsculas en negrita, como \\(\\mathbf{A}\\), y sus elementos con letras minúsculas con subíndices, como \\(a_{ij}\\). Los elementos de una matriz pueden ser números reales, \\(a_{ij} \\in \\mathbb{R}\\). Ejemplo. La matriz \\[ B = \\begin{pmatrix} 3 &amp; 1 &amp; 7 \\\\ 2 &amp; 4 &amp; 5 \\end{pmatrix} \\] es una matriz rectangular de orden \\(2 \\times 3\\). Tiene 2 filas y 3 columnas. El elemento en la fila 2 y columna 3 es \\(b_{23} = 5\\). Traspuesta de una matriz La traspuesta de una matriz \\(A = [a_{ij}]\\) de dimensión \\(m \\times n\\) es otra matriz \\(A&#39; = [a_{ji}]\\) de dimensión \\(n \\times m\\), obtenida al intercambiar filas por columnas. Es decir, la primera fila de \\(A\\) se convierte en la primera columna de \\(A&#39;\\), la segunda fila en la segunda columna, y así sucesivamente. \\[ A&#39; = \\begin{pmatrix} a_{11} &amp; a_{21} &amp; \\cdots &amp; a_{m1} \\\\ a_{12} &amp; a_{22} &amp; \\cdots &amp; a_{m2} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{1n} &amp; a_{2n} &amp; \\cdots &amp; a_{mn} \\end{pmatrix} \\] Ejemplo. Sea la matriz \\[ A = \\begin{pmatrix} 6 &amp; 5 &amp; 7 &amp; 4 \\\\ 5 &amp; 4 &amp; 2 &amp; 5 \\\\ 1 &amp; 1 &amp; 11 &amp; 1 \\end{pmatrix} \\] que es de orden \\(3 \\times 4\\). Su traspuesta es: \\[ A&#39; = \\begin{pmatrix} 6 &amp; 5 &amp; 1 \\\\ 5 &amp; 4 &amp; 1 \\\\ 7 &amp; 2 &amp; 11 \\\\ 4 &amp; 5 &amp; 1 \\end{pmatrix} \\] Esta nueva matriz es de orden \\(4 \\times 3\\). Vectores Un vector columna es una matriz de orden \\(m \\times 1\\), es decir, una matriz que solo tiene una columna: \\[ \\mathbf{a} = \\begin{pmatrix} a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_m \\end{pmatrix} \\] Se denota con una letra minúscula en negrita y se puede escribir de forma abreviada como \\(\\mathbf{a} = [a_i]\\). Cada elemento \\(a_i\\) indica la posición del componente dentro del vector. Un vector fila, en cambio, es una matriz de orden \\(1 \\times m\\), es decir, solo tiene una fila: \\[ \\mathbf{a}&#39; = \\begin{pmatrix} a_1 &amp; a_2 &amp; \\cdots &amp; a_m \\end{pmatrix} \\] La traspuesta de un vector columna es un vector fila, y viceversa. En línea, se escribe \\(\\mathbf{a} = (a_1, a_2, \\ldots, a_m)&#39;\\) para indicar que es columna, usando la notación de traspuesta. Ejemplo Sea el vector columna \\[ \\mathbf{v} = \\begin{pmatrix} 2 \\\\ -1 \\\\ 4 \\end{pmatrix} \\quad \\text{de orden } 3 \\times 1, \\quad \\text{su traspuesta es } \\mathbf{v}&#39; = \\begin{pmatrix} 2 &amp; -1 &amp; 4 \\end{pmatrix} \\] Producto escalar Definición. Sean \\(\\mathbf{a} = (a_1, \\ldots, a_m)&#39;\\) y \\(\\mathbf{b} = (b_1, \\ldots, b_m)&#39;\\) dos vectores columna del mismo orden \\(m \\times 1\\), su producto escalar es: \\[ \\mathbf{a}&#39;\\mathbf{b} = \\sum_{i=1}^m a_i b_i = a_1b_1 + a_2b_2 + \\cdots + a_m b_m \\] Ejemplo Si \\(\\mathbf{a} = (1, 2, 3)&#39;\\) y \\(\\mathbf{b} = (4, 5, 6)&#39;\\), entonces: \\[ \\mathbf{a}&#39;\\mathbf{b} = 1 \\cdot 4 + 2 \\cdot 5 + 3 \\cdot 6 = 4 + 10 + 18 = 32 \\] Norma y normalización Definición. La norma de un vector \\(\\mathbf{x}\\) se define como: \\[ \\|\\mathbf{x}\\| = \\sqrt{\\mathbf{x}&#39;\\mathbf{x}} = \\sqrt{x_1^2 + x_2^2 + \\cdots + x_m^2} \\] El vector normalizado es: \\[ \\frac{\\mathbf{x}}{\\|\\mathbf{x}\\|} \\] Ortogonalidad Definición. Dos vectores \\(\\mathbf{a}\\) y \\(\\mathbf{b}\\) son ortogonales (se denota \\(\\mathbf{a} \\perp \\mathbf{b}\\)) si: \\[ \\mathbf{a}&#39;\\mathbf{b} = 0 \\] \\[ \\bar{y} = \\frac{\\mathbf{i}&#39;\\mathbf{y}}{\\mathbf{i}&#39;\\mathbf{i}} \\] Operaciones con matrices Igualdad de matrices Dos matrices \\(A = [a_{ij}]\\) y \\(B = [b_{ij}]\\) de igual orden \\(m \\times n\\) son iguales si: \\[ a_{ij} = b_{ij}, \\quad \\text{para todo } i = 1, \\ldots, m; \\; j = 1, \\ldots, n \\] Suma y resta de matrices La suma de dos matrices del mismo orden es la matriz \\(C = A + B = [c_{ij}]\\) donde: \\[ c_{ij} = a_{ij} + b_{ij} \\] Propiedades: Conmutativa: \\(A + B = B + A\\) Asociativa: \\((A + B) + C = A + (B + C)\\) Elemento neutro: \\(A + 0 = A\\) Opuesto: \\(A + (-A) = 0\\) Ejemplo: \\[ A = \\begin{pmatrix} 6 &amp; 5 &amp; 7 &amp; 4 \\\\ 5 &amp; 4 &amp; 2 &amp; 5 \\\\ 1 &amp; 1 &amp; 11 &amp; 1 \\end{pmatrix}, \\quad B = \\begin{pmatrix} 7 &amp; 11 &amp; 2 &amp; 9 \\\\ 5 &amp; 8 &amp; 8 &amp; 1 \\\\ 6 &amp; 10 &amp; 8 &amp; 10 \\end{pmatrix} \\] \\[ A + B = \\begin{pmatrix} 13 &amp; 16 &amp; 9 &amp; 13 \\\\ 10 &amp; 12 &amp; 10 &amp; 6 \\\\ 7 &amp; 11 &amp; 19 &amp; 11 \\end{pmatrix} \\] Resta: se define como \\(A - B = A + (-B)\\) Multiplicación por un escalar \\[ \\lambda A = [\\lambda a_{ij}] \\] Ejemplo: \\[ 2A = \\begin{pmatrix} 12 &amp; 10 &amp; 14 &amp; 8 \\\\ 10 &amp; 8 &amp; 4 &amp; 10 \\\\ 2 &amp; 2 &amp; 22 &amp; 2 \\end{pmatrix} \\] Multiplicación de matrices Sean \\(A \\in \\mathbb{R}^{m \\times n}\\) y \\(B \\in \\mathbb{R}^{n \\times p}\\), el producto \\(AB \\in \\mathbb{R}^{m \\times p}\\) se define por: \\[ c_{ij} = \\sum_{k=1}^{n} a_{ik} b_{kj} \\] Propiedades: Asociativa: \\((AB)C = A(BC)\\) Distributiva: \\(A(B + C) = AB + AC\\) No conmutativa: en general \\(AB \\neq BA\\) Ejemplo: \\[ A = \\begin{pmatrix} 6 &amp; 5 &amp; 7 &amp; 4 \\\\ 5 &amp; 4 &amp; 2 &amp; 5 \\\\ 1 &amp; 1 &amp; 11 &amp; 1 \\end{pmatrix}, \\quad B&#39; = \\begin{pmatrix} 7 &amp; 5 &amp; 6 \\\\ 11 &amp; 8 &amp; 10 \\\\ 2 &amp; 8 &amp; 8 \\\\ 9 &amp; 1 &amp; 10 \\end{pmatrix} \\] \\[ F = A B&#39; = \\begin{pmatrix} 147 &amp; 130 &amp; 182 \\\\ 128 &amp; 78 &amp; 136 \\\\ 49 &amp; 102 &amp; 114 \\end{pmatrix} \\] Transposición de matrices Ya definida en la sección anterior. Propiedades clave: \\((A&#39;)&#39; = A\\) \\((A + B)&#39; = A&#39; + B&#39;\\) \\((AB)&#39; = B&#39;A&#39;\\) Traza de una matriz La traza de una matriz cuadrada es la suma de los elementos de su diagonal principal: \\[ \\text{tr}(A) = \\sum_{i=1}^{n} a_{ii} \\] Propiedades: \\(\\text{tr}(A) = \\text{tr}(A&#39;)\\) \\(\\text{tr}(A + B) = \\text{tr}(A) + \\text{tr}(B)\\) \\(\\text{tr}(AB) = \\text{tr}(BA)\\) Ejemplo: \\[ \\text{tr}(F) = 147 + 78 + 114 = 339 \\] Determinantes Para matrices cuadradas \\(A \\in \\mathbb{R}^{n \\times n}\\), el determinante se denota \\(|A|\\). Para \\(2 \\times 2\\): \\[ |A| = \\begin{vmatrix} a &amp; b \\\\ c &amp; d \\end{vmatrix} = ad - bc \\] Para \\(3 \\times 3\\): \\[ |A| = a_{11}a_{22}a_{33} + a_{12}a_{23}a_{31} + a_{13}a_{21}a_{32} - a_{13}a_{22}a_{31} - a_{12}a_{21}a_{33} - a_{11}a_{23}a_{32} \\] Ejemplo: \\[ G = \\begin{pmatrix} 1 &amp; 1 &amp; 3 \\\\ 1 &amp; 1 &amp; 0 \\\\ 3 &amp; 1 &amp; 2 \\end{pmatrix} \\Rightarrow |G| = -6 \\] Matriz inversa Una matriz cuadrada \\(A\\) es invertible si existe \\(A^{-1}\\) tal que: \\[ A A^{-1} = A^{-1} A = I \\] Se calcula como: \\[ A^{-1} = \\frac{1}{|A|} \\cdot \\text{adj}(A) \\] donde \\(\\text{adj}(A)\\) es la matriz adjunta (traspuesta de los cofactores). Propiedades: \\((A^{-1})^{-1} = A\\) \\((AB)^{-1} = B^{-1} A^{-1}\\) \\((A&#39;)^{-1} = (A^{-1})&#39;\\) Ejemplo: \\[ G^{-1} = \\frac{1}{-6} \\begin{pmatrix} 2 &amp; 1 &amp; -3 \\\\ -2 &amp; -7 &amp; 3 \\\\ -2 &amp; 2 &amp; 0 \\end{pmatrix} \\] Rango de una matriz El rango de una matriz es el número máximo de filas (o columnas) linealmente independientes. Definiciones: Vectores son linealmente dependientes si \\(c_1a_1 + \\cdots + c_n a_n = 0\\) con \\(c_i \\neq 0\\) Son independientes si la única combinación que da cero es con todos los \\(c_i = 0\\) Propiedades: \\(\\text{rang}(AB) \\leq \\min\\{\\text{rang}(A), \\text{rang}(B)\\}\\) Si \\(A\\) es invertible: \\(\\text{rang}(AB) = \\text{rang}(B)\\) \\(\\text{rang}(A) = \\text{rang}(A A&#39;) = \\text{rang}(A&#39; A)\\) Sistemas de ecuaciones lineales Un sistema de ecuaciones lineales con \\(m\\) ecuaciones y \\(n\\) incógnitas se puede escribir de la forma: \\[ \\begin{aligned} a_{11}x_1 + a_{12}x_2 + \\cdots + a_{1n}x_n &amp;= b_1 \\\\ a_{21}x_1 + a_{22}x_2 + \\cdots + a_{2n}x_n &amp;= b_2 \\\\ &amp;\\vdots \\\\ a_{m1}x_1 + a_{m2}x_2 + \\cdots + a_{mn}x_n &amp;= b_m \\end{aligned} \\] En forma matricial, este sistema se escribe como: \\[ A \\mathbf{x} = \\mathbf{b} \\] donde \\[ A = \\begin{pmatrix} a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1n} \\\\ a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{m1} &amp; a_{m2} &amp; \\cdots &amp; a_{mn} \\end{pmatrix}, \\quad \\mathbf{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{pmatrix}, \\quad \\mathbf{b} = \\begin{pmatrix} b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_m \\end{pmatrix} \\] Sistema de Cramer Definición. Un sistema de ecuaciones lineales se denomina sistema de Cramer si: La matriz \\(A\\) es cuadrada (\\(m = n\\)) La matriz \\(A\\) es no singular, es decir, \\(|A| \\neq 0\\) En este caso, el sistema tiene una única solución dada por: \\[ \\mathbf{x} = A^{-1} \\mathbf{b} \\] Ejemplo numérico Considere el sistema: \\[ \\begin{aligned} 12x_1 + 20x_2 &amp;= 388 \\\\ 4x_1 + 17x_2 &amp;= 212 \\end{aligned} \\] En forma matricial: \\[ \\begin{pmatrix} 12 &amp; 20 \\\\ 4 &amp; 17 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} 388 \\\\ 212 \\end{pmatrix} \\] Paso 1: Calcular la inversa de \\(A\\) Primero calculamos el determinante: \\[ |A| = 12 \\cdot 17 - 20 \\cdot 4 = 204 - 80 = 124 \\] Luego, la matriz de cofactores traspuesta (adjunta): \\[ \\text{adj}(A) = \\begin{pmatrix} 17 &amp; -20 \\\\ -4 &amp; 12 \\end{pmatrix} \\] Entonces, \\[ A^{-1} = \\frac{1}{124} \\begin{pmatrix} 17 &amp; -20 \\\\ -4 &amp; 12 \\end{pmatrix} \\] Paso 2: Multiplicar \\(A^{-1} \\mathbf{b}\\) \\[ \\mathbf{x} = A^{-1} \\mathbf{b} = \\frac{1}{124} \\begin{pmatrix} 17 &amp; -20 \\\\ -4 &amp; 12 \\end{pmatrix} \\begin{pmatrix} 388 \\\\ 212 \\end{pmatrix} \\] Calculamos el producto: \\[ \\begin{aligned} x_1 &amp;= \\frac{1}{124}(17 \\cdot 388 - 20 \\cdot 212) = \\frac{1}{124}(6596 - 4240) = \\frac{2356}{124} = 19 \\\\ x_2 &amp;= \\frac{1}{124}(-4 \\cdot 388 + 12 \\cdot 212) = \\frac{1}{124}(-1552 + 2544) = \\frac{992}{124} = 8 \\end{aligned} \\] Solución final: \\[ \\boxed{ x_1 = 19, \\quad x_2 = 8 } \\] Este procedimiento es válido siempre que la matriz \\(A\\) sea cuadrada y su determinante no sea cero. Si \\(|A| = 0\\), el sistema no tiene solución única: puede ser incompatible o tener infinitas soluciones. Matrices cuadradas especiales Las siguientes matrices cuadradas tienen propiedades estructurales claves que facilitan el desarrollo de métodos econométricos. 1. Matriz diagonal Una matriz diagonal \\(A = [a_{ij}] \\in \\mathbb{R}^{m \\times m}\\) tiene ceros fuera de la diagonal principal: \\[ A = \\begin{pmatrix} a_{11} &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; a_{22} &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; a_{mm} \\end{pmatrix} = \\text{diag}(a_{11}, a_{22}, \\ldots, a_{mm}) \\] 2. Matriz identidad La matriz identidad \\(I_m\\) es una matriz diagonal con unos en la diagonal: \\[ I_m = \\begin{pmatrix} 1 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; 1 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; 1 \\end{pmatrix} \\] Propiedades: \\(AI_m = I_mA = A\\), \\(I_m^{-1} = I_m\\) 3. Matriz escalar Una matriz escalar es una matriz diagonal cuyos elementos en la diagonal son iguales a un mismo número \\(\\lambda\\): \\[ A = \\lambda I_m \\] 4. Matriz triangular inferior Una matriz triangular inferior cumple: \\[ a_{ij} = 0 \\quad \\text{para todo } i &lt; j \\] \\[ A = \\begin{pmatrix} a_{11} &amp; 0 &amp; \\cdots &amp; 0 \\\\ a_{21} &amp; a_{22} &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{m1} &amp; a_{m2} &amp; \\cdots &amp; a_{mm} \\end{pmatrix} \\] 5. Matriz nula La matriz nula tiene todos sus elementos iguales a cero: \\[ 0 = \\begin{pmatrix} 0 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; 0 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; 0 \\end{pmatrix} \\] 6. Matriz simétrica Una matriz \\(A \\in \\mathbb{R}^{m \\times m}\\) es simétrica si: \\[ A = A&#39; \\quad \\text{es decir, } a_{ij} = a_{ji} \\] 7. Matriz idempotente Una matriz \\(A\\) es idempotente si: \\[ A^2 = A \\] Ejemplo clave: la matriz de proyección sobre el espacio generado por las columnas de \\(X\\): \\[ P = X(X&#39;X)^{-1}X&#39; \\] Cumple: \\(P = P&#39;\\) (simétrica) \\(P^2 = P\\) (idempotente) 8. Matriz ortogonal Una matriz \\(Q\\) es ortogonal si: \\[ Q&#39;Q = QQ&#39; = I \\Rightarrow Q^{-1} = Q&#39; \\] Sus columnas (y filas) son vectores ortonormales. 9. Matrices de proyección: \\(P\\) y \\(M\\) En regresión lineal, dos matrices juegan un rol fundamental: a) Matriz de proyección sobre el espacio columna de \\(X\\): \\[ P = X(X&#39;X)^{-1}X&#39; \\] Idempotente: \\(P^2 = P\\) Simétrica: \\(P&#39; = P\\) Proyecta cualquier vector \\(y\\) sobre el espacio generado por las columnas de \\(X\\): \\(\\hat{y} = P y\\) b) Matriz de aniquilación o proyección ortogonal: \\[ M = I - P \\] Idempotente: \\(M^2 = M\\) Simétrica: \\(M&#39; = M\\) Proyecta sobre el complemento ortogonal del espacio generado por \\(X\\): \\(e = M y\\) (residuos) Estas matrices son centrales para expresar la descomposición: \\[ y = \\hat{y} + e = P y + M y \\] donde \\(\\hat{y}\\) es la parte explicada por \\(X\\), y \\(e\\) es la parte no explicada (residuos). Derivadas de una función multidimensional Derivadas de una forma lineal Sea la forma lineal \\(\\mathbf{a}&#39;\\mathbf{x} = a_1x_1 + a_2x_2 + \\cdots + a_nx_n\\), una función escalar de \\(n\\) variables independientes \\(x_1, \\ldots, x_n\\). La derivada parcial con respecto a una variable \\(x_i\\) es simplemente: \\[ \\frac{\\partial \\mathbf{a}&#39;\\mathbf{x}}{\\partial x_i} = a_i \\] La derivada de \\(\\mathbf{a}&#39;\\mathbf{x}\\) con respecto al vector \\(\\mathbf{x}\\) es: \\[ \\frac{\\partial \\mathbf{a}&#39;\\mathbf{x}}{\\partial \\mathbf{x}} = \\begin{pmatrix} \\frac{\\partial \\mathbf{a}&#39;\\mathbf{x}}{\\partial x_1} \\\\ \\frac{\\partial \\mathbf{a}&#39;\\mathbf{x}}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial \\mathbf{a}&#39;\\mathbf{x}}{\\partial x_n} \\end{pmatrix} = \\begin{pmatrix} a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_n \\end{pmatrix} = \\mathbf{a} \\] De forma análoga, la derivada de \\(\\mathbf{a}&#39;\\mathbf{x}\\) respecto de \\(\\mathbf{x}&#39;\\) es un vector fila: \\[ \\frac{\\partial \\mathbf{a}&#39;\\mathbf{x}}{\\partial \\mathbf{x}&#39;} = \\begin{pmatrix} a_1 &amp; a_2 &amp; \\cdots &amp; a_n \\end{pmatrix} = \\mathbf{a}&#39; \\] Derivadas de una forma cuadrática Sea la forma cuadrática \\(\\mathbf{x}&#39;A\\mathbf{x}\\), donde \\(A\\) es una matriz simétrica. Esta puede escribirse como: \\[ \\mathbf{x}&#39;A\\mathbf{x} = \\sum_{i=1}^{n} a_{ii}x_i^2 + 2\\sum_{i=1}^{n-1}\\sum_{j=i+1}^{n} a_{ij}x_ix_j \\] La derivada de \\(\\mathbf{x}&#39;A\\mathbf{x}\\) con respecto al vector \\(\\mathbf{x}\\) es: \\[ \\frac{\\partial (\\mathbf{x}&#39;A\\mathbf{x})}{\\partial \\mathbf{x}} = 2A\\mathbf{x} \\] Esto es, un vector columna cuya i-ésima componente es: \\[ \\frac{\\partial (\\mathbf{x}&#39;A\\mathbf{x})}{\\partial x_i} = 2(a_{i1}x_1 + a_{i2}x_2 + \\cdots + a_{in}x_n) \\] Derivadas de segundo orden (matriz Hessiana) La derivada segunda de \\(\\mathbf{x}&#39;A\\mathbf{x}\\) con respecto a \\(x_i\\) es: \\[ \\frac{\\partial^2 (\\mathbf{x}&#39;A\\mathbf{x})}{\\partial x_i^2} = 2a_{ii} \\] La derivada mixta con respecto a \\(x_i\\) y \\(x_j\\) es: \\[ \\frac{\\partial^2 (\\mathbf{x}&#39;A\\mathbf{x})}{\\partial x_i \\partial x_j} = 2a_{ij} \\] La matriz de segundas derivadas (Hessiana) es: \\[ \\frac{\\partial^2 (\\mathbf{x}&#39;A\\mathbf{x})}{\\partial \\mathbf{x} \\partial \\mathbf{x}&#39;} = 2A \\] Resumen Derivada de forma lineal: \\(\\frac{\\partial (\\mathbf{a}&#39;\\mathbf{x})}{\\partial \\mathbf{x}} = \\mathbf{a}\\) Derivada de forma cuadrática: \\(\\frac{\\partial (\\mathbf{x}&#39;A\\mathbf{x})}{\\partial \\mathbf{x}} = 2A\\mathbf{x}\\) Matriz Hessiana: \\(\\frac{\\partial^2 (\\mathbf{x}&#39;A\\mathbf{x})}{\\partial \\mathbf{x} \\partial \\mathbf{x}&#39;} = 2A\\) 📘 Preguntas de repaso Sea \\(\\mathbf{i} = (1, 1, \\ldots, 1)&#39;\\) un vector \\(m \\times 1\\) de unos. Calcule \\(\\mathbf{i}&#39;\\mathbf{i}\\). Sean \\(\\mathbf{i} = (1, \\ldots, 1)&#39;\\) y \\(\\mathbf{y} = (y_1, \\ldots, y_m)&#39;\\). Calcule \\(\\mathbf{i}&#39;\\mathbf{y}\\). Demuestre que la media de las observaciones \\(y_1, \\ldots, y_m\\) puede expresarse como: "],["supuestos-del-modelo-clásico-de-regresión-lineal.html", "5 Supuestos del Modelo Clásico de Regresión Lineal Proceso Generador de Datos Tabla Resumen de Supuestos S1. Linealidad en los Parámetros S2. Exogeneidad Estricta S3. Colinealidad Imperfecta S4. Perturbaciones Esféricas S5. Regresores No Estocásticos S6. Normalidad del Error Glosario de Símbolos 📘 Preguntas de repaso", " 5 Supuestos del Modelo Clásico de Regresión Lineal Proceso Generador de Datos El modelo de regresión lineal parte de la siguiente estructura: \\[ Y_i = X_i \\beta + \\epsilon_i \\] Donde: - \\(Y_i\\): variable dependiente (observación i) - \\(X_i\\): vector fila con los regresores de la observación i - \\(\\beta\\): vector de parámetros poblacionales - \\(\\epsilon_i\\): error poblacional (componentes no observables) - \\(i = 1, 2, ..., n\\) Esta formulación describe el proceso generador de datos (PGD), base para los supuestos del MCO. Tabla Resumen de Supuestos Supuesto Notación Implicación principal S1. Linealidad en los parámetros \\(y_i = X_i \\beta + \\epsilon_i\\) El modelo es lineal en los parámetros S2. Exogeneidad estricta \\(\\mathbb{E}[\\epsilon_i \\mid X] = 0\\) No hay correlación entre regresores y error S3. Colinealidad imperfecta \\(\\text{Rango}(X) = K\\) No hay multicolinealidad perfecta; modelo identificable S4. Perturbaciones esféricas \\(\\text{Var}(\\epsilon_i \\mid X) = \\sigma^2\\), \\(\\text{Cov}(\\epsilon_i, \\epsilon_j \\mid X) = 0\\) Homocedasticidad y no autocorrelación S5. Regresores no estocásticos \\(X\\) es fija en repetidas muestras Simplifica demostraciones teóricas S6. Normalidad \\(\\epsilon \\mid X \\sim \\mathcal{N}(0, \\sigma^2 I)\\) Solo necesaria para inferencia exacta S1. Linealidad en los Parámetros El valor esperado de \\(y\\) está relacionado linealmente con los regresores: \\[ \\mathbb{E}[Y_i \\mid X_i] = X_i \\beta \\] Esto permite distintas formas funcionales (lineales en parámetros): Lineal: \\(y_i = \\beta_1 + \\beta_2 x_i + \\epsilon_i\\) Log-log: \\(\\log(y_i) = \\beta_1 + \\beta_2 \\log(x_i) + \\epsilon_i\\) Log-lineal: \\(\\log(y_i) = \\beta_1 + \\beta_2 x_i + \\epsilon_i\\) Lineal-log: \\(y_i = \\beta_1 + \\beta_2 \\log(x_i) + \\epsilon_i\\) Cuadrático: \\(y_i = \\beta_1 + \\beta_2 x_i + \\beta_3 x_i^2 + \\epsilon_i\\) Interactuado: \\(y_i = \\beta_1 + \\beta_2 x_1 + \\beta_3 x_2 + \\beta_4(x_1 x_2) + \\epsilon_i\\) S2. Exogeneidad Estricta \\[ \\mathbb{E}[\\epsilon_i \\mid X] = 0 \\] Esto implica que no existe relación sistemática entre los regresores y el término de error. Ejemplos: \\(\\mathbb{E}[u \\mid X = 1] = 0\\) \\(\\mathbb{E}[u \\mid X_2 = \\text{Mujer}] = 0\\) Demostración (Ley de la esperanza iterada): \\[ \\mathbb{E}[\\epsilon_i] = \\mathbb{E}\\left[ \\mathbb{E}[\\epsilon_i \\mid X] \\right] = \\mathbb{E}[0] = 0 \\] Equivalencia: Si \\(\\mathbb{E}[\\epsilon_i \\mid X] = 0\\), entonces: \\[ \\text{Cov}(\\epsilon_i, X_j) = 0 \\quad \\forall j \\] Pero qué quiere decir? Una forma de pensar en esta definición es: Para cualquier valor de \\(X\\), el valor esperado de los residuos debe ser igual a cero E.g., \\(\\mathop{E}\\left[ u \\mid X=1 \\right]=0\\) and \\(\\mathop{E}\\left[ u \\mid X=100 \\right]=0\\) E.g., \\(\\mathop{E}\\left[ u \\mid X_2=\\text{Mujer} \\right]=0\\) and \\(\\mathop{E}\\left[ u \\mid X_2=\\text{Hombre} \\right]=0\\) Note: \\(\\mathop{E}\\left[ u \\mid X \\right]=0\\) es más restrictivo que \\(\\mathop{E}\\left[ u \\right]=0\\) Graficamente… Exogeneidad Estricta se Incumple, i.e., \\(\\mathop{E}\\left[ \\epsilon \\mid X \\right] \\neq 0\\) S3. Colinealidad Imperfecta \\[ \\text{Rango}(X) = K \\] Para que el modelo esté identificado, debe cumplirse que el número de observaciones sea mayor que el número de regresores: \\(n &gt; K\\). Violaciones comunes: Regresor constante: \\(X_j = c\\) Dos variables idénticas: \\(X_j = X_k\\) Combinación lineal exacta: \\(X_3 = X_1 + X_2\\) Trampa de las variables binarias Ejemplo de matriz con rango 3: \\[ A = \\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 3 &amp; 5 &amp; 7 \\\\ 4 &amp; 6 &amp; 5 \\\\ \\end{bmatrix} \\quad \\Rightarrow \\text{Rango}(A) = 3 \\] Ejemplo de matriz con rango menor a 3: \\[ B = \\begin{bmatrix} 1 &amp; 3 &amp; 1 \\\\ 3 &amp; 8 &amp; 2 \\\\ 2 &amp; 9 &amp; 5 \\\\ \\end{bmatrix} \\quad \\Rightarrow \\text{Rango}(B) \\neq 3 \\] La tercera columna de \\(B\\) es combinación lineal de las otras dos: \\(C_3 = -2 \\cdot C_1 + C_2\\) Wooldridge (2003) aclara que este supuesto permite que los regresores estén correlacionados, siempre que no haya una relación lineal exacta entre ellos. S4. Perturbaciones Esféricas Este supuesto se compone de dos condiciones: 🔹 Homocedasticidad \\[ \\text{Var}(\\epsilon_i \\mid X) = \\sigma^2 \\quad \\forall i \\] La dispersión del término de error es constante para todos los individuos. Esto significa que la varianza de los errores no depende de los regresores. 🔹 No autocorrelación \\[ \\text{Cov}(\\epsilon_i, \\epsilon_j \\mid X) = 0 \\quad \\text{para } i \\neq j \\] Los errores no están correlacionados entre sí. Es especialmente relevante en series de tiempo, pero también puede violarse en datos de corte transversal (e.g., por correlación espacial). 🔸 Implicación conjunta Cuando se cumplen homocedasticidad y no autocorrelación: \\[ \\text{Var}(\\epsilon \\mid X) = \\sigma^2 I \\] La matriz de varianzas-covarianzas de los errores es escalar y diagonal, también llamada matriz esférica. 🧠 Derivación paso a paso {-} \\[ \\text{Var}(\\epsilon \\mid X) = \\mathbb{E}[\\epsilon \\epsilon&#39; \\mid X] - \\mathbb{E}[\\epsilon \\mid X] \\cdot \\mathbb{E}[\\epsilon&#39; \\mid X] \\] Por el supuesto de exogeneidad estricta (S2), sabemos que: \\[ \\mathbb{E}[\\epsilon \\mid X] = 0 \\quad \\Rightarrow \\quad \\text{Var}(\\epsilon \\mid X) = \\mathbb{E}[\\epsilon \\epsilon&#39; \\mid X] \\] Entonces, la matriz resultante es: \\[ \\text{Var}(\\epsilon \\mid X) = \\begin{bmatrix} \\mathbb{E}[\\epsilon_1^2 \\mid X] &amp; \\mathbb{E}[\\epsilon_1 \\epsilon_2 \\mid X] &amp; \\cdots &amp; \\mathbb{E}[\\epsilon_1 \\epsilon_n \\mid X] \\\\ \\mathbb{E}[\\epsilon_2 \\epsilon_1 \\mid X] &amp; \\mathbb{E}[\\epsilon_2^2 \\mid X] &amp; \\cdots &amp; \\mathbb{E}[\\epsilon_2 \\epsilon_n \\mid X] \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\mathbb{E}[\\epsilon_n \\epsilon_1 \\mid X] &amp; \\mathbb{E}[\\epsilon_n \\epsilon_2 \\mid X] &amp; \\cdots &amp; \\mathbb{E}[\\epsilon_n^2 \\mid X] \\end{bmatrix} \\] Aplicando los supuestos: \\(\\text{Var}(\\epsilon_i \\mid X) = \\sigma^2\\) \\(\\text{Cov}(\\epsilon_i, \\epsilon_j \\mid X) = 0\\) para \\(i \\neq j\\) \\[ \\Rightarrow \\text{Var}(\\epsilon \\mid X) = \\begin{bmatrix} \\sigma^2 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\sigma^2 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; \\sigma^2 \\end{bmatrix} = \\sigma^2 I \\] Este supuesto es necesario para garantizar la eficiencia del estimador MCO bajo los supuestos clásicos (Teorema de Gauss-Markov). S5. Regresores No Estocásticos Este supuesto establece que la matriz de regresores \\(X\\) no es aleatoria: sus valores permanecen fijos en repeticiones del experimento o entre muestras. \\[ X = \\text{constante} \\quad \\text{(no varía entre muestras)} \\] 🔹 ¿Qué significa? Aunque en la práctica \\(X\\) proviene de una muestra aleatoria, asumir que es no estocástica permite tratarlo como fijo en la teoría. Esto implica que cualquier inferencia o estimación se condiciona sobre \\(X\\). ✅ Ventajas teóricas Simplifica la demostración de propiedades como insesgamiento y varianza mínima. Permite eliminar la distinción entre: valor esperado condicional: \\(\\mathbb{E}[\\hat{\\beta} \\mid X]\\) y valor esperado incondicional: \\(\\mathbb{E}[\\hat{\\beta}]\\) ⚠️ En la práctica… Este supuesto rara vez se cumple literalmente, ya que \\(X\\) normalmente proviene de una muestra aleatoria. Sin embargo, es común en teoría clásica porque: No afecta la validez del MCO si se asume que \\(X\\) es independiente de \\(\\epsilon\\). Se puede relajar en contextos de modelos más generales (paneles, variables instrumentales, etc.). En modelos con regresores estocásticos, se requiere en cambio que \\(\\mathbb{E}[\\epsilon \\mid X] = 0\\), lo que recupera el supuesto de exogeneidad estricta (S2). S6. Normalidad del Error \\[ \\epsilon \\mid X \\sim \\mathcal{N}(0, \\sigma^2 I) \\] Este supuesto establece que los errores, condicionales a los regresores, siguen una distribución normal multivariada con media cero y matriz de varianza-covarianza esférica \\(\\sigma^2 I\\). 🎯 ¿Para qué sirve? Este supuesto no es necesario para que el estimador de Mínimos Cuadrados Ordinarios (MCO) sea: Insesgado (S2 ya garantiza eso), Eficiente entre estimadores lineales insesgados (por el Teorema de Gauss-Markov). Sin embargo, sí es crucial para que se cumpla la distribución exacta de ciertos estadísticos en muestras pequeñas. ✅ Aplicaciones de la normalidad: Validez de las pruebas t para significancia individual. Validez de las pruebas F para restricciones conjuntas. Construcción exacta de intervalos de confianza para \\(\\beta\\). 🧠 ¿Qué pasa en muestras grandes? Gracias al Teorema Central del Límite y **La Ley de los Grandes Números*, incluso si \\(\\epsilon\\) no es normal, el estimador \\(\\hat{\\beta}\\) tenderá a seguir una distribución normal asintótica: \\[ \\hat{\\beta} \\overset{approx}{\\sim} \\mathcal{N}\\left(\\beta, \\sigma^2 (X&#39;X)^{-1}\\right) \\] Por eso, la normalidad puede relajarse si \\(n\\) es suficientemente grande. Glosario de Símbolos Símbolo Significado \\(Y_i\\) Variable dependiente \\(X_{ij}\\) Regresor j para observación i \\(\\beta_j\\) Parámetro poblacional \\(\\epsilon_i\\) Error poblacional \\(n\\) Número de observaciones \\(k\\) Número de regresores (sin constante) 📘 Preguntas de repaso 📘 1. Conceptuales Defina brevemente los siguientes términos: Econometría teórica Econometría aplicada ¿Qué papel juega cada uno de los seis supuestos del modelo clásico de regresión lineal en garantizar las propiedades del estimador de MCO? 🧮 2. Clasificación de modelos Clasifique los siguientes modelos como lineales en parámetros o no lineales: \\(y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\\) \\(\\log(y_i) = \\beta_0 + \\beta_1 \\log(x_i) + \\epsilon_i\\) \\(y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\epsilon_i\\) \\(y_i = \\frac{\\beta_0}{1 + e^{-\\beta_1 x_i}} + \\epsilon_i\\) \\(y_i = \\alpha + \\theta^{x_i} + \\epsilon_i\\) 📏 3. Interpretación de la pendiente Interprete el coeficiente \\(\\beta_1\\) en los siguientes modelos de regresión lineal simple: \\(y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\\) \\(\\log(y_i) = \\beta_0 + \\beta_1 x_i + \\epsilon_i\\) \\(y_i = \\beta_0 + \\beta_1 \\log(x_i) + \\epsilon_i\\) \\(\\log(y_i) = \\beta_0 + \\beta_1 \\log(x_i) + \\epsilon_i\\) En cada caso, explique qué representa un aumento marginal en \\(x_i\\), y si los efectos son absolutos, porcentuales o elásticos. 🎥 Recursos audiovisuales ¿Qué hacen los economistas? (Video 1) An Uneven Paying Field (Video 2) "]]
