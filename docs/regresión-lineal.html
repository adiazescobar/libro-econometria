<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2 Regresión lineal | Econometría II</title>
  <meta name="description" content="<p>This is a minimal example of using the bookdown package to write a book.
The HTML output format for this example is bookdown::gitbook,
set in the _output.yml file.</p>" />
  <meta name="generator" content="bookdown 0.43 and GitBook 2.6.7" />

  <meta property="og:title" content="2 Regresión lineal | Econometría II" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="<p>This is a minimal example of using the bookdown package to write a book.
The HTML output format for this example is bookdown::gitbook,
set in the _output.yml file.</p>" />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2 Regresión lineal | Econometría II" />
  
  <meta name="twitter:description" content="<p>This is a minimal example of using the bookdown package to write a book.
The HTML output format for this example is bookdown::gitbook,
set in the _output.yml file.</p>" />
  

<meta name="author" content="Ana María Díaz" />


<meta name="date" content="2025-07-16" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="repaso.html"/>
<link rel="next" href="repaso-de-matrices.html"/>
<script src="libs/jquery/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections/anchor-sections.js"></script>
<script src="libs/htmlwidgets/htmlwidgets.js"></script>
<script src="libs/plotly-binding/plotly.js"></script>
<script src="libs/typedarray/typedarray.min.js"></script>
<link href="libs/crosstalk/css/crosstalk.min.css" rel="stylesheet" />
<script src="libs/crosstalk/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main/plotly-latest.min.js"></script>
<script src="libs/rglWebGL-binding/rglWebGL.js"></script>
<link href="libs/rglwidgetClass/rgl.css" rel="stylesheet" />
<script src="libs/rglwidgetClass/rglClass.min.js"></script>
<script src="libs/CanvasMatrix4/CanvasMatrix.min.js"></script>


<style type="text/css">
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">📘 Econometría II</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path=""><a href="#informaci%C3%B3n-general"><i class="fa fa-check"></i>Información general</a>
<ul>
<li class="chapter" data-level="" data-path=""><a href="#descripci%C3%B3n-del-curso"><i class="fa fa-check"></i>Descripción del curso</a></li>
<li class="chapter" data-level="" data-path=""><a href="#material-bibliogr%C3%A1fico"><i class="fa fa-check"></i>Material bibliográfico</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Libro obligatorio</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#libros-recomendados"><i class="fa fa-check"></i>Libros recomendados</a></li>
</ul></li>
<li class="chapter" data-level="" data-path=""><a href="#evaluaci%C3%B3n"><i class="fa fa-check"></i>Evaluación</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#programa-semanal"><i class="fa fa-check"></i>Programa semanal</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#recursos-adicionales"><i class="fa fa-check"></i>Recursos adicionales</a></li>
<li class="chapter" data-level="" data-path=""><a href="#inclusi%C3%B3n"><i class="fa fa-check"></i>Inclusión</a></li>
<li class="chapter" data-level="" data-path=""><a href="#integridad-acad%C3%A9mica"><i class="fa fa-check"></i>Integridad académica</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="repaso.html"><a href="repaso.html"><i class="fa fa-check"></i><b>1</b> Repaso</a>
<ul>
<li class="chapter" data-level="" data-path=""><a href="#qu%C3%A9-estudia-la-econometr%C3%ADa"><i class="fa fa-check"></i>¿Qué estudia la econometría?</a></li>
<li class="chapter" data-level="" data-path="repaso.html"><a href="repaso.html#el-proceso-generador-de-datos"><i class="fa fa-check"></i>El proceso generador de datos</a></li>
<li class="chapter" data-level="" data-path=""><a href="#qu%C3%A9-hacemos-entonces"><i class="fa fa-check"></i>¿Qué hacemos entonces?</a></li>
<li class="chapter" data-level="" data-path=""><a href="#construimos-una-poblaci%C3%B3n-de-juguete"><i class="fa fa-check"></i>Construimos una población de juguete</a>
<ul>
<li class="chapter" data-level="" data-path=""><a href="#la-relaci%C3%B3n-verdadera-en-la-poblaci%C3%B3n"><i class="fa fa-check"></i>La relación verdadera en la población</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="repaso.html"><a href="repaso.html#y-si-mi-muestra-es-mala"><i class="fa fa-check"></i>¿Y si mi muestra es mala?</a>
<ul>
<li class="chapter" data-level="" data-path=""><a href="#c%C3%B3mo-saber-si-me-toc%C3%B3-una-muestra-mala"><i class="fa fa-check"></i>¿Cómo saber si me tocó una muestra mala?</a></li>
<li class="chapter" data-level="" data-path="repaso.html"><a href="repaso.html#se-puede-reducir-la-incertidumbre-muestral"><i class="fa fa-check"></i>¿Se puede reducir la incertidumbre muestral?</a></li>
<li class="chapter" data-level="" data-path="repaso.html"><a href="repaso.html#la-importancia-de-la-fuente-de-los-datos"><i class="fa fa-check"></i>✅ La importancia de la fuente de los datos</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="repaso.html"><a href="repaso.html#y-si-mantengo-fija-la-muestra"><i class="fa fa-check"></i>¿Y si mantengo fija la muestra?</a>
<ul>
<li class="chapter" data-level="" data-path="repaso.html"><a href="repaso.html#recordemos-el-modelo"><i class="fa fa-check"></i>Recordemos el modelo:</a></li>
<li class="chapter" data-level="" data-path=""><a href="#una-aclaraci%C3%B3n-importante-sobre-el-insesgamiento"><i class="fa fa-check"></i>☝️ Una aclaración importante sobre el insesgamiento</a></li>
<li class="chapter" data-level="" data-path=""><a href="#cu%C3%A1ndo-es-cierto-que-nuestras-estimaciones-se-agrupan-alrededor-del-verdadero-beta_1"><i class="fa fa-check"></i>🎯 ¿Cuándo es cierto que nuestras estimaciones “se agrupan” alrededor del verdadero <span class="math inline">\(\beta_1\)</span>?</a></li>
<li class="chapter" data-level="" data-path="repaso.html"><a href="repaso.html#entonces-las-simulaciones-que-hicimos-son-realistas"><i class="fa fa-check"></i>💬 Entonces, ¿las simulaciones que hicimos son “realistas”?</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="repaso.html"><a href="repaso.html#preguntas-de-repaso"><i class="fa fa-check"></i>📘 Preguntas de repaso</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="02-MCOsuma.html"><a href="#regresi%C3%B3n-lineal"><i class="fa fa-check"></i><b>2</b> Regresión lineal</a>
<ul>
<li class="chapter" data-level="" data-path=""><a href="#objetivo-del-cap%C3%ADtulo"><i class="fa fa-check"></i>🎯 Objetivo del capítulo</a></li>
<li class="chapter" data-level="" data-path=""><a href="#qu%C3%A9-significa-encontrar-la-mejor-l%C3%ADnea"><i class="fa fa-check"></i>🔍 ¿Qué significa encontrar la “mejor línea”?</a>
<ul>
<li class="chapter" data-level="" data-path="regresión-lineal.html"><a href="regresión-lineal.html"><i class="fa fa-check"></i>🎨 Ilustremos esto con un ejemplo visual</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="regresión-lineal.html"><a href="regresión-lineal.html#mco"><i class="fa fa-check"></i>MCO</a>
<ul>
<li class="chapter" data-level="" data-path="regresión-lineal.html"><a href="regresión-lineal.html#formalmente"><i class="fa fa-check"></i>Formalmente</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="regresión-lineal.html"><a href="regresión-lineal.html#propiedades-y-supuestos"><i class="fa fa-check"></i>📊 Propiedades y supuestos</a>
<ul>
<li class="chapter" data-level="" data-path=""><a href="#qu%C3%A9-hace-a-un-buen-estimador"><i class="fa fa-check"></i>¿Qué hace a un buen estimador?</a></li>
<li class="chapter" data-level="2.0.1" data-path="regresión-lineal.html"><a href="regresión-lineal.html#repaso-funciones-de-densidad"><i class="fa fa-check"></i><b>2.0.1</b> 📈 Repaso: Funciones de densidad</a></li>
<li class="chapter" data-level="" data-path=""><a href="#qu%C3%A9-propiedades-buscamos-en-un-estimador"><i class="fa fa-check"></i>🤔 ¿Qué propiedades buscamos en un estimador?</a></li>
<li class="chapter" data-level="" data-path="regresión-lineal.html"><a href="regresión-lineal.html#el-trade-off-sesgo-vs.-varianza"><i class="fa fa-check"></i>🎯 El trade-off: sesgo vs. varianza</a></li>
<li class="chapter" data-level="" data-path="regresión-lineal.html"><a href="regresión-lineal.html#propiedad-3-consistencia"><i class="fa fa-check"></i>Propiedad 3: Consistencia</a></li>
<li class="chapter" data-level="" data-path="regresión-lineal.html"><a href="regresión-lineal.html#propiedad-4-eficiencia"><i class="fa fa-check"></i>Propiedad 4: Eficiencia</a></li>
<li class="chapter" data-level="" data-path="regresión-lineal.html"><a href="regresión-lineal.html#resumen-de-las-propiedades"><i class="fa fa-check"></i>Resumen de las propiedades</a></li>
<li class="chapter" data-level="" data-path=""><a href="#nota-de-cierre-c%C3%B3mo-interpretar-cada-propiedad"><i class="fa fa-check"></i>🧠 Nota de cierre: cómo interpretar cada propiedad</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="regresión-lineal.html"><a href="regresión-lineal.html#preguntas-de-repaso-1"><i class="fa fa-check"></i>📘 Preguntas de repaso</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="repaso-de-matrices.html"><a href="repaso-de-matrices.html"><i class="fa fa-check"></i><b>3</b> Repaso de matrices</a>
<ul>
<li class="chapter" data-level="" data-path="repaso-de-matrices.html"><a href="repaso-de-matrices.html#matrices"><i class="fa fa-check"></i>Matrices</a>
<ul>
<li class="chapter" data-level="" data-path="repaso-de-matrices.html"><a href="repaso-de-matrices.html#traspuesta-de-una-matriz"><i class="fa fa-check"></i>Traspuesta de una matriz</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="repaso-de-matrices.html"><a href="repaso-de-matrices.html#vectores"><i class="fa fa-check"></i>Vectores</a>
<ul>
<li class="chapter" data-level="" data-path="repaso-de-matrices.html"><a href="repaso-de-matrices.html#producto-escalar"><i class="fa fa-check"></i>Producto escalar</a></li>
<li class="chapter" data-level="" data-path=""><a href="#norma-y-normalizaci%C3%B3n"><i class="fa fa-check"></i>Norma y normalización</a></li>
<li class="chapter" data-level="" data-path="repaso-de-matrices.html"><a href="repaso-de-matrices.html#ortogonalidad"><i class="fa fa-check"></i>Ortogonalidad</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="repaso-de-matrices.html"><a href="repaso-de-matrices.html#operaciones-con-matrices"><i class="fa fa-check"></i>Operaciones con matrices</a>
<ul>
<li class="chapter" data-level="" data-path="repaso-de-matrices.html"><a href="repaso-de-matrices.html#igualdad-de-matrices"><i class="fa fa-check"></i>Igualdad de matrices</a></li>
<li class="chapter" data-level="" data-path="repaso-de-matrices.html"><a href="repaso-de-matrices.html#suma-y-resta-de-matrices"><i class="fa fa-check"></i>Suma y resta de matrices</a></li>
<li class="chapter" data-level="" data-path=""><a href="#multiplicaci%C3%B3n-por-un-escalar"><i class="fa fa-check"></i>Multiplicación por un escalar</a></li>
<li class="chapter" data-level="" data-path=""><a href="#multiplicaci%C3%B3n-de-matrices"><i class="fa fa-check"></i>Multiplicación de matrices</a></li>
<li class="chapter" data-level="" data-path=""><a href="#transposici%C3%B3n-de-matrices"><i class="fa fa-check"></i>Transposición de matrices</a></li>
<li class="chapter" data-level="" data-path="repaso-de-matrices.html"><a href="repaso-de-matrices.html#traza-de-una-matriz"><i class="fa fa-check"></i>Traza de una matriz</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="repaso-de-matrices.html"><a href="repaso-de-matrices.html#determinantes"><i class="fa fa-check"></i>Determinantes</a></li>
<li class="chapter" data-level="" data-path="repaso-de-matrices.html"><a href="repaso-de-matrices.html#matriz-inversa"><i class="fa fa-check"></i>Matriz inversa</a></li>
<li class="chapter" data-level="" data-path="repaso-de-matrices.html"><a href="repaso-de-matrices.html#rango-de-una-matriz"><i class="fa fa-check"></i>Rango de una matriz</a></li>
<li class="chapter" data-level="" data-path="repaso-de-matrices.html"><a href="repaso-de-matrices.html#sistemas-de-ecuaciones-lineales"><i class="fa fa-check"></i>Sistemas de ecuaciones lineales</a>
<ul>
<li class="chapter" data-level="" data-path="repaso-de-matrices.html"><a href="repaso-de-matrices.html#sistema-de-cramer"><i class="fa fa-check"></i>Sistema de Cramer</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="repaso-de-matrices.html"><a href="repaso-de-matrices.html#matrices-cuadradas-especiales"><i class="fa fa-check"></i>Matrices cuadradas especiales</a>
<ul>
<li class="chapter" data-level="" data-path="repaso-de-matrices.html"><a href="repaso-de-matrices.html#matriz-diagonal"><i class="fa fa-check"></i>1. Matriz diagonal</a></li>
<li class="chapter" data-level="" data-path="repaso-de-matrices.html"><a href="repaso-de-matrices.html#matriz-identidad"><i class="fa fa-check"></i>2. Matriz identidad</a></li>
<li class="chapter" data-level="" data-path="repaso-de-matrices.html"><a href="repaso-de-matrices.html#matriz-escalar"><i class="fa fa-check"></i>3. Matriz escalar</a></li>
<li class="chapter" data-level="" data-path="repaso-de-matrices.html"><a href="repaso-de-matrices.html#matriz-triangular-inferior"><i class="fa fa-check"></i>4. Matriz triangular inferior</a></li>
<li class="chapter" data-level="" data-path="repaso-de-matrices.html"><a href="repaso-de-matrices.html#matriz-nula"><i class="fa fa-check"></i>5. Matriz nula</a></li>
<li class="chapter" data-level="" data-path=""><a href="#matriz-sim%C3%A9trica"><i class="fa fa-check"></i>6. Matriz simétrica</a></li>
<li class="chapter" data-level="" data-path="repaso-de-matrices.html"><a href="repaso-de-matrices.html#matriz-idempotente"><i class="fa fa-check"></i>7. Matriz idempotente</a></li>
<li class="chapter" data-level="" data-path="repaso-de-matrices.html"><a href="repaso-de-matrices.html#matriz-ortogonal"><i class="fa fa-check"></i>8. Matriz ortogonal</a></li>
<li class="chapter" data-level="" data-path=""><a href="#matrices-de-proyecci%C3%B3n-p-y-m"><i class="fa fa-check"></i>9. Matrices de proyección: <span class="math inline">\(P\)</span> y <span class="math inline">\(M\)</span></a></li>
</ul></li>
<li class="chapter" data-level="" data-path=""><a href="#derivadas-de-una-funci%C3%B3n-multidimensional"><i class="fa fa-check"></i>Derivadas de una función multidimensional</a>
<ul>
<li class="chapter" data-level="" data-path="repaso-de-matrices.html"><a href="repaso-de-matrices.html#derivadas-de-una-forma-lineal"><i class="fa fa-check"></i>Derivadas de una forma lineal</a></li>
<li class="chapter" data-level="" data-path=""><a href="#derivadas-de-una-forma-cuadr%C3%A1tica"><i class="fa fa-check"></i>Derivadas de una forma cuadrática</a></li>
<li class="chapter" data-level="" data-path="repaso-de-matrices.html"><a href="repaso-de-matrices.html#derivadas-de-segundo-orden-matriz-hessiana"><i class="fa fa-check"></i>Derivadas de segundo orden (matriz Hessiana)</a></li>
<li class="chapter" data-level="" data-path="repaso-de-matrices.html"><a href="repaso-de-matrices.html#resumen"><i class="fa fa-check"></i>Resumen</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="repaso-de-matrices.html"><a href="repaso-de-matrices.html#preguntas-de-repaso-2"><i class="fa fa-check"></i>📘 Preguntas de repaso</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="supuestos-de-mco.html"><a href="supuestos-de-mco.html"><i class="fa fa-check"></i><b>4</b> Supuestos de MCO</a>
<ul>
<li class="chapter" data-level="" data-path="supuestos-de-mco.html"><a href="supuestos-de-mco.html#proceso-generador-de-datos"><i class="fa fa-check"></i>Proceso Generador de Datos</a></li>
<li class="chapter" data-level="" data-path="supuestos-de-mco.html"><a href="supuestos-de-mco.html#tabla-resumen-de-supuestos"><i class="fa fa-check"></i>Tabla Resumen de Supuestos</a></li>
<li class="chapter" data-level="" data-path=""><a href="#s1.-linealidad-en-los-par%C3%A1metros"><i class="fa fa-check"></i>S1. Linealidad en los Parámetros</a></li>
<li class="chapter" data-level="" data-path="supuestos-de-mco.html"><a href="supuestos-de-mco.html#s2.-exogeneidad-estricta"><i class="fa fa-check"></i>S2. Exogeneidad Estricta</a></li>
<li class="chapter" data-level="" data-path="supuestos-de-mco.html"><a href="supuestos-de-mco.html#s3.-colinealidad-imperfecta"><i class="fa fa-check"></i>S3. Colinealidad Imperfecta</a></li>
<li class="chapter" data-level="" data-path=""><a href="#s4.-perturbaciones-esf%C3%A9ricas"><i class="fa fa-check"></i>S4. Perturbaciones Esféricas</a></li>
<li class="chapter" data-level="" data-path=""><a href="#s5.-regresores-no-estoc%C3%A1sticos"><i class="fa fa-check"></i>S5. Regresores No Estocásticos</a></li>
<li class="chapter" data-level="" data-path="supuestos-de-mco.html"><a href="supuestos-de-mco.html#s6.-normalidad-del-error"><i class="fa fa-check"></i>S6. Normalidad del Error</a></li>
<li class="chapter" data-level="" data-path=""><a href="#glosario-de-s%C3%ADmbolos"><i class="fa fa-check"></i>Glosario de Símbolos</a></li>
<li class="chapter" data-level="" data-path="supuestos-de-mco.html"><a href="supuestos-de-mco.html#preguntas-de-repaso-3"><i class="fa fa-check"></i>📘 Preguntas de repaso</a>
<ul>
<li class="chapter" data-level="" data-path="supuestos-de-mco.html"><a href="supuestos-de-mco.html#recursos-audiovisuales"><i class="fa fa-check"></i>🎥 Recursos audiovisuales</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="02-MCOsuma.html"><a href="#regresi%C3%B3n-por-m%C3%ADnimos-cuadrados-ordinarios-mco"><i class="fa fa-check"></i><b>5</b> Regresión por Mínimos Cuadrados Ordinarios (MCO)</a>
<ul>
<li class="chapter" data-level="" data-path="regresión-por-mínimos-cuadrados-ordinarios-mco.html"><a href="regresión-por-mínimos-cuadrados-ordinarios-mco.html"><i class="fa fa-check"></i>Modelo</a></li>
<li class="chapter" data-level="" data-path=""><a href="#qu%C3%A9-hace-mco"><i class="fa fa-check"></i>¿Qué hace MCO?</a></li>
<li class="chapter" data-level="" data-path=""><a href="#c%C3%B3mo-se-encuentra-el-vector-hatbeta"><i class="fa fa-check"></i>¿Cómo se encuentra el vector <span class="math inline">\(\hat{\beta}\)</span>?</a></li>
<li class="chapter" data-level="" data-path=""><a href="#es-un-m%C3%ADnimo"><i class="fa fa-check"></i>¿Es un mínimo?</a></li>
<li class="chapter" data-level="" data-path=""><a href="#interpretaci%C3%B3n-en-t%C3%A9rminos-de-contraparte-muestral"><i class="fa fa-check"></i>Interpretación en términos de contraparte muestral</a></li>
<li class="chapter" data-level="" data-path=""><a href="#supuestos-clave-empleados-hasta-ac%C3%A1"><i class="fa fa-check"></i>Supuestos clave empleados hasta acá</a></li>
<li class="chapter" data-level="" data-path=""><a href="#diferencia-entre-la-regresi%C3%B3n-simple-y-la-regresi%C3%B3n-m%C3%BAltiple"><i class="fa fa-check"></i>Diferencia entre la regresión simple y la regresión múltiple</a>
<ul>
<li class="chapter" data-level="" data-path=""><a href="#regresi%C3%B3n-sin-variables-explicativas"><i class="fa fa-check"></i>Regresión sin variables explicativas</a></li>
<li class="chapter" data-level="" data-path=""><a href="#regresi%C3%B3n-simple-con-una-variable-explicativa"><i class="fa fa-check"></i>Regresión simple con una variable explicativa</a></li>
<li class="chapter" data-level="" data-path="regresión-por-mínimos-cuadrados-ordinarios-mco.html"><a href="regresión-por-mínimos-cuadrados-ordinarios-mco.html#pausa"><i class="fa fa-check"></i>📝 Pausa</a></li>
<li class="chapter" data-level="" data-path=""><a href="#regresi%C3%B3n-m%C3%BAltiple-con-m%C3%BAltiples-variables-explicativas"><i class="fa fa-check"></i>Regresión múltiple con múltiples variables explicativas</a></li>
</ul></li>
<li class="chapter" data-level="" data-path=""><a href="#ap%C3%A9ndice"><i class="fa fa-check"></i>Apéndice</a>
<ul>
<li class="chapter" data-level="" data-path=""><a href="#%C3%A1lgebra-mucha-%C3%A1lgebra"><i class="fa fa-check"></i>Álgebra… mucha álgebra</a></li>
<li class="chapter" data-level="5.0.1" data-path="02-MCOsuma.html"><a href="#c%C3%A1lculo-de-xy"><i class="fa fa-check"></i><b>5.0.1</b> Cálculo de <span class="math inline">\(X&#39;y\)</span></a></li>
</ul></li>
<li class="chapter" data-level="" data-path=""><a href="#regresi%C3%B3n-m%C3%BAltiple"><i class="fa fa-check"></i>Regresión múltiple</a></li>
<li class="chapter" data-level="" data-path="regresión-por-mínimos-cuadrados-ordinarios-mco.html"><a href="regresión-por-mínimos-cuadrados-ordinarios-mco.html#estimador-de-mco-en-r-empleando-matrices"><i class="fa fa-check"></i>Estimador de MCO en R empleando matrices</a></li>
<li class="chapter" data-level="" data-path="regresión-por-mínimos-cuadrados-ordinarios-mco.html"><a href="regresión-por-mínimos-cuadrados-ordinarios-mco.html#estimador-de-mco-en-stata-usando-mata"><i class="fa fa-check"></i>Estimador de MCO en Stata usando MATA</a></li>
<li class="chapter" data-level="" data-path="regresión-por-mínimos-cuadrados-ordinarios-mco.html"><a href="regresión-por-mínimos-cuadrados-ordinarios-mco.html#estimador-de-mco-en-python-usando-numpy"><i class="fa fa-check"></i>Estimador de MCO en Python usando NumPy</a>
<ul>
<li class="chapter" data-level="" data-path="regresión-por-mínimos-cuadrados-ordinarios-mco.html"><a href="regresión-por-mínimos-cuadrados-ordinarios-mco.html#preguntas-de-repaso-4"><i class="fa fa-check"></i>📘 Preguntas de repaso</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="02-MCOsuma.html"><a href="#anatom%C3%ADa-de-la-regresi%C3%B3n-m%C3%BAltiple"><i class="fa fa-check"></i><b>6</b> Anatomía de la Regresión Múltiple</a>
<ul>
<li class="chapter" data-level="" data-path=""><a href="#matrices-de-proyecci%C3%B3n"><i class="fa fa-check"></i>Matrices de Proyección</a>
<ul>
<li class="chapter" data-level="" data-path=""><a href="#intuici%C3%B3n-geom%C3%A9trica"><i class="fa fa-check"></i>🔍 Intuición geométrica</a></li>
<li class="chapter" data-level="" data-path="anatomía-de-la-regresión-múltiple.html"><a href="anatomía-de-la-regresión-múltiple.html"><i class="fa fa-check"></i>📐 Propiedades algebraicas clave</a></li>
<li class="chapter" data-level="" data-path=""><a href="#visualizaci%C3%B3n-tridimensional-de-la-proyecci%C3%B3n"><i class="fa fa-check"></i>✨ Visualización tridimensional de la proyección</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="anatomía-de-la-regresión-múltiple.html"><a href="anatomía-de-la-regresión-múltiple.html#teorema-de-frisch-waugh-lovell-fwl"><i class="fa fa-check"></i>Teorema de Frisch-Waugh-Lovell (FWL)</a></li>
<li class="chapter" data-level="" data-path=""><a href="#notaci%C3%B3n-y-motivaci%C3%B3n"><i class="fa fa-check"></i>🎯 Notación y Motivación</a></li>
<li class="chapter" data-level="" data-path=""><a href="#cu%C3%A1l-es-el-problema-que-resuelve-el-fwl"><i class="fa fa-check"></i>❓¿Cuál es el problema que resuelve el FWL?</a></li>
<li class="chapter" data-level="" data-path="anatomía-de-la-regresión-múltiple.html"><a href="anatomía-de-la-regresión-múltiple.html#paso-a-paso-del-teorema-de-frisch-waugh-lovell-fwl"><i class="fa fa-check"></i>✨ Paso a paso del Teorema de Frisch-Waugh-Lovell (FWL)</a></li>
<li class="chapter" data-level="" data-path="anatomía-de-la-regresión-múltiple.html"><a href="anatomía-de-la-regresión-múltiple.html#paso-1-proyectar-y-sobre-x_s-y-obtener-los-residuos"><i class="fa fa-check"></i>🧩 Paso 1: Proyectar <span class="math inline">\(y\)</span> sobre <span class="math inline">\(X_s\)</span> y obtener los residuos</a></li>
<li class="chapter" data-level="" data-path="anatomía-de-la-regresión-múltiple.html"><a href="anatomía-de-la-regresión-múltiple.html#paso-2-proyectar-x_r-sobre-x_s-y-obtener-los-residuos"><i class="fa fa-check"></i>🧩 Paso 2: Proyectar <span class="math inline">\(X_r\)</span> sobre <span class="math inline">\(X_s\)</span> y obtener los residuos</a></li>
<li class="chapter" data-level="" data-path="anatomía-de-la-regresión-múltiple.html"><a href="anatomía-de-la-regresión-múltiple.html#paso-3-regresar-tildey-sobre-tildex_r"><i class="fa fa-check"></i>🧩 Paso 3: Regresar <span class="math inline">\(\tilde{y}\)</span> sobre <span class="math inline">\(\tilde{X}_r\)</span></a></li>
<li class="chapter" data-level="" data-path=""><a href="#interpretaci%C3%B3n-final"><i class="fa fa-check"></i>✅ Interpretación final</a></li>
<li class="chapter" data-level="" data-path=""><a href="#conclusi%C3%B3n"><i class="fa fa-check"></i>📦 Conclusión</a></li>
<li class="chapter" data-level="" data-path=""><a href="#demostraci%C3%B3n-formal"><i class="fa fa-check"></i>Demostración Formal</a></li>
<li class="chapter" data-level="" data-path=""><a href="#ejemplo-pr%C3%A1ctico-del-teorema-de-frisch-waugh-lovell-en-stata-r-y-python"><i class="fa fa-check"></i>🧪 Ejemplo práctico del Teorema de Frisch-Waugh-Lovell en Stata, R y Python</a>
<ul>
<li class="chapter" data-level="6.0.1" data-path="02-MCOsuma.html"><a href="#c%C3%B3digo-en-stata"><i class="fa fa-check"></i><b>6.0.1</b> 🔵 Código en Stata</a></li>
<li class="chapter" data-level="6.0.2" data-path="02-MCOsuma.html"><a href="#c%C3%B3digo-en-r"><i class="fa fa-check"></i><b>6.0.2</b> 🟢 Código en R</a></li>
<li class="chapter" data-level="6.0.3" data-path="02-MCOsuma.html"><a href="#c%C3%B3digo-en-python"><i class="fa fa-check"></i><b>6.0.3</b> 🔴 Código en Python</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/adiazescobar/libro-econometria" target="_blank">📖 Ver en GitHub</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Econometría II</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="regresión-lineal" class="section level1 hasAnchor" number="2">
<h1><span class="header-section-number">2</span> Regresión lineal<a href="#regresi%C3%B3n-lineal" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="objetivo-del-capítulo" class="section level2 unnumbered hasAnchor">
<h2>🎯 Objetivo del capítulo<a href="#objetivo-del-cap%C3%ADtulo" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>En este capitulo vamos a:
1. Entender qué es una regresión lineal y cómo se ve gráficamente.
2. Aprender cómo se calcula la mejor línia con mínimos cuadrados ordinarios (MCO)
3. Explorar qué hace un buen estiamdor y cómo evaluarlo</p>
</div>
<div id="qué-significa-encontrar-la-mejor-línea" class="section level2 unnumbered hasAnchor">
<h2>🔍 ¿Qué significa encontrar la “mejor línea”?<a href="#qu%C3%A9-significa-encontrar-la-mejor-l%C3%ADnea" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Antes de hablar de estimaciones, pensemos en cómo se generan los datos:</p>
<blockquote>
<p>Supondremos que hay un <strong>modelo poblacional</strong> o proceso generador de datos:</p>
</blockquote>
<p><span class="math display">\[ y_i = \beta_0 + \beta_1 x_i + \epsilon_i \]</span></p>
<ul>
<li><span class="math inline">\(y_i\)</span>: variable dependiente (lo que queremos explicar)</li>
<li><span class="math inline">\(x_i\)</span>: variable independiente</li>
<li><span class="math inline">\(\beta_0, \beta_1\)</span>: parámetros poblacionales</li>
<li><span class="math inline">\(\epsilon_i\)</span>: <strong>término de error</strong>: todo lo que afecta a <span class="math inline">\(y_i\)</span> y no está en <span class="math inline">\(x_i\)</span></li>
</ul>
<p>El término <span class="math inline">\(\epsilon_i\)</span> captura factores no observados, errores de medición, y variación aleatoria. Es fundamental porque incluso si tuviéramos los valores verdaderos de <span class="math inline">\(\beta_0\)</span> y <span class="math inline">\(\beta_1\)</span>, seguiríamos sin poder predecir perfectamente <span class="math inline">\(y_i\)</span> debido a este componente.</p>
<p>En la práctica, estimamos los parámetros a partir de una muestra. Esto nos da una versión estimada del modelo:</p>
<p><span class="math display">\[ \hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_i \]</span></p>
<p>Y calculamos los <strong>residuos</strong> (errores estimados):</p>
<p><span class="math display">\[ \hat{\epsilon}_i = y_i - \hat{y}_i \]</span></p>
<p>Queremos encontrar la línea que prediga <span class="math inline">\(y_i\)</span> con la menor cantidad posible de errores. Eso significa minimizar:</p>
<p><span class="math display">\[ \text{SRC} = \sum_{i = 1}^{n} \hat{\epsilon}_i^2 \]</span></p>
<p>Esto se conoce como el <strong>criterio de mínimos cuadrados</strong>.</p>
<div id="ilustremos-esto-con-un-ejemplo-visual" class="section level3 unnumbered hasAnchor">
<h3>🎨 Ilustremos esto con un ejemplo visual<a href="regresión-lineal.html#ilustremos-esto-con-un-ejemplo-visual" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Creemos unos nuevos datos para ilustrar esto.</p>
<p><img src="_main_files/figure-html/ols%20vs%20lines%201-1.svg" width="75%" style="display: block; margin: auto;" /></p>
<p>La linea de regresión es igual a <span class="math inline">\(\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x\)</span> donde _0$ y <span class="math inline">\(\hat{\beta}_1\)</span> son los parámetros estimados de la regresión. En este caso, <span class="math inline">\(\hat{\beta}_0 = 6\)</span> y <span class="math inline">\(\hat{\beta}_1 = 0.2\)</span>. Para cada una de las observaciones podemos encontrar el y estimado <span class="math inline">\(\hat{y}_i\)</span>. En la siguiente figura, la línea naranja representa la línea de regresión estimada.</p>
<p><img src="_main_files/figure-html/vs%20lines%202-1.svg" width="75%" style="display: block; margin: auto;" /></p>
<p>Para cada una de las observaciones podemos calcular los errores: <span class="math inline">\(\epsilon_i = y_i - \hat{y}_i\)</span>, como se observa en el siguiente gráfico.</p>
<p><img src="_main_files/figure-html/ols%20vs%20lines%203-1.svg" width="75%" style="display: block; margin: auto;" /></p>
<p>Ahora podemos probar con otras lineas y ver cómo se comportan los errores. En el siguiente grafico, la línea de regresión estimada es <span class="math inline">\(\hat{y} = 3 + 0.2 x\)</span>. Es evidente que los errores estiamdos son más grandes que los errores estimados en el gráfico anterior.</p>
<p><img src="_main_files/figure-html/ols%20vs%20lines%204-1.svg" width="75%" style="display: block; margin: auto;" /></p>
<p>Probemos ahora con una línea de regresión estimada que no se ajusta a los datos, <span class="math inline">\(\hat{y} = 10 - 0.8 x\)</span>. En este caso, los errores son aún más grandes.</p>
<p><img src="_main_files/figure-html/ols%20vs%20lines%205-1.svg" width="75%" style="display: block; margin: auto;" /></p>
<p>Recuerda que SRC es igual a: <span class="math inline">\(\left(\sum e_i^2\right)\)</span>: Errores más grandes reciben penalizaciones más grandes.</p>
<p><img src="_main_files/figure-html/ols%20vs%20lines%206-1.svg" width="75%" style="display: block; margin: auto;" /></p>
<p>La estimación de MCO es la combinación de <span class="math inline">\(\hat{\beta}_0\)</span> y <span class="math inline">\(\hat{\beta}_1\)</span> que minimiza la SRC</p>
<p><img src="_main_files/figure-html/ols%20vs%20lines%207-1.svg" width="75%" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="mco" class="section level2 unnumbered hasAnchor">
<h2>MCO<a href="regresión-lineal.html#mco" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="formalmente" class="section level3 unnumbered hasAnchor">
<h3>Formalmente<a href="regresión-lineal.html#formalmente" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>En una regresión lineal simple, el estimador de MCO proviene de escoger <span class="math inline">\(\hat{\beta}_0\)</span> y <span class="math inline">\(\hat{\beta}_1\)</span> que minimice la suma de residuos al cuadrado (SRC), <em>i.e.</em>,</p>
<p><span class="math display">\[ \min_{\hat{\beta}_0,\, \hat{\beta}_1} \text{SRC} \]</span></p>
<p>donde
<span class="math display">\[ \text{SRC} = \sum_{i = 1}^{n} (y_i - \hat{y}_i)^2 = \sum_{i = 1}^{n} (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)^2 \]</span>
El estimador de MCO es el valor de <span class="math inline">\(\hat{\beta}_0\)</span> y <span class="math inline">\(\hat{\beta}_1\)</span> que minimiza la SRC.</p>
<p>pero nosotros sabemos que <span class="math inline">\(\text{SRC} = \sum_i \tilde{\epsilon_i}^2\)</span>. Now use the definitions of <span class="math inline">\(\tilde{\epsilon_i}\)</span> and <span class="math inline">\(\hat{y}\)</span>.</p>
<p><span class="math display">\[
\begin{aligned}
  \tilde{\epsilon_i}^2 &amp;= \left( y_i - \hat{y}_i \right)^2 = \left( y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i \right)^2 \\
  &amp;= y_i^2 - 2 y_i \hat{\beta}_0 - 2 y_i \hat{\beta}_1 x_i + \hat{\beta}_0^2 + 2 \hat{\beta}_0 \hat{\beta}_1 x_i + \hat{\beta}_1^2 x_i^2
\end{aligned}
\]</span></p>
<p><strong>Recordatorio:</strong> Minimizar una función multivariada requiere (<strong>1</strong>) que las primeras derivadas sean iguales a cero (las <em>condiciones de primer orden</em>) y (<strong>2</strong>) las condiciones de segundo orden (concavidad).</p>
<p>Nos estamos acercando. Necesitamos <strong>minimizar la SRC</strong>.</p>
<p><span class="math display">\[ \text{SRE} = \sum_i \tilde{e_i}^2 = \sum_i \left( y_i^2 - 2 y_i \hat{\beta}_0 - 2 y_i \hat{\beta}_1 x_i + \hat{\beta}_0^2 + 2 \hat{\beta}_0 \hat{\beta}_1 x_i + \hat{\beta}_1^2 x_i^2 \right) \]</span></p>
<p>For the first-order conditions of minimization, we now take the first derivates of SSE with respect to <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span>.</p>
<p><span class="math display">\[
\begin{aligned}
  \dfrac{\partial \text{SRC}}{\partial \hat{\beta}_0} &amp;= \sum_i \left( 2 \hat{\beta}_0 + 2 \hat{\beta}_1 x_i - 2 y_i \right) = 2n \hat{\beta}_0 + 2 \hat{\beta}_1 \sum_i x_i - 2 \sum_i y_i \\
  &amp;= 2n \hat{\beta}_0 + 2n \hat{\beta}_1 \overline{x} - 2n \overline{y}
\end{aligned}
\]</span></p>
<p>donde <span class="math inline">\(\overline{x} = \frac{\sum x_i}{n}\)</span> y <span class="math inline">\(\overline{y} = \frac{\sum y_i}{n}\)</span> son medias muestrales de <span class="math inline">\(x\)</span> y <span class="math inline">\(y\)</span> (de tamaño <span class="math inline">\(n\)</span>).</p>
<p>Las condiciones de primer orden establecen que las derivadas deben ser iguales a cero:
<span class="math display">\[ \dfrac{\partial \text{SSE}}{\partial \hat{\beta}_0} = 2n \hat{\beta}_0 + 2n \hat{\beta}_1 \overline{x} - 2n \overline{y} = 0 \]</span></p>
<p>Lo que implica</p>
<p><span class="math display">\[ \hat{\beta}_0 = \overline{y} - \hat{\beta}_1 \overline{x} \]</span>
Ahora para <span class="math inline">\(\hat{\beta}_1\)</span>.</p>
<p>Tomemos la derivada de la SRC con respecto a <span class="math inline">\(\hat{\beta}_1\)</span></p>
<p><span class="math display">\[
\begin{aligned}
  \dfrac{\partial \text{SSE}}{\partial \hat{\beta}_1} &amp;= \sum_i \left( 2 \hat{\beta}_0 x_i + 2 \hat{\beta}_1 x_i^2 - 2 y_i x_i \right) = 2 \hat{\beta}_0 \sum_i x_i + 2 \hat{\beta}_1 \sum_i x_i^2 - 2 \sum_i y_i x_i \\
  &amp;= 2n \hat{\beta}_0 \overline{x} + 2 \hat{\beta}_1 \sum_i x_i^2 - 2 \sum_i y_i x_i
\end{aligned}
\]</span>
Igualarlo a cero</p>
<p><span class="math display">\[ \dfrac{\partial \text{SSE}}{\partial \hat{\beta}_1} = 2n \hat{\beta}_0 \overline{x} + 2 \hat{\beta}_1 \sum_i x_i^2 - 2 \sum_i y_i x_i = 0 \]</span>
y reemplazarlo <span class="math inline">\(\hat{\beta}_0\)</span>, <em>i.e.</em>, <span class="math inline">\(\hat{\beta}_0 = \overline{y} - \hat{\beta}_1 \overline{x}\)</span>. Thus,</p>
<p><span class="math display">\[
2n \left(\overline{y} - \hat{\beta}_1 \overline{x}\right) \overline{x} + 2 \hat{\beta}_1 \sum_i x_i^2 - 2 \sum_i y_i x_i = 0
\]</span>
Continuando</p>
<p><span class="math display">\[ 2n \left(\overline{y} - \hat{\beta}_1 \overline{x}\right) \overline{x} + 2 \hat{\beta}_1 \sum_i x_i^2 - 2 \sum_i y_i x_i = 0 \]</span>
<span class="math display">\[ 2n \overline{y}\,\overline{x} - 2n \hat{\beta}_1 \overline{x}^2 + 2 \hat{\beta}_1 \sum_i x_i^2 - 2 \sum_i y_i x_i = 0 \]</span>
<span class="math display">\[ \implies 2 \hat{\beta}_1 \left( \sum_i x_i^2 - n \overline{x}^2 \right) = 2 \sum_i y_i x_i - 2n \overline{y}\,\overline{x} \]</span></p>
<p><span class="math display">\[ \implies \hat{\beta}_1 = \dfrac{\sum_i y_i x_i - 2n \overline{y}\,\overline{x}}{\sum_i x_i^2 - n \overline{x}^2} = \dfrac{\sum_i (x_i - \overline{x})(y_i - \overline{y})}{\sum_i (x_i - \overline{x})^2} \]</span>
LISTOO!</p>
<p>Ahora tenemos nuestros lindos estimadores</p>
<p><span class="math display">\[ \hat{\beta}_1 = \dfrac{\sum_i (x_i - \overline{x})(y_i - \overline{y})}{\sum_i (x_i - \overline{x})^2} \]</span>
and the intercept</p>
<p><span class="math display">\[ \hat{\beta}_0 = \overline{y} - \hat{\beta}_1 \overline{x} \]</span></p>
<p>Ya sabes de dónde proviene la parte de <em>mínimos cuadrados</em> en el término “mínimos cuadrados ordinarios”. 🎊</p>
<p>Ahora pasamos a las propiedades (implícitas) de los Mínimos Cuadrados Ordinarios (MCO / OLS).</p>
</div>
</div>
<div id="propiedades-y-supuestos" class="section level2 unnumbered hasAnchor">
<h2>📊 Propiedades y supuestos<a href="regresión-lineal.html#propiedades-y-supuestos" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="qué-hace-a-un-buen-estimador" class="section level3 unnumbered hasAnchor">
<h3>¿Qué hace a un buen estimador?<a href="#qu%C3%A9-hace-a-un-buen-estimador" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Antes de hablar de propiedades del estimador de MCO, recordemos algunas herramientas fundamentales de estadística.</p>
</div>
<div id="repaso-funciones-de-densidad" class="section level3 hasAnchor" number="2.0.1">
<h3><span class="header-section-number">2.0.1</span> 📈 Repaso: Funciones de densidad<a href="regresión-lineal.html#repaso-funciones-de-densidad" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Las <strong>funciones de densidad de probabilidad</strong> (FDP, o PDF en inglés) describen la probabilidad de que una <strong>variable aleatoria continua</strong> tome valores dentro de un intervalo dado. La probabilidad total bajo la curva es 1.</p>
<p>Ejemplo: para una variable normal estándar, la probabilidad de que tome un valor entre -2 y 0 es:</p>
<p><span class="math display">\[ \mathop{\text{P}}\left(-2 \leq X \leq 0\right) = 0.48 \]</span></p>
<p><img src="_main_files/figure-html/example:%20pdf-1.svg" width="75%" style="display: block; margin: auto;" /></p>
<p>Otro ejemplo clásico es la probabilidad de que una variable aleatoria normal estándar tome un valor entre -1.96 y 1.96: <span class="math inline">\(\mathop{\text{P}}\left(-1.96 \leq X \leq 1.96\right) = 0.95\)</span></p>
<p><img src="_main_files/figure-html/example:%20pdf%202-1.svg" width="75%" style="display: block; margin: auto;" />
O la probabilidad de que una variable aleatoria normal estándar tome un valor mayor a 2: <span class="math inline">\(\mathop{\text{P}}\left(X &gt; 2\right) = 0.023\)</span></p>
<p><img src="_main_files/figure-html/example:%20pdf%203-1.svg" width="75%" style="display: block; margin: auto;" /></p>
</div>
<div id="qué-propiedades-buscamos-en-un-estimador" class="section level3 unnumbered hasAnchor">
<h3>🤔 ¿Qué propiedades buscamos en un estimador?<a href="#qu%C3%A9-propiedades-buscamos-en-un-estimador" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Imaginemos que intentamos estimar un parámetro verdadero <span class="math inline">\(\beta\)</span>, y tenemos tres métodos distintos. Cada uno produce una distribución diferente para <span class="math inline">\(\hat{\beta}\)</span>.</p>
<p><img src="_main_files/figure-html/competing_pdfs-1.png" width="75%" style="display: block; margin: auto;" /></p>
<p><strong>Pregunta:</strong> ¿Qué propiedades podrían ser importantes para un estimador?</p>
<p><strong>Propiedad 1. Insesgamiento</strong><br />
Es decir, si repitiéramos el experimento muchas veces, ¿el estimador tiende a acercarse al valor verdadero del parámetro que estamos tratando de estimar?</p>
<p>El sesgo mide si el estimador se acerca al valor real en promedio:</p>
<blockquote>
<p>🧪 <strong>¿Qué significa “repetir el experimento”?</strong><br />
En este contexto, <em>repetir el experimento</em> puede entenderse de tres formas, todas válidas para pensar en la incertidumbre de un estimador:</p>
<ol style="list-style-type: decimal">
<li><strong>Cambiar la muestra</strong>: imaginar que tomamos muchas muestras aleatorias distintas de la población.<br />
</li>
<li><strong>Mantener fija la muestra, pero cambiar los errores</strong>: incluso si los valores de <span class="math inline">\(x_i\)</span> no cambian, los valores de <span class="math inline">\(y_i\)</span> pueden variar si asumimos que los errores <span class="math inline">\(\epsilon_i\)</span> son aleatorios. Recuerda que <span class="math inline">\(y_i\)</span> sigue un proceso generador de datos subyacente.<br />
</li>
<li><strong>Cambiar ambos simultáneamente</strong>: es el caso más común en simulaciones — se sortean tanto los <span class="math inline">\(x_i\)</span> como los <span class="math inline">\(\epsilon_i\)</span>.</li>
</ol>
<p>En cualquiera de los tres escenarios, obtendríamos distintos valores de <span class="math inline">\(\hat{\beta}\)</span>. Eso nos permite construir una <strong>distribución muestral</strong> del estimador y analizar propiedades como el sesgo.</p>
<p>⚠️ <strong>Importante:</strong> cuando hablamos de “repetir el experimento”, no queremos decir que volvamos a observar a las <em>mismas</em> personas varias veces con diferentes valores de <span class="math inline">\(x\)</span> (por ejemplo, dándoles distintos años de educación).<br />
Lo que estamos haciendo es imaginar escenarios hipotéticos en los que la muestra o los errores cambian, y ver cómo eso afecta al estimador.<br />
Estos experimentos no se pueden realizar en la realidad con una misma persona, pero sí los podemos simular por computadora o analizar teóricamente.</p>
</blockquote>
<p><strong>Más formalmente:</strong> ¿La media de la distribución del estimador es igual al parámetro que estima?</p>
<p>En promedio (después de <em>muchas</em> repeticiones), ¿el estimador tiende hacia el valor correcto?
<strong>Más formalmente:</strong> ¿La media de la distribución del estimador es igual al parámetro que estima?
<span class="math display">\[ \mathop{\text{Sesgo}}_\beta \left( \hat{\beta} \right) = \mathop{\boldsymbol{E}}\left[ \hat{\beta} \right] - \beta \]</span>
<strong>Estimador Insesagado:</strong> <span class="math inline">\(\mathop{\boldsymbol{E}}\left[ \hat{\beta} \right] = \beta\)</span></p>
<p><img src="_main_files/figure-html/unbiased_pdf-1.svg" width="75%" style="display: block; margin: auto;" /></p>
<p><strong>Estimador Sesagado:</strong> <span class="math inline">\(\mathop{\boldsymbol{E}}\left[ \hat{\beta} \right] \neq \beta\)</span></p>
<p><img src="_main_files/figure-html/biased%20pdf-1.svg" width="75%" style="display: block; margin: auto;" /></p>
<p><strong>Propiedad 2: Varianza</strong></p>
<p>También queremos que nuestras estimaciones <strong>no varíen demasiado de una muestra a otra</strong>. En otras palabras: <strong>queremos un estimador que sea estable</strong>, no que en cada muestra nos dé un valor completamente distinto.</p>
<p>La <strong>varianza</strong> mide cuánta variación hay en las estimaciones <span class="math inline">\(\hat{\beta}\)</span> que obtenemos al repetir el experimento (cambiando la muestra, los errores, o ambos):</p>
<p><span class="math display">\[
\text{Var} \left( \hat{\beta} \right) = \mathbb{E} \left[ \left( \hat{\beta} - \mathbb{E}[\hat{\beta}] \right)^2 \right]
\]</span></p>
<p>Un estimador con <strong>menor varianza</strong> produce resultados más consistentes entre muestras. Esto lo hace más confiable, incluso si no es perfecto.</p>
<blockquote>
<p>🎯 <em>Queremos que nuestras estimaciones estén “concentradas” cerca del valor esperado, no dispersas como tiros al aire.</em></p>
</blockquote>
<p>Veamos un ejemplo visual de cómo la varianza afecta a las distribuciones de los estimadores.</p>
<p><img src="_main_files/figure-html/variance%20pdf-1.svg" width="75%" style="display: block; margin: auto;" /></p>
<p>La curva rosada representa un estimador con baja varianza: la mayoría de los valores de <span class="math inline">\(\hat{\beta}\)</span> están cerca de <span class="math inline">\(\beta\)</span>. Mientras que la curva gris oscuro representa un estimador con alta varianza: sus valores están más dispersos. A igualdad de sesgo, preferimos el estimador que tenga menor varianza.</p>
</div>
<div id="el-trade-off-sesgo-vs.-varianza" class="section level3 unnumbered hasAnchor">
<h3>🎯 El trade-off: sesgo vs. varianza<a href="regresión-lineal.html#el-trade-off-sesgo-vs.-varianza" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Hasta ahora hablamos del sesgo y de la varianza por separado. Pero muchas veces, mejorar uno implica empeorar el otro. Esto se conoce como el ** trade-off entre sesgo y varianza**.</p>
<blockquote>
<p>¿Deberíamos aceptar un poco de sesgo si eso nos permite reducir mucho la varianza?</p>
</blockquote>
<p>En econometría, solemos preferir estimadores <strong>insesgados</strong> (o al menos <strong>consistentes</strong>), porque valoramos la interpretación causal y teórica de los parámetros. Pero en otras disciplinas, como el aprendizaje automático o la predicción estadística, es común aceptar un pequeño sesgo si con ello se logra una gran reducción en la varianza y, en consecuencia, una mejor predicción promedio.</p>
<p>veámos esta idea:</p>
<p><img src="_main_files/figure-html/variance%20bias-1.svg" width="75%" style="display: block; margin: auto;" /></p>
</div>
<div id="propiedad-3-consistencia" class="section level3 unnumbered hasAnchor">
<h3>Propiedad 3: Consistencia<a href="regresión-lineal.html#propiedad-3-consistencia" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>La <strong>consistencia</strong> es una propiedad clave que nos dice qué pasa con el estimador cuando la muestra es cada vez más grande.</p>
<blockquote>
<p>Intuitivamente, un estimador es <strong>consistente</strong> si, al aumentar el tamaño de la muestra, sus valores se acercan cada vez más al valor verdadero del parámetro <span class="math inline">\(\beta\)</span>.</p>
</blockquote>
<p>Esto nos da confianza de que, con datos suficientes, estaremos muy cerca del valor correcto.</p>
<p>Formalmente, un estimador <span class="math inline">\(\hat{\beta}\)</span> es consistente si:</p>
<p><span class="math display">\[
\hat{\beta} \xrightarrow{p} \beta \quad \text{cuando } n \to \infty
\]</span></p>
<p>Esto se lee como: “<span class="math inline">\(\hat{\beta}\)</span> converge en probabilidad a <span class="math inline">\(\beta\)</span>”.<br />
Es decir, <strong>la probabilidad de que <span class="math inline">\(\hat{\beta}\)</span> se aleje mucho de <span class="math inline">\(\beta\)</span> se hace cada vez más pequeña a medida que usamos muestras más grandes.</strong></p>
<hr />
<p>Veámos lo que ocurre cuando la muestra crece:</p>
<p><img src="_main_files/figure-html/consistency%20pdf-1.svg" width="75%" style="display: block; margin: auto;" /></p>
<p>La curva naranja representa una estimación con mucha incertidumbre (muestra pequeña). La curva gris oscura representa una muestra de tamaño mediano. Mientras que la curva rosada muestra cómo la estimación se concentra alrededor de <span class="math inline">\(\beta\)</span> con una muestra grande.</p>
<p>🎯 Un estimador consistente se “afina” con más datos: no solo mejora su varianza, sino que tiende a decir la verdad.</p>
</div>
<div id="propiedad-4-eficiencia" class="section level3 unnumbered hasAnchor">
<h3>Propiedad 4: Eficiencia<a href="regresión-lineal.html#propiedad-4-eficiencia" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>La <strong>eficiencia</strong> combina las ideas de sesgo y varianza.</p>
<blockquote>
<p>Entre todos los estimadores <strong>insesgados</strong>, el más eficiente es aquel que tiene la <strong>menor varianza</strong> posible.<br />
Es decir, si dos estimadores son igual de “correctos en promedio”, preferimos el que sea más <strong>estable</strong>.</p>
</blockquote>
<p>La eficiencia no se refiere a un único estimador, sino a una <strong>comparación entre estimadores</strong>.</p>
<hr />
<p>Formalmente, un estimador <span class="math inline">\(\hat{\beta}\)</span> es eficiente si:</p>
<p><span class="math display">\[
\text{Var}(\hat{\beta}) \leq \text{Var}(\hat{\beta}&#39;)
\]</span></p>
<p>para cualquier otro estimador <span class="math inline">\(\hat{\beta}&#39;\)</span> que también sea insesgado.</p>
<blockquote>
<p>Esto significa que <strong>ningún otro estimador insesgado</strong> tiene una varianza menor que <span class="math inline">\(\hat{\beta}\)</span>.</p>
</blockquote>
<hr />
<p>💡 En el contexto de mínimos cuadrados ordinarios (MCO), cuando se cumplen ciertos supuestos (los del teorema de Gauss-Markov), el estimador <span class="math inline">\(\hat{\beta}_{\text{MCO}}\)</span> es el <strong>Mejor Estimador Lineal Insesgado</strong>, también conocido como <strong>MELI</strong>:</p>
<blockquote>
<p>✔️ Mejor → tiene la menor varianza<br />
✔️ Estimador Lineal → combinación lineal de los datos<br />
✔️ Insesgado → <span class="math inline">\(\mathbb{E}[\hat{\beta}] = \beta\)</span></p>
</blockquote>
<hr />
<blockquote>
<p>🎓 En resumen, un estimador eficiente <strong>es tan preciso como permite la información disponible en los datos</strong>, sin sacrificar insesgamiento.</p>
</blockquote>
</div>
<div id="resumen-de-las-propiedades" class="section level3 unnumbered hasAnchor">
<h3>Resumen de las propiedades<a href="regresión-lineal.html#resumen-de-las-propiedades" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<table>
<colgroup>
<col width="13%" />
<col width="86%" />
</colgroup>
<thead>
<tr class="header">
<th>Propiedad</th>
<th>Descripción</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Insesgamiento</td>
<td>El estimador no se aleja sistemáticamente del valor verdadero.</td>
</tr>
<tr class="even">
<td>Varianza</td>
<td>El estimador tiene poca variación entre muestras.</td>
</tr>
<tr class="odd">
<td>Consistencia</td>
<td>A medida que aumenta el tamaño de la muestra, el estimador converge al valor verdadero.</td>
</tr>
<tr class="even">
<td>Eficiencia</td>
<td>El estimador tiene la menor varianza posible entre todos los estimadores insesgados.</td>
</tr>
</tbody>
</table>
<hr />
</div>
<div id="nota-de-cierre-cómo-interpretar-cada-propiedad" class="section level3 unnumbered hasAnchor">
<h3>🧠 Nota de cierre: cómo interpretar cada propiedad<a href="#nota-de-cierre-c%C3%B3mo-interpretar-cada-propiedad" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Cada propiedad que vimos tiene un enfoque ligeramente distinto sobre cómo pensar la incertidumbre:</p>
<ul>
<li><strong>Sesgo</strong>: ¿En promedio (tras repetir el experimento), el estimador acierta?</li>
<li><strong>Varianza</strong>: ¿Qué tanto cambia el estimador de una muestra a otra?</li>
<li><strong>Consistencia</strong>: ¿El estimador se acerca al valor verdadero si usamos una muestra más grande del mismo experimento?</li>
<li><strong>Eficiencia</strong>: ¿Este estimador es mejor (más preciso) que otros estimadores insesgados disponibles?</li>
</ul>
<blockquote>
<p>🔁 Las primeras dos propiedades (sesgo y varianza) se entienden a través de <em>repeticiones hipotéticas</em> del experimento.<br />
📈 La consistencia se analiza observando lo que ocurre cuando crece el tamaño muestral.<br />
⚖️ La eficiencia es una comparación entre estimadores, <strong>dado que todos sean insesgados</strong>.</p>
</blockquote>
<p>Estas ideas son fundamentales para entender cómo evaluar y justificar un estimador en econometría.</p>
<hr />
</div>
</div>
<div id="preguntas-de-repaso-1" class="section level2 unnumbered hasAnchor">
<h2>📘 Preguntas de repaso<a href="regresión-lineal.html#preguntas-de-repaso-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ol style="list-style-type: decimal">
<li>Verdadero o falso</li>
</ol>
<ul>
<li><strong>(V/F)</strong> Un estimador puede ser insesgado pero tener alta varianza.</li>
<li><strong>(V/F)</strong> La consistencia se refiere a repetir el experimento muchas veces.</li>
<li><strong>(V/F)</strong> Un estimador eficiente siempre es consistente.</li>
<li><strong>(V/F)</strong> Si un estimador es insesgado y eficiente, no puede ser mejorado bajo los supuestos del modelo.</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>Selección múltiple {-}</li>
</ol>
<p><strong>¿Cuál de las siguientes afirmaciones es correcta respecto a la eficiencia?</strong></p>
<p>A. Es una propiedad absoluta de un estimador.<br />
B. Se refiere a qué tan cerca está <span class="math inline">\(\hat{\beta}\)</span> del promedio de los datos.<br />
C. Compara la varianza entre estimadores insesgados.<br />
D. Es sinónimo de consistencia.</p>
<p><strong>¿Qué pasa con un estimador consistente cuando el tamaño muestral crece?</strong></p>
<p>A. Se vuelve insesgado automáticamente.<br />
B. Se aleja del valor verdadero.<br />
C. Su varianza se hace infinita.<br />
D. Se aproxima al valor verdadero con alta probabilidad.</p>
<ol start="3" style="list-style-type: decimal">
<li><p>Respuesta abierta</p></li>
<li><p>Explica con tus palabras qué significa que un estimador sea insesgado. ¿Por qué esta propiedad es importante en econometría?**</p></li>
<li><p>¿Por qué puede ser útil, en algunos contextos, aceptar un estimador sesgado? Da un ejemplo donde podría ser preferible.**</p></li>
<li><p>¿En qué se diferencia el concepto de varianza del de eficiencia? ¿Pueden dos estimadores tener la misma varianza pero distinta eficiencia?**</p></li>
<li><p>Supón que tienes dos estimadores: - A es insesgado pero tiene alta varianza. - B tiene un pequeño sesgo pero varianza muy baja. ¿Cuál elegirías para un problema donde la prioridad es predecir bien el valor de <span class="math inline">\(y\)</span>? ¿Cambiaría tu respuesta si el objetivo fuera estimar un efecto causal? Justifica tu elección.</p></li>
</ol>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="repaso.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="repaso-de-matrices.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": "https://github.com/adiazescobar/libro-econometria/edit/main/02-MCOsuma.Rmd",
    "text": "Edit"
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": ["_main.pdf", "_main.epub"],
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
