<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3 RegresiÃ³n lineal | Ãlgebra Matricial para EconometrÃ­a</title>
  <meta name="description" content="3 RegresiÃ³n lineal | Ãlgebra Matricial para EconometrÃ­a" />
  <meta name="generator" content="bookdown 0.34 and GitBook 2.6.7" />

  <meta property="og:title" content="3 RegresiÃ³n lineal | Ãlgebra Matricial para EconometrÃ­a" />
  <meta property="og:type" content="book" />
  
  
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3 RegresiÃ³n lineal | Ãlgebra Matricial para EconometrÃ­a" />
  
  
  

<meta name="author" content="Ana MarÃ­a DÃ­az" />


<meta name="date" content="2025-07-15" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="repaso.html"/>
<link rel="next" href="Ã¡lgebra-matricial-para-econometrÃ­a.html"/>
<script src="libs/jquery/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections/anchor-sections.js"></script>


<style type="text/css">
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">EconometrÃ­a Avanzada</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="02-MCOsuma.html"><a href="#informaci%C3%B3n-general"><i class="fa fa-check"></i><b>1</b> InformaciÃ³n general</a>
<ul>
<li class="chapter" data-level="" data-path=""><a href="#descripci%C3%B3n-del-curso"><i class="fa fa-check"></i>DescripciÃ³n del curso</a></li>
<li class="chapter" data-level="" data-path=""><a href="#material-bibliogr%C3%A1fico"><i class="fa fa-check"></i>Material bibliogrÃ¡fico</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Libro obligatorio</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#libros-recomendados"><i class="fa fa-check"></i>Libros recomendados</a></li>
</ul></li>
<li class="chapter" data-level="" data-path=""><a href="#evaluaci%C3%B3n"><i class="fa fa-check"></i>EvaluaciÃ³n</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#programa-semanal"><i class="fa fa-check"></i>Programa semanal</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#recursos-adicionales"><i class="fa fa-check"></i>Recursos adicionales</a></li>
<li class="chapter" data-level="" data-path=""><a href="#inclusi%C3%B3n"><i class="fa fa-check"></i>InclusiÃ³n</a></li>
<li class="chapter" data-level="" data-path=""><a href="#integridad-acad%C3%A9mica"><i class="fa fa-check"></i>Integridad acadÃ©mica</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="repaso.html"><a href="repaso.html"><i class="fa fa-check"></i><b>2</b> Repaso</a>
<ul>
<li class="chapter" data-level="" data-path=""><a href="#qu%C3%A9-estudia-la-econometr%C3%ADa"><i class="fa fa-check"></i>Â¿QuÃ© estudia la econometrÃ­a?</a></li>
<li class="chapter" data-level="" data-path="repaso.html"><a href="repaso.html#el-proceso-generador-de-datos"><i class="fa fa-check"></i>El proceso generador de datos</a></li>
<li class="chapter" data-level="" data-path=""><a href="#qu%C3%A9-hacemos-entonces"><i class="fa fa-check"></i>Â¿QuÃ© hacemos entonces?</a></li>
<li class="chapter" data-level="" data-path=""><a href="#construimos-una-poblaci%C3%B3n-de-juguete"><i class="fa fa-check"></i>Construimos una poblaciÃ³n de juguete</a>
<ul>
<li class="chapter" data-level="" data-path=""><a href="#la-relaci%C3%B3n-verdadera-en-la-poblaci%C3%B3n"><i class="fa fa-check"></i>La relaciÃ³n verdadera en la poblaciÃ³n</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="repaso.html"><a href="repaso.html#y-si-mi-muestra-es-mala"><i class="fa fa-check"></i>Â¿Y si mi muestra es mala?</a>
<ul>
<li class="chapter" data-level="" data-path=""><a href="#c%C3%B3mo-saber-si-me-toc%C3%B3-una-muestra-mala"><i class="fa fa-check"></i>Â¿CÃ³mo saber si me tocÃ³ una muestra mala?</a></li>
<li class="chapter" data-level="" data-path="repaso.html"><a href="repaso.html#se-puede-reducir-la-incertidumbre-muestral"><i class="fa fa-check"></i>Â¿Se puede reducir la incertidumbre muestral?</a></li>
<li class="chapter" data-level="" data-path="repaso.html"><a href="repaso.html#la-importancia-de-la-fuente-de-los-datos"><i class="fa fa-check"></i>âœ… La importancia de la fuente de los datos</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="repaso.html"><a href="repaso.html#y-si-mantengo-fija-la-muestra"><i class="fa fa-check"></i>Â¿Y si mantengo fija la muestra?</a>
<ul>
<li class="chapter" data-level="" data-path="repaso.html"><a href="repaso.html#recordemos-el-modelo"><i class="fa fa-check"></i>Recordemos el modelo:</a></li>
<li class="chapter" data-level="" data-path=""><a href="#una-aclaraci%C3%B3n-importante-sobre-el-insesgamiento"><i class="fa fa-check"></i>â˜ï¸ Una aclaraciÃ³n importante sobre el insesgamiento</a></li>
<li class="chapter" data-level="" data-path=""><a href="#cu%C3%A1ndo-es-cierto-que-nuestras-estimaciones-se-agrupan-alrededor-del-verdadero-beta_1"><i class="fa fa-check"></i>ğŸ¯ Â¿CuÃ¡ndo es cierto que nuestras estimaciones â€œse agrupanâ€ alrededor del verdadero <span class="math inline">\(\beta_1\)</span>?</a></li>
<li class="chapter" data-level="" data-path="repaso.html"><a href="repaso.html#entonces-las-simulaciones-que-hicimos-son-realistas"><i class="fa fa-check"></i>ğŸ’¬ Entonces, Â¿las simulaciones que hicimos son â€œrealistasâ€?</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="repaso.html"><a href="repaso.html#preguntas-de-repaso"><i class="fa fa-check"></i>ğŸ“˜ Preguntas de repaso</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Ãlgebra Matricial para EconometrÃ­a</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="regresiÃ³n-lineal" class="section level1 hasAnchor" number="3">
<h1><span class="header-section-number">3</span> RegresiÃ³n lineal<a href="#regresi%C3%B3n-lineal" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="objetivo-del-capÃ­tulo" class="section level2 unnumbered hasAnchor">
<h2>ğŸ¯ Objetivo del capÃ­tulo<a href="#objetivo-del-cap%C3%ADtulo" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>En este capitulo vamos a:
1. Entender quÃ© es una regresiÃ³n lineal y cÃ³mo se ve grÃ¡ficamente.
2. Aprender cÃ³mo se calcula la mejor lÃ­nia con mÃ­nimos cuadrados ordinarios (MCO)
3. Explorar quÃ© hace un buen estiamdor y cÃ³mo evaluarlo</p>
</div>
<div id="quÃ©-significa-encontrar-la-mejor-lÃ­nea" class="section level2 unnumbered hasAnchor">
<h2>ğŸ” Â¿QuÃ© significa encontrar la â€œmejor lÃ­neaâ€?<a href="#qu%C3%A9-significa-encontrar-la-mejor-l%C3%ADnea" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Antes de hablar de estimaciones, pensemos en cÃ³mo se generan los datos:</p>
<blockquote>
<p>Supondremos que hay un <strong>modelo poblacional</strong> o proceso generador de datos:</p>
</blockquote>
<p><span class="math display">\[ y_i = \beta_0 + \beta_1 x_i + \epsilon_i \]</span></p>
<ul>
<li><span class="math inline">\(y_i\)</span>: variable dependiente (lo que queremos explicar)</li>
<li><span class="math inline">\(x_i\)</span>: variable independiente</li>
<li><span class="math inline">\(\beta_0, \beta_1\)</span>: parÃ¡metros poblacionales</li>
<li><span class="math inline">\(\epsilon_i\)</span>: <strong>tÃ©rmino de error</strong>: todo lo que afecta a <span class="math inline">\(y_i\)</span> y no estÃ¡ en <span class="math inline">\(x_i\)</span></li>
</ul>
<p>El tÃ©rmino <span class="math inline">\(\epsilon_i\)</span> captura factores no observados, errores de mediciÃ³n, y variaciÃ³n aleatoria. Es fundamental porque incluso si tuviÃ©ramos los valores verdaderos de <span class="math inline">\(\beta_0\)</span> y <span class="math inline">\(\beta_1\)</span>, seguirÃ­amos sin poder predecir perfectamente <span class="math inline">\(y_i\)</span> debido a este componente.</p>
<p>En la prÃ¡ctica, estimamos los parÃ¡metros a partir de una muestra. Esto nos da una versiÃ³n estimada del modelo:</p>
<p><span class="math display">\[ \hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_i \]</span></p>
<p>Y calculamos los <strong>residuos</strong> (errores estimados):</p>
<p><span class="math display">\[ \hat{\epsilon}_i = y_i - \hat{y}_i \]</span></p>
<p>Queremos encontrar la lÃ­nea que prediga <span class="math inline">\(y_i\)</span> con la menor cantidad posible de errores. Eso significa minimizar:</p>
<p><span class="math display">\[ \text{SRC} = \sum_{i = 1}^{n} \hat{\epsilon}_i^2 \]</span></p>
<p>Esto se conoce como el <strong>criterio de mÃ­nimos cuadrados</strong>.</p>
<div id="ilustremos-esto-con-un-ejemplo-visual" class="section level3 unnumbered hasAnchor">
<h3>ğŸ¨ Ilustremos esto con un ejemplo visual<a href="regresiÃ³n-lineal.html#ilustremos-esto-con-un-ejemplo-visual" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Creemos unos nuevos datos para ilustrar esto.</p>
<p><img src="_main_files/figure-html/ols%20vs%20lines%201-1.svg" width="75%" style="display: block; margin: auto;" /></p>
<p>La linea de regresiÃ³n es igual a <span class="math inline">\(\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x\)</span> donde _0$ y <span class="math inline">\(\hat{\beta}_1\)</span> son los parÃ¡metros estimados de la regresiÃ³n. En este caso, <span class="math inline">\(\hat{\beta}_0 = 6\)</span> y <span class="math inline">\(\hat{\beta}_1 = 0.2\)</span>. Para cada una de las observaciones podemos encontrar el y estimado <span class="math inline">\(\hat{y}_i\)</span>. En la siguiente figura, la lÃ­nea naranja representa la lÃ­nea de regresiÃ³n estimada.</p>
<p><img src="_main_files/figure-html/vs%20lines%202-1.svg" width="75%" style="display: block; margin: auto;" /></p>
<p>Para cada una de las observaciones podemos calcular los errores: <span class="math inline">\(\epsilon_i = y_i - \hat{y}_i\)</span>, como se observa en el siguiente grÃ¡fico.</p>
<p><img src="_main_files/figure-html/ols%20vs%20lines%203-1.svg" width="75%" style="display: block; margin: auto;" /></p>
<p>Ahora podemos probar con otras lineas y ver cÃ³mo se comportan los errores. En el siguiente grafico, la lÃ­nea de regresiÃ³n estimada es <span class="math inline">\(\hat{y} = 3 + 0.2 x\)</span>. Es evidente que los errores estiamdos son mÃ¡s grandes que los errores estimados en el grÃ¡fico anterior.</p>
<p><img src="_main_files/figure-html/ols%20vs%20lines%204-1.svg" width="75%" style="display: block; margin: auto;" /></p>
<p>Probemos ahora con una lÃ­nea de regresiÃ³n estimada que no se ajusta a los datos, <span class="math inline">\(\hat{y} = 10 - 0.8 x\)</span>. En este caso, los errores son aÃºn mÃ¡s grandes.</p>
<p><img src="_main_files/figure-html/ols%20vs%20lines%205-1.svg" width="75%" style="display: block; margin: auto;" /></p>
<p>Recuerda que SRC es igual a: <span class="math inline">\(\left(\sum e_i^2\right)\)</span>: Errores mÃ¡s grandes reciben penalizaciones mÃ¡s grandes.</p>
<p><img src="_main_files/figure-html/ols%20vs%20lines%206-1.svg" width="75%" style="display: block; margin: auto;" /></p>
<p>La estimaciÃ³n de MCO es la combinaciÃ³n de <span class="math inline">\(\hat{\beta}_0\)</span> y <span class="math inline">\(\hat{\beta}_1\)</span> que minimiza la SRC</p>
<p><img src="_main_files/figure-html/ols%20vs%20lines%207-1.svg" width="75%" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="mco" class="section level2 unnumbered hasAnchor">
<h2>MCO<a href="regresiÃ³n-lineal.html#mco" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="formalmente" class="section level3 unnumbered hasAnchor">
<h3>Formalmente<a href="regresiÃ³n-lineal.html#formalmente" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>En una regresiÃ³n lineal simple, el estimador de MCO proviene de escoger <span class="math inline">\(\hat{\beta}_0\)</span> y <span class="math inline">\(\hat{\beta}_1\)</span> que minimice la suma de residuos al cuadrado (SRC), <em>i.e.</em>,</p>
<p><span class="math display">\[ \min_{\hat{\beta}_0,\, \hat{\beta}_1} \text{SRC} \]</span></p>
<p>donde
<span class="math display">\[ \text{SRC} = \sum_{i = 1}^{n} (y_i - \hat{y}_i)^2 = \sum_{i = 1}^{n} (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)^2 \]</span>
El estimador de MCO es el valor de <span class="math inline">\(\hat{\beta}_0\)</span> y <span class="math inline">\(\hat{\beta}_1\)</span> que minimiza la SRC.</p>
<p>pero nosotros sabemos que <span class="math inline">\(\text{SRC} = \sum_i \tilde{\epsilon_i}^2\)</span>. Now use the definitions of <span class="math inline">\(\tilde{\epsilon_i}\)</span> and <span class="math inline">\(\hat{y}\)</span>.</p>
<p><span class="math display">\[
\begin{aligned}
  \tilde{\epsilon_i}^2 &amp;= \left( y_i - \hat{y}_i \right)^2 = \left( y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i \right)^2 \\
  &amp;= y_i^2 - 2 y_i \hat{\beta}_0 - 2 y_i \hat{\beta}_1 x_i + \hat{\beta}_0^2 + 2 \hat{\beta}_0 \hat{\beta}_1 x_i + \hat{\beta}_1^2 x_i^2
\end{aligned}
\]</span></p>
<p><strong>Recordatorio:</strong> Minimizar una funciÃ³n multivariada requiere (<strong>1</strong>) que las primeras derivadas sean iguales a cero (las <em>condiciones de primer orden</em>) y (<strong>2</strong>) las condiciones de segundo orden (concavidad).</p>
<p>Nos estamos acercando. Necesitamos <strong>minimizar la SRC</strong>.</p>
<p><span class="math display">\[ \text{SRE} = \sum_i \tilde{e_i}^2 = \sum_i \left( y_i^2 - 2 y_i \hat{\beta}_0 - 2 y_i \hat{\beta}_1 x_i + \hat{\beta}_0^2 + 2 \hat{\beta}_0 \hat{\beta}_1 x_i + \hat{\beta}_1^2 x_i^2 \right) \]</span></p>
<p>For the first-order conditions of minimization, we now take the first derivates of SSE with respect to <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span>.</p>
<p><span class="math display">\[
\begin{aligned}
  \dfrac{\partial \text{SRC}}{\partial \hat{\beta}_0} &amp;= \sum_i \left( 2 \hat{\beta}_0 + 2 \hat{\beta}_1 x_i - 2 y_i \right) = 2n \hat{\beta}_0 + 2 \hat{\beta}_1 \sum_i x_i - 2 \sum_i y_i \\
  &amp;= 2n \hat{\beta}_0 + 2n \hat{\beta}_1 \overline{x} - 2n \overline{y}
\end{aligned}
\]</span></p>
<p>donde <span class="math inline">\(\overline{x} = \frac{\sum x_i}{n}\)</span> y <span class="math inline">\(\overline{y} = \frac{\sum y_i}{n}\)</span> son medias muestrales de <span class="math inline">\(x\)</span> y <span class="math inline">\(y\)</span> (de tamaÃ±o <span class="math inline">\(n\)</span>).</p>
<p>Las condiciones de primer orden establecen que las derivadas deben ser iguales a cero:
<span class="math display">\[ \dfrac{\partial \text{SSE}}{\partial \hat{\beta}_0} = 2n \hat{\beta}_0 + 2n \hat{\beta}_1 \overline{x} - 2n \overline{y} = 0 \]</span></p>
<p>Lo que implica</p>
<p><span class="math display">\[ \hat{\beta}_0 = \overline{y} - \hat{\beta}_1 \overline{x} \]</span>
Ahora para <span class="math inline">\(\hat{\beta}_1\)</span>.</p>
<p>Tomemos la derivada de la SRC con respecto a <span class="math inline">\(\hat{\beta}_1\)</span></p>
<p><span class="math display">\[
\begin{aligned}
  \dfrac{\partial \text{SSE}}{\partial \hat{\beta}_1} &amp;= \sum_i \left( 2 \hat{\beta}_0 x_i + 2 \hat{\beta}_1 x_i^2 - 2 y_i x_i \right) = 2 \hat{\beta}_0 \sum_i x_i + 2 \hat{\beta}_1 \sum_i x_i^2 - 2 \sum_i y_i x_i \\
  &amp;= 2n \hat{\beta}_0 \overline{x} + 2 \hat{\beta}_1 \sum_i x_i^2 - 2 \sum_i y_i x_i
\end{aligned}
\]</span>
Igualarlo a cero</p>
<p><span class="math display">\[ \dfrac{\partial \text{SSE}}{\partial \hat{\beta}_1} = 2n \hat{\beta}_0 \overline{x} + 2 \hat{\beta}_1 \sum_i x_i^2 - 2 \sum_i y_i x_i = 0 \]</span>
y reemplazarlo <span class="math inline">\(\hat{\beta}_0\)</span>, <em>i.e.</em>, <span class="math inline">\(\hat{\beta}_0 = \overline{y} - \hat{\beta}_1 \overline{x}\)</span>. Thus,</p>
<p><span class="math display">\[
2n \left(\overline{y} - \hat{\beta}_1 \overline{x}\right) \overline{x} + 2 \hat{\beta}_1 \sum_i x_i^2 - 2 \sum_i y_i x_i = 0
\]</span>
Continuando</p>
<p><span class="math display">\[ 2n \left(\overline{y} - \hat{\beta}_1 \overline{x}\right) \overline{x} + 2 \hat{\beta}_1 \sum_i x_i^2 - 2 \sum_i y_i x_i = 0 \]</span>
<span class="math display">\[ 2n \overline{y}\,\overline{x} - 2n \hat{\beta}_1 \overline{x}^2 + 2 \hat{\beta}_1 \sum_i x_i^2 - 2 \sum_i y_i x_i = 0 \]</span>
<span class="math display">\[ \implies 2 \hat{\beta}_1 \left( \sum_i x_i^2 - n \overline{x}^2 \right) = 2 \sum_i y_i x_i - 2n \overline{y}\,\overline{x} \]</span></p>
<p><span class="math display">\[ \implies \hat{\beta}_1 = \dfrac{\sum_i y_i x_i - 2n \overline{y}\,\overline{x}}{\sum_i x_i^2 - n \overline{x}^2} = \dfrac{\sum_i (x_i - \overline{x})(y_i - \overline{y})}{\sum_i (x_i - \overline{x})^2} \]</span>
LISTOO!</p>
<p>Ahora tenemos nuestros lindos estimadores</p>
<p><span class="math display">\[ \hat{\beta}_1 = \dfrac{\sum_i (x_i - \overline{x})(y_i - \overline{y})}{\sum_i (x_i - \overline{x})^2} \]</span>
and the intercept</p>
<p><span class="math display">\[ \hat{\beta}_0 = \overline{y} - \hat{\beta}_1 \overline{x} \]</span></p>
<p>Ya sabes de dÃ³nde proviene la parte de <em>mÃ­nimos cuadrados</em> en el tÃ©rmino â€œmÃ­nimos cuadrados ordinariosâ€. ğŸŠ</p>
<p>Ahora pasamos a las propiedades (implÃ­citas) de los MÃ­nimos Cuadrados Ordinarios (MCO / OLS).</p>
</div>
</div>
<div id="propiedades-y-supuestos" class="section level2 unnumbered hasAnchor">
<h2>ğŸ“Š Propiedades y supuestos<a href="regresiÃ³n-lineal.html#propiedades-y-supuestos" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="quÃ©-hace-a-un-buen-estimador" class="section level3 unnumbered hasAnchor">
<h3>Â¿QuÃ© hace a un buen estimador?<a href="#qu%C3%A9-hace-a-un-buen-estimador" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Antes de hablar de propiedades del estimador de MCO, recordemos algunas herramientas fundamentales de estadÃ­stica.</p>
</div>
<div id="repaso-funciones-de-densidad" class="section level3 hasAnchor" number="3.0.1">
<h3><span class="header-section-number">3.0.1</span> ğŸ“ˆ Repaso: Funciones de densidad<a href="regresiÃ³n-lineal.html#repaso-funciones-de-densidad" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Las <strong>funciones de densidad de probabilidad</strong> (FDP, o PDF en inglÃ©s) describen la probabilidad de que una <strong>variable aleatoria continua</strong> tome valores dentro de un intervalo dado. La probabilidad total bajo la curva es 1.</p>
<p>Ejemplo: para una variable normal estÃ¡ndar, la probabilidad de que tome un valor entre -2 y 0 es:</p>
<p><span class="math display">\[ \mathop{\text{P}}\left(-2 \leq X \leq 0\right) = 0.48 \]</span></p>
<p><img src="_main_files/figure-html/example:%20pdf-1.svg" width="75%" style="display: block; margin: auto;" /></p>
<p>Otro ejemplo clÃ¡sico es la probabilidad de que una variable aleatoria normal estÃ¡ndar tome un valor entre -1.96 y 1.96: <span class="math inline">\(\mathop{\text{P}}\left(-1.96 \leq X \leq 1.96\right) = 0.95\)</span></p>
<p><img src="_main_files/figure-html/example:%20pdf%202-1.svg" width="75%" style="display: block; margin: auto;" />
O la probabilidad de que una variable aleatoria normal estÃ¡ndar tome un valor mayor a 2: <span class="math inline">\(\mathop{\text{P}}\left(X &gt; 2\right) = 0.023\)</span></p>
<p><img src="_main_files/figure-html/example:%20pdf%203-1.svg" width="75%" style="display: block; margin: auto;" /></p>
</div>
<div id="quÃ©-propiedades-buscamos-en-un-estimador" class="section level3 unnumbered hasAnchor">
<h3>ğŸ¤” Â¿QuÃ© propiedades buscamos en un estimador?<a href="#qu%C3%A9-propiedades-buscamos-en-un-estimador" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Imaginemos que intentamos estimar un parÃ¡metro verdadero <span class="math inline">\(\beta\)</span>, y tenemos tres mÃ©todos distintos. Cada uno produce una distribuciÃ³n diferente para <span class="math inline">\(\hat{\beta}\)</span>.</p>
<p><img src="_main_files/figure-html/competing_pdfs-1.png" width="75%" style="display: block; margin: auto;" /></p>
<p><strong>Pregunta:</strong> Â¿QuÃ© propiedades podrÃ­an ser importantes para un estimador?</p>
<p><strong>Propiedad 1. Insesgamiento</strong><br />
Es decir, si repitiÃ©ramos el experimento muchas veces, Â¿el estimador tiende a acercarse al valor verdadero del parÃ¡metro que estamos tratando de estimar?</p>
<p>El sesgo mide si el estimador se acerca al valor real en promedio:</p>
<blockquote>
<p>ğŸ§ª <strong>Â¿QuÃ© significa â€œrepetir el experimentoâ€?</strong><br />
En este contexto, <em>repetir el experimento</em> puede entenderse de tres formas, todas vÃ¡lidas para pensar en la incertidumbre de un estimador:</p>
<ol style="list-style-type: decimal">
<li><strong>Cambiar la muestra</strong>: imaginar que tomamos muchas muestras aleatorias distintas de la poblaciÃ³n.<br />
</li>
<li><strong>Mantener fija la muestra, pero cambiar los errores</strong>: incluso si los valores de <span class="math inline">\(x_i\)</span> no cambian, los valores de <span class="math inline">\(y_i\)</span> pueden variar si asumimos que los errores <span class="math inline">\(\epsilon_i\)</span> son aleatorios. Recuerda que <span class="math inline">\(y_i\)</span> sigue un proceso generador de datos subyacente.<br />
</li>
<li><strong>Cambiar ambos simultÃ¡neamente</strong>: es el caso mÃ¡s comÃºn en simulaciones â€” se sortean tanto los <span class="math inline">\(x_i\)</span> como los <span class="math inline">\(\epsilon_i\)</span>.</li>
</ol>
<p>En cualquiera de los tres escenarios, obtendrÃ­amos distintos valores de <span class="math inline">\(\hat{\beta}\)</span>. Eso nos permite construir una <strong>distribuciÃ³n muestral</strong> del estimador y analizar propiedades como el sesgo.</p>
<p>âš ï¸ <strong>Importante:</strong> cuando hablamos de â€œrepetir el experimentoâ€, no queremos decir que volvamos a observar a las <em>mismas</em> personas varias veces con diferentes valores de <span class="math inline">\(x\)</span> (por ejemplo, dÃ¡ndoles distintos aÃ±os de educaciÃ³n).<br />
Lo que estamos haciendo es imaginar escenarios hipotÃ©ticos en los que la muestra o los errores cambian, y ver cÃ³mo eso afecta al estimador.<br />
Estos experimentos no se pueden realizar en la realidad con una misma persona, pero sÃ­ los podemos simular por computadora o analizar teÃ³ricamente.</p>
</blockquote>
<p><strong>MÃ¡s formalmente:</strong> Â¿La media de la distribuciÃ³n del estimador es igual al parÃ¡metro que estima?</p>
<p>En promedio (despuÃ©s de <em>muchas</em> repeticiones), Â¿el estimador tiende hacia el valor correcto?
<strong>MÃ¡s formalmente:</strong> Â¿La media de la distribuciÃ³n del estimador es igual al parÃ¡metro que estima?
<span class="math display">\[ \mathop{\text{Sesgo}}_\beta \left( \hat{\beta} \right) = \mathop{\boldsymbol{E}}\left[ \hat{\beta} \right] - \beta \]</span>
<strong>Estimador Insesagado:</strong> <span class="math inline">\(\mathop{\boldsymbol{E}}\left[ \hat{\beta} \right] = \beta\)</span></p>
<p><img src="_main_files/figure-html/unbiased_pdf-1.svg" width="75%" style="display: block; margin: auto;" /></p>
<p><strong>Estimador Sesagado:</strong> <span class="math inline">\(\mathop{\boldsymbol{E}}\left[ \hat{\beta} \right] \neq \beta\)</span></p>
<p><img src="_main_files/figure-html/biased%20pdf-1.svg" width="75%" style="display: block; margin: auto;" /></p>
<p><strong>Propiedad 2: Varianza</strong></p>
<p>TambiÃ©n queremos que nuestras estimaciones <strong>no varÃ­en demasiado de una muestra a otra</strong>. En otras palabras: <strong>queremos un estimador que sea estable</strong>, no que en cada muestra nos dÃ© un valor completamente distinto.</p>
<p>La <strong>varianza</strong> mide cuÃ¡nta variaciÃ³n hay en las estimaciones <span class="math inline">\(\hat{\beta}\)</span> que obtenemos al repetir el experimento (cambiando la muestra, los errores, o ambos):</p>
<p><span class="math display">\[
\text{Var} \left( \hat{\beta} \right) = \mathbb{E} \left[ \left( \hat{\beta} - \mathbb{E}[\hat{\beta}] \right)^2 \right]
\]</span></p>
<p>Un estimador con <strong>menor varianza</strong> produce resultados mÃ¡s consistentes entre muestras. Esto lo hace mÃ¡s confiable, incluso si no es perfecto.</p>
<blockquote>
<p>ğŸ¯ <em>Queremos que nuestras estimaciones estÃ©n â€œconcentradasâ€ cerca del valor esperado, no dispersas como tiros al aire.</em></p>
</blockquote>
<p>Veamos un ejemplo visual de cÃ³mo la varianza afecta a las distribuciones de los estimadores.</p>
<p><img src="_main_files/figure-html/variance%20pdf-1.svg" width="75%" style="display: block; margin: auto;" /></p>
<p>La curva rosada representa un estimador con baja varianza: la mayorÃ­a de los valores de <span class="math inline">\(\hat{\beta}\)</span> estÃ¡n cerca de <span class="math inline">\(\beta\)</span>. Mientras que la curva gris oscuro representa un estimador con alta varianza: sus valores estÃ¡n mÃ¡s dispersos. A igualdad de sesgo, preferimos el estimador que tenga menor varianza.</p>
</div>
<div id="el-trade-off-sesgo-vs.-varianza" class="section level3 unnumbered hasAnchor">
<h3>ğŸ¯ El trade-off: sesgo vs.Â varianza<a href="regresiÃ³n-lineal.html#el-trade-off-sesgo-vs.-varianza" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Hasta ahora hablamos del sesgo y de la varianza por separado. Pero muchas veces, mejorar uno implica empeorar el otro. Esto se conoce como el ** trade-off entre sesgo y varianza**.</p>
<blockquote>
<p>Â¿DeberÃ­amos aceptar un poco de sesgo si eso nos permite reducir mucho la varianza?</p>
</blockquote>
<p>En econometrÃ­a, solemos preferir estimadores <strong>insesgados</strong> (o al menos <strong>consistentes</strong>), porque valoramos la interpretaciÃ³n causal y teÃ³rica de los parÃ¡metros. Pero en otras disciplinas, como el aprendizaje automÃ¡tico o la predicciÃ³n estadÃ­stica, es comÃºn aceptar un pequeÃ±o sesgo si con ello se logra una gran reducciÃ³n en la varianza y, en consecuencia, una mejor predicciÃ³n promedio.</p>
<p>veÃ¡mos esta idea:</p>
<p><img src="_main_files/figure-html/variance%20bias-1.svg" width="75%" style="display: block; margin: auto;" /></p>
</div>
<div id="propiedad-3-consistencia" class="section level3 unnumbered hasAnchor">
<h3>Propiedad 3: Consistencia<a href="regresiÃ³n-lineal.html#propiedad-3-consistencia" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>La <strong>consistencia</strong> es una propiedad clave que nos dice quÃ© pasa con el estimador cuando la muestra es cada vez mÃ¡s grande.</p>
<blockquote>
<p>Intuitivamente, un estimador es <strong>consistente</strong> si, al aumentar el tamaÃ±o de la muestra, sus valores se acercan cada vez mÃ¡s al valor verdadero del parÃ¡metro <span class="math inline">\(\beta\)</span>.</p>
</blockquote>
<p>Esto nos da confianza de que, con datos suficientes, estaremos muy cerca del valor correcto.</p>
<p>Formalmente, un estimador <span class="math inline">\(\hat{\beta}\)</span> es consistente si:</p>
<p><span class="math display">\[
\hat{\beta} \xrightarrow{p} \beta \quad \text{cuando } n \to \infty
\]</span></p>
<p>Esto se lee como: â€œ<span class="math inline">\(\hat{\beta}\)</span> converge en probabilidad a <span class="math inline">\(\beta\)</span>â€.<br />
Es decir, <strong>la probabilidad de que <span class="math inline">\(\hat{\beta}\)</span> se aleje mucho de <span class="math inline">\(\beta\)</span> se hace cada vez mÃ¡s pequeÃ±a a medida que usamos muestras mÃ¡s grandes.</strong></p>
<hr />
<p>VeÃ¡mos lo que ocurre cuando la muestra crece:</p>
<p><img src="_main_files/figure-html/consistency%20pdf-1.svg" width="75%" style="display: block; margin: auto;" /></p>
<p>La curva naranja representa una estimaciÃ³n con mucha incertidumbre (muestra pequeÃ±a). La curva gris oscura representa una muestra de tamaÃ±o mediano. Mientras que la curva rosada muestra cÃ³mo la estimaciÃ³n se concentra alrededor de <span class="math inline">\(\beta\)</span> con una muestra grande.</p>
<p>ğŸ¯ Un estimador consistente se â€œafinaâ€ con mÃ¡s datos: no solo mejora su varianza, sino que tiende a decir la verdad.</p>
</div>
<div id="propiedad-4-eficiencia" class="section level3 unnumbered hasAnchor">
<h3>Propiedad 4: Eficiencia<a href="regresiÃ³n-lineal.html#propiedad-4-eficiencia" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>La <strong>eficiencia</strong> combina las ideas de sesgo y varianza.</p>
<blockquote>
<p>Entre todos los estimadores <strong>insesgados</strong>, el mÃ¡s eficiente es aquel que tiene la <strong>menor varianza</strong> posible.<br />
Es decir, si dos estimadores son igual de â€œcorrectos en promedioâ€, preferimos el que sea mÃ¡s <strong>estable</strong>.</p>
</blockquote>
<p>La eficiencia no se refiere a un Ãºnico estimador, sino a una <strong>comparaciÃ³n entre estimadores</strong>.</p>
<hr />
<p>Formalmente, un estimador <span class="math inline">\(\hat{\beta}\)</span> es eficiente si:</p>
<p><span class="math display">\[
\text{Var}(\hat{\beta}) \leq \text{Var}(\hat{\beta}&#39;)
\]</span></p>
<p>para cualquier otro estimador <span class="math inline">\(\hat{\beta}&#39;\)</span> que tambiÃ©n sea insesgado.</p>
<blockquote>
<p>Esto significa que <strong>ningÃºn otro estimador insesgado</strong> tiene una varianza menor que <span class="math inline">\(\hat{\beta}\)</span>.</p>
</blockquote>
<hr />
<p>ğŸ’¡ En el contexto de mÃ­nimos cuadrados ordinarios (MCO), cuando se cumplen ciertos supuestos (los del teorema de Gauss-Markov), el estimador <span class="math inline">\(\hat{\beta}_{\text{MCO}}\)</span> es el <strong>Mejor Estimador Lineal Insesgado</strong>, tambiÃ©n conocido como <strong>MELI</strong>:</p>
<blockquote>
<p>âœ”ï¸ Mejor â†’ tiene la menor varianza<br />
âœ”ï¸ Estimador Lineal â†’ combinaciÃ³n lineal de los datos<br />
âœ”ï¸ Insesgado â†’ <span class="math inline">\(\mathbb{E}[\hat{\beta}] = \beta\)</span></p>
</blockquote>
<hr />
<blockquote>
<p>ğŸ“ En resumen, un estimador eficiente <strong>es tan preciso como permite la informaciÃ³n disponible en los datos</strong>, sin sacrificar insesgamiento.</p>
</blockquote>
</div>
<div id="resumen-de-las-propiedades" class="section level3 unnumbered hasAnchor">
<h3>Resumen de las propiedades<a href="regresiÃ³n-lineal.html#resumen-de-las-propiedades" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<table>
<colgroup>
<col width="13%" />
<col width="86%" />
</colgroup>
<thead>
<tr>
<th>Propiedad</th>
<th>DescripciÃ³n</th>
</tr>
</thead>
<tbody>
<tr>
<td>Insesgamiento</td>
<td>El estimador no se aleja sistemÃ¡ticamente del valor verdadero.</td>
</tr>
<tr>
<td>Varianza</td>
<td>El estimador tiene poca variaciÃ³n entre muestras.</td>
</tr>
<tr>
<td>Consistencia</td>
<td>A medida que aumenta el tamaÃ±o de la muestra, el estimador converge al valor verdadero.</td>
</tr>
<tr>
<td>Eficiencia</td>
<td>El estimador tiene la menor varianza posible entre todos los estimadores insesgados.</td>
</tr>
</tbody>
</table>
<hr />
</div>
<div id="nota-de-cierre-cÃ³mo-interpretar-cada-propiedad" class="section level3 unnumbered hasAnchor">
<h3>ğŸ§  Nota de cierre: cÃ³mo interpretar cada propiedad<a href="#nota-de-cierre-c%C3%B3mo-interpretar-cada-propiedad" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Cada propiedad que vimos tiene un enfoque ligeramente distinto sobre cÃ³mo pensar la incertidumbre:</p>
<ul>
<li><strong>Sesgo</strong>: Â¿En promedio (tras repetir el experimento), el estimador acierta?</li>
<li><strong>Varianza</strong>: Â¿QuÃ© tanto cambia el estimador de una muestra a otra?</li>
<li><strong>Consistencia</strong>: Â¿El estimador se acerca al valor verdadero si usamos una muestra mÃ¡s grande del mismo experimento?</li>
<li><strong>Eficiencia</strong>: Â¿Este estimador es mejor (mÃ¡s preciso) que otros estimadores insesgados disponibles?</li>
</ul>
<blockquote>
<p>ğŸ” Las primeras dos propiedades (sesgo y varianza) se entienden a travÃ©s de <em>repeticiones hipotÃ©ticas</em> del experimento.<br />
ğŸ“ˆ La consistencia se analiza observando lo que ocurre cuando crece el tamaÃ±o muestral.<br />
âš–ï¸ La eficiencia es una comparaciÃ³n entre estimadores, <strong>dado que todos sean insesgados</strong>.</p>
</blockquote>
<p>Estas ideas son fundamentales para entender cÃ³mo evaluar y justificar un estimador en econometrÃ­a.</p>
<hr />
</div>
</div>
<div id="preguntas-de-repaso-1" class="section level2 unnumbered hasAnchor">
<h2>ğŸ“˜ Preguntas de repaso<a href="regresiÃ³n-lineal.html#preguntas-de-repaso-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ol style="list-style-type: decimal">
<li>Verdadero o falso</li>
</ol>
<ul>
<li><strong>(V/F)</strong> Un estimador puede ser insesgado pero tener alta varianza.</li>
<li><strong>(V/F)</strong> La consistencia se refiere a repetir el experimento muchas veces.</li>
<li><strong>(V/F)</strong> Un estimador eficiente siempre es consistente.</li>
<li><strong>(V/F)</strong> Si un estimador es insesgado y eficiente, no puede ser mejorado bajo los supuestos del modelo.</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>SelecciÃ³n mÃºltiple {-}</li>
</ol>
<p><strong>Â¿CuÃ¡l de las siguientes afirmaciones es correcta respecto a la eficiencia?</strong></p>
<p>A. Es una propiedad absoluta de un estimador.<br />
B. Se refiere a quÃ© tan cerca estÃ¡ <span class="math inline">\(\hat{\beta}\)</span> del promedio de los datos.<br />
C. Compara la varianza entre estimadores insesgados.<br />
D. Es sinÃ³nimo de consistencia.</p>
<p><strong>Â¿QuÃ© pasa con un estimador consistente cuando el tamaÃ±o muestral crece?</strong></p>
<p>A. Se vuelve insesgado automÃ¡ticamente.<br />
B. Se aleja del valor verdadero.<br />
C. Su varianza se hace infinita.<br />
D. Se aproxima al valor verdadero con alta probabilidad.</p>
<ol start="3" style="list-style-type: decimal">
<li><p>Respuesta abierta</p></li>
<li><p>Explica con tus palabras quÃ© significa que un estimador sea insesgado. Â¿Por quÃ© esta propiedad es importante en econometrÃ­a?**</p></li>
<li><p>Â¿Por quÃ© puede ser Ãºtil, en algunos contextos, aceptar un estimador sesgado? Da un ejemplo donde podrÃ­a ser preferible.**</p></li>
<li><p>Â¿En quÃ© se diferencia el concepto de varianza del de eficiencia? Â¿Pueden dos estimadores tener la misma varianza pero distinta eficiencia?**</p></li>
<li><p>SupÃ³n que tienes dos estimadores: - A es insesgado pero tiene alta varianza. - B tiene un pequeÃ±o sesgo pero varianza muy baja. Â¿CuÃ¡l elegirÃ­as para un problema donde la prioridad es predecir bien el valor de <span class="math inline">\(y\)</span>? Â¿CambiarÃ­a tu respuesta si el objetivo fuera estimar un efecto causal? Justifica tu elecciÃ³n.</p></li>
</ol>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="repaso.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="Ã¡lgebra-matricial-para-econometrÃ­a.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/USERNAME/REPO/edit/BRANCH/02-MCOsuma.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
