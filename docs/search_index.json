[["supuestos-de-mco.html", "4 Supuestos de MCO Proceso Generador de Datos Tabla Resumen de Supuestos S1. Linealidad en los Par√°metros S2. Exogeneidad Estricta S3. Colinealidad Imperfecta S4. Perturbaciones Esf√©ricas S5. Regresores No Estoc√°sticos S6. Normalidad del Error Glosario de S√≠mbolos üìò Preguntas de repaso", " 4 Supuestos de MCO Proceso Generador de Datos El modelo de regresi√≥n lineal parte de la siguiente estructura: \\[ Y_i = X_i \\beta + \\epsilon_i \\] Donde: - \\(Y_i\\): variable dependiente (observaci√≥n i) - \\(X_i\\): vector fila con los regresores de la observaci√≥n i - \\(\\beta\\): vector de par√°metros poblacionales - \\(\\epsilon_i\\): error poblacional (componentes no observables) - \\(i = 1, 2, ..., n\\) Esta formulaci√≥n describe el proceso generador de datos (PGD), base para los supuestos del MCO. Tabla Resumen de Supuestos Supuesto Notaci√≥n Implicaci√≥n principal S1. Linealidad en los par√°metros \\(y_i = X_i \\beta + \\epsilon_i\\) El modelo es lineal en los par√°metros S2. Exogeneidad estricta \\(\\mathbb{E}[\\epsilon_i \\mid X] = 0\\) No hay correlaci√≥n entre regresores y error S3. Colinealidad imperfecta \\(\\text{Rango}(X) = K\\) No hay multicolinealidad perfecta; modelo identificable S4. Perturbaciones esf√©ricas \\(\\text{Var}(\\epsilon_i \\mid X) = \\sigma^2\\), \\(\\text{Cov}(\\epsilon_i, \\epsilon_j \\mid X) = 0\\) Homocedasticidad y no autocorrelaci√≥n S5. Regresores no estoc√°sticos \\(X\\) es fija en repetidas muestras Simplifica demostraciones te√≥ricas S6. Normalidad \\(\\epsilon \\mid X \\sim \\mathcal{N}(0, \\sigma^2 I)\\) Solo necesaria para inferencia exacta S1. Linealidad en los Par√°metros El valor esperado de \\(y\\) est√° relacionado linealmente con los regresores: \\[ \\mathbb{E}[Y_i \\mid X_i] = X_i \\beta \\] Esto permite distintas formas funcionales (lineales en par√°metros): Lineal: \\(y_i = \\beta_1 + \\beta_2 x_i + \\epsilon_i\\) Log-log: \\(\\log(y_i) = \\beta_1 + \\beta_2 \\log(x_i) + \\epsilon_i\\) Log-lineal: \\(\\log(y_i) = \\beta_1 + \\beta_2 x_i + \\epsilon_i\\) Lineal-log: \\(y_i = \\beta_1 + \\beta_2 \\log(x_i) + \\epsilon_i\\) Cuadr√°tico: \\(y_i = \\beta_1 + \\beta_2 x_i + \\beta_3 x_i^2 + \\epsilon_i\\) Interactuado: \\(y_i = \\beta_1 + \\beta_2 x_1 + \\beta_3 x_2 + \\beta_4(x_1 x_2) + \\epsilon_i\\) S2. Exogeneidad Estricta \\[ \\mathbb{E}[\\epsilon_i \\mid X] = 0 \\] Esto implica que no existe relaci√≥n sistem√°tica entre los regresores y el t√©rmino de error. Ejemplos: \\(\\mathbb{E}[u \\mid X = 1] = 0\\) \\(\\mathbb{E}[u \\mid X_2 = \\text{Mujer}] = 0\\) Demostraci√≥n (Ley de la esperanza iterada): \\[ \\mathbb{E}[\\epsilon_i] = \\mathbb{E}\\left[ \\mathbb{E}[\\epsilon_i \\mid X] \\right] = \\mathbb{E}[0] = 0 \\] Equivalencia: Si \\(\\mathbb{E}[\\epsilon_i \\mid X] = 0\\), entonces: \\[ \\text{Cov}(\\epsilon_i, X_j) = 0 \\quad \\forall j \\] Pero qu√© quiere decir? Una forma de pensar en esta definici√≥n es: Para cualquier valor de \\(X\\), el valor esperado de los residuos debe ser igual a cero E.g., \\(\\mathop{E}\\left[ u \\mid X=1 \\right]=0\\) and \\(\\mathop{E}\\left[ u \\mid X=100 \\right]=0\\) E.g., \\(\\mathop{E}\\left[ u \\mid X_2=\\text{Mujer} \\right]=0\\) and \\(\\mathop{E}\\left[ u \\mid X_2=\\text{Hombre} \\right]=0\\) Note: \\(\\mathop{E}\\left[ u \\mid X \\right]=0\\) es m√°s restrictivo que \\(\\mathop{E}\\left[ u \\right]=0\\) Graficamente‚Ä¶ Exogeneidad Estricta se Incumple, i.e., \\(\\mathop{E}\\left[ \\epsilon \\mid X \\right] \\neq 0\\) S3. Colinealidad Imperfecta \\[ \\text{Rango}(X) = K \\] Para que el modelo est√© identificado, debe cumplirse que el n√∫mero de observaciones sea mayor que el n√∫mero de regresores: \\(n &gt; K\\). Violaciones comunes: Regresor constante: \\(X_j = c\\) Dos variables id√©nticas: \\(X_j = X_k\\) Combinaci√≥n lineal exacta: \\(X_3 = X_1 + X_2\\) Trampa de las variables binarias Ejemplo de matriz con rango 3: \\[ A = \\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 3 &amp; 5 &amp; 7 \\\\ 4 &amp; 6 &amp; 5 \\\\ \\end{bmatrix} \\quad \\Rightarrow \\text{Rango}(A) = 3 \\] Ejemplo de matriz con rango menor a 3: \\[ B = \\begin{bmatrix} 1 &amp; 3 &amp; 1 \\\\ 3 &amp; 8 &amp; 2 \\\\ 2 &amp; 9 &amp; 5 \\\\ \\end{bmatrix} \\quad \\Rightarrow \\text{Rango}(B) \\neq 3 \\] La tercera columna de \\(B\\) es combinaci√≥n lineal de las otras dos: \\(C_3 = -2 \\cdot C_1 + C_2\\) Wooldridge (2003) aclara que este supuesto permite que los regresores est√©n correlacionados, siempre que no haya una relaci√≥n lineal exacta entre ellos. S4. Perturbaciones Esf√©ricas Este supuesto se compone de dos condiciones: üîπ Homocedasticidad \\[ \\text{Var}(\\epsilon_i \\mid X) = \\sigma^2 \\quad \\forall i \\] La dispersi√≥n del t√©rmino de error es constante para todos los individuos. Esto significa que la varianza de los errores no depende de los regresores. üîπ No autocorrelaci√≥n \\[ \\text{Cov}(\\epsilon_i, \\epsilon_j \\mid X) = 0 \\quad \\text{para } i \\neq j \\] Los errores no est√°n correlacionados entre s√≠. Es especialmente relevante en series de tiempo, pero tambi√©n puede violarse en datos de corte transversal (e.g., por correlaci√≥n espacial). üî∏ Implicaci√≥n conjunta Cuando se cumplen homocedasticidad y no autocorrelaci√≥n: \\[ \\text{Var}(\\epsilon \\mid X) = \\sigma^2 I \\] La matriz de varianzas-covarianzas de los errores es escalar y diagonal, tambi√©n llamada matriz esf√©rica. üß† Derivaci√≥n paso a paso {-} \\[ \\text{Var}(\\epsilon \\mid X) = \\mathbb{E}[\\epsilon \\epsilon&#39; \\mid X] - \\mathbb{E}[\\epsilon \\mid X] \\cdot \\mathbb{E}[\\epsilon&#39; \\mid X] \\] Por el supuesto de exogeneidad estricta (S2), sabemos que: \\[ \\mathbb{E}[\\epsilon \\mid X] = 0 \\quad \\Rightarrow \\quad \\text{Var}(\\epsilon \\mid X) = \\mathbb{E}[\\epsilon \\epsilon&#39; \\mid X] \\] Entonces, la matriz resultante es: \\[ \\text{Var}(\\epsilon \\mid X) = \\begin{bmatrix} \\mathbb{E}[\\epsilon_1^2 \\mid X] &amp; \\mathbb{E}[\\epsilon_1 \\epsilon_2 \\mid X] &amp; \\cdots &amp; \\mathbb{E}[\\epsilon_1 \\epsilon_n \\mid X] \\\\ \\mathbb{E}[\\epsilon_2 \\epsilon_1 \\mid X] &amp; \\mathbb{E}[\\epsilon_2^2 \\mid X] &amp; \\cdots &amp; \\mathbb{E}[\\epsilon_2 \\epsilon_n \\mid X] \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\mathbb{E}[\\epsilon_n \\epsilon_1 \\mid X] &amp; \\mathbb{E}[\\epsilon_n \\epsilon_2 \\mid X] &amp; \\cdots &amp; \\mathbb{E}[\\epsilon_n^2 \\mid X] \\end{bmatrix} \\] Aplicando los supuestos: \\(\\text{Var}(\\epsilon_i \\mid X) = \\sigma^2\\) \\(\\text{Cov}(\\epsilon_i, \\epsilon_j \\mid X) = 0\\) para \\(i \\neq j\\) \\[ \\Rightarrow \\text{Var}(\\epsilon \\mid X) = \\begin{bmatrix} \\sigma^2 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\sigma^2 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; \\sigma^2 \\end{bmatrix} = \\sigma^2 I \\] Este supuesto es necesario para garantizar la eficiencia del estimador MCO bajo los supuestos cl√°sicos (Teorema de Gauss-Markov). S5. Regresores No Estoc√°sticos Este supuesto establece que la matriz de regresores \\(X\\) no es aleatoria: sus valores permanecen fijos en repeticiones del experimento o entre muestras. \\[ X = \\text{constante} \\quad \\text{(no var√≠a entre muestras)} \\] üîπ ¬øQu√© significa? Aunque en la pr√°ctica \\(X\\) proviene de una muestra aleatoria, asumir que es no estoc√°stica permite tratarlo como fijo en la teor√≠a. Esto implica que cualquier inferencia o estimaci√≥n se condiciona sobre \\(X\\). ‚úÖ Ventajas te√≥ricas Simplifica la demostraci√≥n de propiedades como insesgamiento y varianza m√≠nima. Permite eliminar la distinci√≥n entre: valor esperado condicional: \\(\\mathbb{E}[\\hat{\\beta} \\mid X]\\) y valor esperado incondicional: \\(\\mathbb{E}[\\hat{\\beta}]\\) ‚ö†Ô∏è En la pr√°ctica‚Ä¶ Este supuesto rara vez se cumple literalmente, ya que \\(X\\) normalmente proviene de una muestra aleatoria. Sin embargo, es com√∫n en teor√≠a cl√°sica porque: No afecta la validez del MCO si se asume que \\(X\\) es independiente de \\(\\epsilon\\). Se puede relajar en contextos de modelos m√°s generales (paneles, variables instrumentales, etc.). En modelos con regresores estoc√°sticos, se requiere en cambio que \\(\\mathbb{E}[\\epsilon \\mid X] = 0\\), lo que recupera el supuesto de exogeneidad estricta (S2). S6. Normalidad del Error \\[ \\epsilon \\mid X \\sim \\mathcal{N}(0, \\sigma^2 I) \\] Este supuesto establece que los errores, condicionales a los regresores, siguen una distribuci√≥n normal multivariada con media cero y matriz de varianza-covarianza esf√©rica \\(\\sigma^2 I\\). üéØ ¬øPara qu√© sirve? Este supuesto no es necesario para que el estimador de M√≠nimos Cuadrados Ordinarios (MCO) sea: Insesgado (S2 ya garantiza eso), Eficiente entre estimadores lineales insesgados (por el Teorema de Gauss-Markov). Sin embargo, s√≠ es crucial para que se cumpla la distribuci√≥n exacta de ciertos estad√≠sticos en muestras peque√±as. ‚úÖ Aplicaciones de la normalidad: Validez de las pruebas t para significancia individual. Validez de las pruebas F para restricciones conjuntas. Construcci√≥n exacta de intervalos de confianza para \\(\\beta\\). üß† ¬øQu√© pasa en muestras grandes? Gracias al Teorema Central del L√≠mite y **La Ley de los Grandes N√∫meros*, incluso si \\(\\epsilon\\) no es normal, el estimador \\(\\hat{\\beta}\\) tender√° a seguir una distribuci√≥n normal asint√≥tica: \\[ \\hat{\\beta} \\overset{approx}{\\sim} \\mathcal{N}\\left(\\beta, \\sigma^2 (X&#39;X)^{-1}\\right) \\] Por eso, la normalidad puede relajarse si \\(n\\) es suficientemente grande. Glosario de S√≠mbolos S√≠mbolo Significado \\(Y_i\\) Variable dependiente \\(X_{ij}\\) Regresor j para observaci√≥n i \\(\\beta_j\\) Par√°metro poblacional \\(\\epsilon_i\\) Error poblacional \\(n\\) N√∫mero de observaciones \\(k\\) N√∫mero de regresores (sin constante) üìò Preguntas de repaso üìò 1. Conceptuales Defina brevemente los siguientes t√©rminos: Econometr√≠a te√≥rica Econometr√≠a aplicada ¬øQu√© papel juega cada uno de los seis supuestos del modelo cl√°sico de regresi√≥n lineal en garantizar las propiedades del estimador de MCO? üßÆ 2. Clasificaci√≥n de modelos Clasifique los siguientes modelos como lineales en par√°metros o no lineales: \\(y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\\) \\(\\log(y_i) = \\beta_0 + \\beta_1 \\log(x_i) + \\epsilon_i\\) \\(y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\epsilon_i\\) \\(y_i = \\frac{\\beta_0}{1 + e^{-\\beta_1 x_i}} + \\epsilon_i\\) \\(y_i = \\alpha + \\theta^{x_i} + \\epsilon_i\\) üìè 3. Interpretaci√≥n de la pendiente Interprete el coeficiente \\(\\beta_1\\) en los siguientes modelos de regresi√≥n lineal simple: \\(y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\\) \\(\\log(y_i) = \\beta_0 + \\beta_1 x_i + \\epsilon_i\\) \\(y_i = \\beta_0 + \\beta_1 \\log(x_i) + \\epsilon_i\\) \\(\\log(y_i) = \\beta_0 + \\beta_1 \\log(x_i) + \\epsilon_i\\) En cada caso, explique qu√© representa un aumento marginal en \\(x_i\\), y si los efectos son absolutos, porcentuales o el√°sticos. üé• Recursos audiovisuales ¬øQu√© hacen los economistas? (Video 1) An Uneven Paying Field (Video 2) "]]
