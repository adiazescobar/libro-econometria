# AnatomÃ­a de la RegresiÃ³n MÃºltiple

## Matrices de ProyecciÃ³n {-}

Supongamos que estamos estimando una regresiÃ³n lineal mÃºltiple del tipo:

\[
y = X\beta + \varepsilon
\]

donde:

- \( y \in \mathbb{R}^n \) es el vector de la variable dependiente
- \( X \in \mathbb{R}^{n \times k} \) es la matriz de regresores de rango completo
- \( \beta \in \mathbb{R}^k \) es el vector de parÃ¡metros

La estimaciÃ³n de mÃ­nimos cuadrados se basa en proyectar el vector \( y \) sobre el **espacio columna de \( X \)**, denotado \( \mathcal{C}(X) \). Esta proyecciÃ³n busca encontrar el punto mÃ¡s cercano en ese subespacio a \( y \), minimizando la suma de los residuos al cuadrado.

Esta proyecciÃ³n se logra mediante la **matriz de proyecciÃ³n**:

\[
\mathbf{P}_X = X(X'X)^{-1}X'
\]

Y su complemento ortogonal (que proyecta sobre el espacio ortogonal a \( \mathcal{C}(X) \)) es:

\[
\mathbf{M}_X = I - \mathbf{P}_X
\]

\[
\hat{y} = \mathbf{P}_X y \quad \text{y} \quad \hat{\varepsilon} = \mathbf{M}_X y
\]

### ğŸ” IntuiciÃ³n geomÃ©trica {-}

- \( y \): vector observado
- \( \hat{y} = \mathbf{P}_X y \): predicciÃ³n, o "sombra" de \( y \) sobre el plano generado por \( X \)
- \( \hat{\varepsilon} = y - \hat{y} \): residuo, perpendicular al plano

El estimador de MCO se obtiene al minimizar:

\[
\min_{\beta} (y - X\beta)'(y - X\beta)
\]

La soluciÃ³n es:

\[
\hat{\beta} = (X'X)^{-1}X'y
\]


### ğŸ“ Propiedades algebraicas clave {-}

| Propiedad       | \( \mathbf{P}_X \)                   | \( \mathbf{M}_X \)                   |
|----------------|--------------------------------------|--------------------------------------|
| SimetrÃ­a       | \( \mathbf{P}_X' = \mathbf{P}_X \)   | \( \mathbf{M}_X' = \mathbf{M}_X \)   |
| Idempotencia   | \( \mathbf{P}_X^2 = \mathbf{P}_X \)  | \( \mathbf{M}_X^2 = \mathbf{M}_X \)  |
| Ortogonalidad  | \( \mathbf{P}_X \mathbf{M}_X = 0 \)  | \( \hat{y} \perp \hat{\varepsilon} \) |

Estas propiedades nos permitirÃ¡n probar resultados como: ortogonalidad entre predicciÃ³n y residuos, descomposiciÃ³n de la varianza total, y derivar el estimador de varianza del error.


### âœ¨ VisualizaciÃ³n tridimensional de la proyecciÃ³n {-}

```{r proyeccion-bonita, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
library(rgl)
library(htmlwidgets)
library(knitr)

# Asegurar integraciÃ³n con knitr
setupKnitr()

# Crear escena (sin open3d)
bg3d("white")
par3d(windowRect = c(100, 100, 900, 900), zoom = 0.8)

# Espacio base
x1 <- c(1, 0, 0)
x2 <- c(0, 1, 0)
X <- cbind(x1, x2)
grid_x <- seq(-2, 2, length.out = 10)
grid_y <- seq(-2, 2, length.out = 10)
z_plane <- rep(0, 100)
y <- c(1, 1, 2)
P <- X %*% solve(t(X) %*% X) %*% t(X)
Py <- P %*% y

# Plano
surface3d(grid_x, grid_y, matrix(z_plane, 10), color = "#A7D3D3", alpha = 0.4)

# Vectores
lines3d(c(0, y[1]), c(0, y[2]), c(0, y[3]), col = "orangered", lwd = 4)
text3d(y[1], y[2], y[3], texts = "Y", col = "orangered", cex = 1.2)

lines3d(c(0, Py[1]), c(0, Py[2]), c(0, Py[3]), col = "darkgreen", lwd = 4)
text3d(Py[2], Py[2.5], Py[3], texts = "PxY", col = "darkgreen", cex = 1.2)

lines3d(c(Py[1], y[1]), c(Py[2], y[2]), c(Py[3], y[3]), col = "gray40", lwd = 3)
text3d((y[1]+Py[1])/2, (y[2]+Py[2])/2, (y[3]+Py[3])/2, texts = "Residuo", col = "gray40", cex = 1.2)

axes3d()
title3d("ProyecciÃ³n de y sobre el espacio generado por X")

# Mostrar el grÃ¡fico interactivamente en el documento
rglwidget()

```


La figura muestra cÃ³mo el vector \( y \) se proyecta sobre el espacio generado por las columnas de \( X \). La diferencia \( y - P_X y \) es ortogonal al plano y corresponde a los residuos.

## Teorema de Frisch-Waugh-Lovell (FWL) {-}

El **Teorema de Frisch-Waugh-Lovell (FWL)** es uno de los resultados mÃ¡s importantes de la econometrÃ­a, porque **nos permite entender quÃ© significa realmente el coeficiente de una variable en una regresiÃ³n mÃºltiple**.

FWL nos dice que, si estamos interesados en el efecto de una variable \( X_r \) sobre \( y \), **controlando por otras variables** \( X_s \), **podemos obtener exactamente el mismo coeficiente** de una manera alternativa, usando **proyecciones**.

Esto es extremadamente Ãºtil porque:

- Da una interpretaciÃ³n clara del coeficiente como un efecto "depurado".
- Permite construir regresiones parciales paso a paso.
- Ayuda a entender la mecÃ¡nica interna de los modelos de regresiÃ³n mÃºltiples.

## ğŸ¯ NotaciÃ³n y MotivaciÃ³n {-}

Considera el siguiente modelo de regresiÃ³n lineal:

\[
y = X_r \beta_r + X_s \beta_s + \varepsilon
\]

donde:

- \( y \in \mathbb{R}^{n \times 1} \) es la variable dependiente (por ejemplo, salario).
- \( X_r \in \mathbb{R}^{n \times 1} \) es la **variable de interÃ©s** (por ejemplo, educaciÃ³n).
- \( X_s \in \mathbb{R}^{n \times k} \) es el conjunto de **variables de control** (por ejemplo, experiencia, edad, gÃ©nero, etc.).
- \( \varepsilon \in \mathbb{R}^{n \times 1} \) es el error.

Esta forma tambiÃ©n puede escribirse mÃ¡s compactamente asÃ­:

\[
y = [X_r \quad X_s] \begin{bmatrix} \beta_r \\ \beta_s \end{bmatrix} + \varepsilon
\]

Donde la matriz \( X = [X_r \quad X_s] \in \mathbb{R}^{n \times (k+1)} \) contiene **todas** las variables explicativas, y el vector de parÃ¡metros \( \beta \in \mathbb{R}^{k+1} \) contiene sus coeficientes respectivos.

## â“Â¿CuÃ¡l es el problema que resuelve el FWL? {-}

Imagina que quieres saber el **efecto de la educaciÃ³n sobre el salario**, pero sabes que hay muchas otras variables (edad, experiencia, gÃ©nero, etc.) que tambiÃ©n afectan el salario. Entonces haces una regresiÃ³n mÃºltiple, y obtienes el coeficiente de educaciÃ³n controlando por todo lo demÃ¡s.

El teorema FWL te dice: *"Ese coeficiente se puede obtener en tres pasos, sin necesidad de correr la regresiÃ³n completa."* Y lo mejor: **el resultado serÃ¡ exactamente igual**.

Eso es lo que veremos a continuaciÃ³n en el paso a paso.


## âœ¨ Paso a paso del Teorema de Frisch-Waugh-Lovell (FWL) {-}

El Teorema de FWL nos dice que podemos obtener el coeficiente \( \beta_r \) de una regresiÃ³n mÃºltiple:

\[
y = X_r \beta_r + X_s \beta_s + \varepsilon
\]

realizando **tres regresiones parciales**, sin necesidad de incluir todos los regresores al mismo tiempo. A continuaciÃ³n explicamos cada paso con todo el detalle necesario.


## ğŸ§© Paso 1: Proyectar \( y \) sobre \( X_s \) y obtener los residuos {-}

Primero, eliminamos de \( y \) la parte que puede ser explicada por los controles \( X_s \). Esto se hace **regresando \( y \) sobre \( X_s \)** y guardando los **residuos**. Es decir:

\[
\tilde{y} = M_s y = (I - P_s)y
\]

donde:

- \( P_s = X_s (X_s'X_s)^{-1} X_s' \) es la **matriz de proyecciÃ³n** sobre el espacio generado por los controles \( X_s \).
- \( M_s = I - P_s \) es la **matriz de residuos** o complemento ortogonal: elimina todo lo que estÃ© explicado por \( X_s \).

ğŸ” **Â¿QuÃ© significa esto?**

- Estamos "limpiando" a \( y \), quitÃ¡ndole la parte que se debe a los controles.
- El resultado \( \tilde{y} \) representa la **parte de \( y \) que es ortogonal a los controles**.

ğŸ“ **InterpretaciÃ³n geomÃ©trica**: proyectamos \( y \) sobre el subespacio generado por \( X_s \) y nos quedamos con la componente perpendicular.


## ğŸ§© Paso 2: Proyectar \( X_r \) sobre \( X_s \) y obtener los residuos {-}

Ahora hacemos lo mismo con la variable de interÃ©s \( X_r \): le quitamos la parte que puede explicarse con los controles.

\[
\tilde{X}_r = M_s X_r = (I - P_s)X_r
\]

ğŸ” **Â¿QuÃ© significa esto?**

- Estamos "depurando" a \( X_r \), eliminando cualquier relaciÃ³n lineal con los controles.
- \( \tilde{X}_r \) es la **parte de \( X_r \) que no se puede predecir con \( X_s \)**.

ğŸ“ **InterpretaciÃ³n geomÃ©trica**: proyectamos \( X_r \) sobre \( \mathcal{C}(X_s) \) y guardamos el residuo, que es ortogonal a ese subespacio.

## ğŸ§© Paso 3: Regresar \( \tilde{y} \) sobre \( \tilde{X}_r \) {-}

Finalmente, estimamos el coeficiente que relaciona las dos variables "limpias" o **depuradas**:

\[
\tilde{y} = \tilde{X}_r \cdot \gamma + u
\]

El estimador de MCO de esta regresiÃ³n es:

\[
\hat{\gamma} = (\tilde{X}_r'\tilde{X}_r)^{-1} \tilde{X}_r' \tilde{y}
\]

ğŸ”” **Â¡Sorpresa!** Este estimador es exactamente igual a:

\[
\hat{\beta}_r \quad \text{(el coeficiente de \( X_r \) en la regresiÃ³n completa)}
\]

## âœ… InterpretaciÃ³n final{-}

Este resultado nos dice que:

- El efecto de \( X_r \) sobre \( y \), **controlando por \( X_s \)**, es igual al efecto de la **parte de \( X_r \) que no se relaciona con \( X_s \)** sobre la **parte de \( y \) que tampoco se relaciona con \( X_s \)**.
- En palabras simples: es una regresiÃ³n entre los residuos.

## ğŸ“¦ ConclusiÃ³n {-}

Este teorema tiene implicaciones profundas:

- Muestra que **controlar por variables** equivale a **quitarles su efecto tanto a la variable explicativa como a la dependiente**, y luego ver cÃ³mo se relacionan esas partes "limpias".
- Es la base para entender tÃ©cnicas como control por regresiÃ³n parcial, y tambiÃ©n para desarrollar intuiciones sobre variables instrumentales, efectos marginales y mÃ¡s.


## DemostraciÃ³n Formal {-}

Usando Ã¡lgebra matricial:

\[
\hat{\beta}_r = (X_r'M_s X_r)^{-1} X_r'M_s y
\]

Esto se deduce de la forma general del estimador de MCO:

\[
\hat{\beta} = (X'X)^{-1}X'y
\]

pero aplicado al modelo reducido, en el que \( y \) y \( X_r \) han sido "limpiados" de \( X_s \).


## ğŸ§ª Ejemplo prÃ¡ctico del Teorema de Frisch-Waugh-Lovell en Stata, R y Python {-}

A continuaciÃ³n presentamos una simulaciÃ³n sencilla para ilustrar el Teorema de Frisch-Waugh-Lovell (FWL) y mostrar cÃ³mo cambia el coeficiente estimado de una variable de interÃ©s dependiendo de la correlaciÃ³n con los controles. TambiÃ©n implementamos paso a paso la construcciÃ³n del estimador usando residuos.

---

### ğŸ”µ CÃ³digo en Stata

```stata
clear
set seed 6981473
set obs 1000

* Variable de interÃ©s Xr
gen altura = runiform()*30+150
replace altura= round(altura)
label var altura "altura"

* Crear Xs con correlaciÃ³n positiva con Xr
gen ingreso_hh = rnormal() + 5*altura

* Variable dependiente Y
gen salario = 1 + 2*altura + 5*ingreso_hh + rnormal()

* CASO 1: cov(altura, ingreso_hh)>0
reg salario altura
reg salario altura ingreso_hh

* CASO 2: cov(altura, ingreso_hh)=0
gen ingreso_hh2 = rnormal()
gen salario2 = 1 + 2*altura + 5*ingreso_hh2 + rnormal()
reg salario2 altura
reg salario2 altura ingreso_hh2

* CASO 3: cov(salario, ingreso_hh)=0
gen ingreso_hh3 = rnormal() + 3*altura
gen salario3 = 1 + 2*altura + 0*ingreso_hh3 + rnormal()
reg salario3 altura
reg salario3 altura ingreso_hh3

* Teorema de FWL paso a paso
reg salario ingreso_hh
predict My, res

reg altura ingreso_hh
predict MXr, res

reg salario altura ingreso_hh
reg My MXr
* Comparar coeficientes
di _b[MXr] _b[altura]
````
### ğŸŸ¢ CÃ³digo en R

```r
library(tidyverse)

set.seed(6981473)
n <- 1000

# Variable de interÃ©s
altura <- runif(n, 0, 30) + 150
altura <- round(altura)

# Control correlacionado con altura
ingreso_hh <- rnorm(n) + 5 * altura

# Variable dependiente
salario <- 1 + 2 * altura + 5 * ingreso_hh + rnorm(n)

df <- tibble(altura, ingreso_hh, salario)

# CASO 1: cov(altura, ingreso_hh)>0
summary(lm(salario ~ altura, data = df))
summary(lm(salario ~ altura + ingreso_hh, data = df))

# CASO 2: cov(altura, ingreso_hh)=0
df$ingreso_hh2 <- rnorm(n)
df$salario2 <- 1 + 2 * df$altura + 5 * df$ingreso_hh2 + rnorm(n)
summary(lm(salario2 ~ altura, data = df))
summary(lm(salario2 ~ altura + ingreso_hh2, data = df))

# CASO 3: cov(y, ingreso_hh)=0
df$ingreso_hh3 <- rnorm(n) + 3 * df$altura
df$salario3 <- 1 + 2 * df$altura + 0 * df$ingreso_hh3 + rnorm(n)
summary(lm(salario3 ~ altura, data = df))
summary(lm(salario3 ~ altura + ingreso_hh3, data = df))

# FWL paso a paso
modelo_y <- lm(salario ~ ingreso_hh, data = df)
df$My <- resid(modelo_y)

modelo_xr <- lm(altura ~ ingreso_hh, data = df)
df$MXr <- resid(modelo_xr)

summary(lm(My ~ MXr, data = df))  # Igual al coef. de altura

summary(lm(salario ~ altura + ingreso_hh, data = df))  # VerificaciÃ³n
# Comparar coeficientes
cat("Coeficiente de altura (residuos):", coef(lm(My ~ MXr, data = df))["MXr"], "\n")
cat("Coeficiente de altura (modelo completo):", coef(lm(salario ~ altura + ingreso_hh, data = df))["altura"], "\n")
```

### ğŸ”´ CÃ³digo en Python
```python
import numpy as np
import pandas as pd
import statsmodels.api as sm
import statsmodels.formula.api as smf

np.random.seed(6981473)
n = 1000

# Variable de interÃ©s
altura = np.round(np.random.uniform(0, 30, n) + 150)

# Control correlacionado
ingreso_hh = np.random.normal(0, 1, n) + 5 * altura

# Variable dependiente
salario = 1 + 2 * altura + 5 * ingreso_hh + np.random.normal(0, 1, n)

df = pd.DataFrame({
    'altura': altura,
    'ingreso_hh': ingreso_hh,
    'salario': salario
})

# CASO 1
print(sm.OLS.from_formula('salario ~ altura', data=df).fit().summary())
print(sm.OLS.from_formula('salario ~ altura + ingreso_hh', data=df).fit().summary())

# CASO 2
df['ingreso_hh2'] = np.random.normal(0, 1, n)
df['salario2'] = 1 + 2 * df['altura'] + 5 * df['ingreso_hh2'] + np.random.normal(0, 1, n)

print(sm.OLS.from_formula('salario2 ~ altura', data=df).fit().summary())
print(sm.OLS.from_formula('salario2 ~ altura + ingreso_hh2', data=df).fit().summary())

# CASO 3
df['ingreso_hh3'] = np.random.normal(0, 1, n) + 3 * df['altura']
df['salario3'] = 1 + 2 * df['altura'] + np.random.normal(0, 1, n)

print(sm.OLS.from_formula('salario3 ~ altura', data=df).fit().summary())
print(sm.OLS.from_formula('salario3 ~ altura + ingreso_hh3', data=df).fit().summary())

# FWL paso a paso
modelo_y = sm.OLS.from_formula('salario ~ ingreso_hh', data=df).fit()
df['My'] = modelo_y.resid

modelo_xr = sm.OLS.from_formula('altura ~ ingreso_hh', data=df).fit()
df['MXr'] = modelo_xr.resid

print(sm.OLS.from_formula('My ~ MXr', data=df).fit().summary())

# VerificaciÃ³n
print(sm.OLS.from_formula('salario ~ altura + ingreso_hh', data=df).fit().summary())
# Comparar coeficientes
print("Coeficiente de altura (residuos):", sm.OLS.from_formula('My ~ MXr', data=df).fit().params['MXr'])
print("Coeficiente de altura (modelo completo):", sm.OLS.from_formula('salario ~ altura + ingreso_hh', data=df).fit().params['altura'])
```

<div class="box-repaso">

### ğŸ“˜ Preguntas de repaso {-}

1. Â¿QuÃ© significa que el coeficiente de una variable en una regresiÃ³n mÃºltiple sea "depurado"?

### ğŸ“Œ Preguntas sobre FWL y matrices de proyecciÃ³n {-}

1. Sea \( y \in \mathbb{R}^{n \times 1} \), \( X \in \mathbb{R}^{n \times k} \), y \( D \in \mathbb{R}^{n \times 1} \) una variable binaria tal que \( D_i = 1 \) solo para una observaciÃ³n \( i \), y \( D_j = 0 \) para \( j \neq i \).
   
   a. Use los pasos del teorema de Frisch-Waugh-Lovell para demostrar que el coeficiente estimado de \( D \) representa la diferencia entre la observaciÃ³n \( i \) y la predicciÃ³n para esa observaciÃ³n basada en el resto de la muestra.  
   
   b. Â¿QuÃ© ocurre con la matriz de proyecciÃ³n \( P_D \)? Â¿QuÃ© dimensiÃ³n tiene y cÃ³mo se interpreta cuando solo proyecta sobre una observaciÃ³n?
   
   c. Â¿QuÃ© ocurre con la matriz de aniquilaciÃ³n \( M_D = I - P_D \)? Â¿QuÃ© efecto tiene sobre el resto del vector \( y \)?
   
   d. Use esta informaciÃ³n para demostrar que al incluir \( D \) en la regresiÃ³n, se estÃ¡ excluyendo efectivamente la observaciÃ³n \( i \) del resto del modelo. Es decir, la estimaciÃ³n de los coeficientes asociados a \( X \) se hace como si se eliminara la observaciÃ³n \( i \).

2. Repita el anÃ¡lisis anterior, pero ahora asuma que la variable \( D \) es una constante. Â¿QuÃ© ocurre con las matrices de proyecciÃ³n y aniquilaciÃ³n? Â¿QuÃ© interpretaciÃ³n tiene proyectar sobre una constante?

3. Suponga ahora que \( X \) es una dummy igual a 1 si el individuo es hombre, y que \( D \) es una dummy igual a 1 si el individuo es mujer. Â¿QuÃ© ocurre en este caso? Â¿CÃ³mo se interpretan los coeficientes al incluir ambas dummies en la regresiÃ³n?



### ğŸ“˜ Preguntas sobre modelos con variables binarias y constantes {-}

4. Se desea estudiar el nÃºmero de horas de lectura diaria \( Y \) como funciÃ³n del nivel educativo. Se tiene una muestra de \( N \) individuos clasificados en tres grupos:  
   - Grupo 1: estudios superiores  
   - Grupo 2: estudios medios  
   - Grupo 3: estudios bajos  

   Se definen tres variables binarias \( D_1, D_2, D_3 \), donde:

   \[
   D_j = \begin{cases}
   1 & \text{si el individuo pertenece al grupo } j \\
   0 & \text{en caso contrario}
   \end{cases}
   \]

   Al estimar el siguiente modelo:

   \[
   Y = 10 D_1 + 5 D_2 + 2 D_3 + u
   \]

   a. Demuestre matemÃ¡ticamente que las medias condicionales de horas de lectura son 10, 5 y 2 para cada grupo.

   b. Â¿QuÃ© sucede si se incluye una constante en este modelo? Â¿QuÃ© problema empÃ­rico surge? Proponga una soluciÃ³n (por ejemplo, eliminar una de las dummies para evitar colinealidad perfecta).


### ğŸ§® Preguntas sobre regresiones simples con constantes y dummies {-}

5. Si se estima una regresiÃ³n de \( Y \) contra una constante y \( D_1 \), Â¿cuÃ¡l es el intercepto y cuÃ¡l es el coeficiente de \( D_1 \)? InterprÃ©telos.

6. Si se estima una regresiÃ³n de \( Y \) contra una constante y \( D_2 \), Â¿quÃ© coeficiente acompaÃ±a a \( D_2 \)? Â¿CÃ³mo cambia la interpretaciÃ³n con respecto al caso anterior?



### ğŸ“Š Pregunta sobre FWL y Ã¡lgebra matricial {-}

7. Considere el siguiente modelo lineal sin constante:

\[
Y = X \beta + u
\]

donde \( Y \in \mathbb{R}^{n \times 1} \), \( X \in \mathbb{R}^{n \times k} \), y \( u \in \mathbb{R}^{n \times 1} \). Suponga ademÃ¡s que desea controlar por un conjunto adicional de variables \( Z \in \mathbb{R}^{n \times m} \).

   a. Defina \( \tilde{Y} = M_Z Y \) y \( \tilde{X} = M_Z X \), donde \( M_Z = I - P_Z \) y \( P_Z = Z(Z'Z)^{-1}Z' \).  
   Explique con detalle quÃ© representan estas transformaciones. Â¿QuÃ© parte de \( Y \) y de \( X \) estÃ¡n conservando? Â¿QuÃ© parte estÃ¡n eliminando?

   b. Demuestre que el vector de coeficientes \( \hat{\beta} \) para \( X \), en la regresiÃ³n de \( Y \) sobre \( X \) y \( Z \), puede obtenerse a partir de la siguiente expresiÃ³n:

   \[
   \hat{\beta} = (\tilde{X}' \tilde{X})^{-1} \tilde{X}' \tilde{Y}
   \]

   c. Â¿Bajo quÃ© condiciones es vÃ¡lida esta expresiÃ³n? Â¿QuÃ© sucede si \( \tilde{X}' \tilde{X} \) no es invertible?

   d. Explique cÃ³mo se interpreta este resultado en tÃ©rminos del Teorema de Frisch-Waugh-Lovell.

</div>
