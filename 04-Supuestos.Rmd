# Capitulo 5: Supuestos del Modelo ClÃ¡sico de RegresiÃ³n Lineal

## Proceso Generador de Datos {-}

El modelo de regresiÃ³n lineal parte de la siguiente estructura:

\[
Y_i = X_i \beta + \epsilon_i
\]

Donde:
- \( Y_i \): variable dependiente (observaciÃ³n i)
- \( X_i \): vector fila con los regresores de la observaciÃ³n i
- \( \beta \): vector de parÃ¡metros poblacionales
- \( \epsilon_i \): error poblacional (componentes no observables)
- \( i = 1, 2, ..., n \)

> Esta formulaciÃ³n describe el proceso generador de datos (PGD), base para los supuestos del MCO.



## Tabla Resumen de Supuestos {-}

| Supuesto                         | NotaciÃ³n                                        | ImplicaciÃ³n principal                                                   |
|----------------------------------|--------------------------------------------------|-------------------------------------------------------------------------|
| S1. Linealidad en los parÃ¡metros | \( y_i = X_i \beta + \epsilon_i \)              | El modelo es lineal en los parÃ¡metros                                  |
| S2. Exogeneidad estricta         | \( \mathbb{E}[\epsilon_i \mid X] = 0 \)         | No hay correlaciÃ³n entre regresores y error                            |
| S3. Colinealidad imperfecta      | \( \text{Rango}(X) = K \)                       | No hay multicolinealidad perfecta; modelo identificable                |
| S4. Perturbaciones esfÃ©ricas     | \( \text{Var}(\epsilon_i \mid X) = \sigma^2 \), \( \text{Cov}(\epsilon_i, \epsilon_j \mid X) = 0 \) | Homocedasticidad y no autocorrelaciÃ³n                                 |
| S5. Regresores no estocÃ¡sticos   | \( X \) es fija en repetidas muestras           | Simplifica demostraciones teÃ³ricas                                     |
| S6. Normalidad                   | \( \epsilon \mid X \sim \mathcal{N}(0, \sigma^2 I) \) | Solo necesaria para inferencia exacta                                  |



## S1. Linealidad en los ParÃ¡metros {-}

El valor esperado de \( y \) estÃ¡ relacionado linealmente con los regresores:

\[
\mathbb{E}[Y_i \mid X_i] = X_i \beta
\]

Esto permite distintas formas funcionales (lineales en parÃ¡metros):

- Lineal: \( y_i = \beta_1 + \beta_2 x_i + \epsilon_i \)
- Log-log: \( \log(y_i) = \beta_1 + \beta_2 \log(x_i) + \epsilon_i \)
- Log-lineal: \( \log(y_i) = \beta_1 + \beta_2 x_i + \epsilon_i \)
- Lineal-log: \( y_i = \beta_1 + \beta_2 \log(x_i) + \epsilon_i \)
- CuadrÃ¡tico: \( y_i = \beta_1 + \beta_2 x_i + \beta_3 x_i^2 + \epsilon_i \)
- Interactuado: \( y_i = \beta_1 + \beta_2 x_1 + \beta_3 x_2 + \beta_4(x_1 x_2) + \epsilon_i \)



## S2. Exogeneidad Estricta {-}

\[
\mathbb{E}[\epsilon_i \mid X] = 0
\]

>Esto implica que no existe relaciÃ³n sistemÃ¡tica entre los regresores y el tÃ©rmino de error.

Ejemplos:

- \( \mathbb{E}[u \mid X = 1] = 0 \)
- \( \mathbb{E}[u \mid X_2 = \text{Mujer}] = 0 \)

DemostraciÃ³n (Ley de la esperanza iterada):

\[
\mathbb{E}[\epsilon_i] = \mathbb{E}\left[ \mathbb{E}[\epsilon_i \mid X] \right] = \mathbb{E}[0] = 0
\]

Equivalencia: 
Si \( \mathbb{E}[\epsilon_i \mid X] = 0 \), entonces:

\[
\text{Cov}(\epsilon_i, X_j) = 0 \quad \forall j
\]


Pero quÃ© quiere decir?

Una forma de pensar en esta definiciÃ³n es:

> Para *cualquier* valor de  $X$, el valor esperado de los residuos debe ser igual a cero

- _E.g._, $\mathop{E}\left[ u \mid X=1 \right]=0$ *and* $\mathop{E}\left[ u \mid X=100 \right]=0$

- _E.g._, $\mathop{E}\left[ u \mid X_2=\text{Mujer} \right]=0$ *and* $\mathop{E}\left[ u \mid X_2=\text{Hombre} \right]=0$

- Note: $\mathop{E}\left[ u \mid X \right]=0$ es mÃ¡s restrictivo que  $\mathop{E}\left[ u \right]=0$

Graficamente...

```{R, conditional_expectation_setup, include = F, cache = T}
# Setup ----------------------------------------------------------------------------------
  # Options
  options(stringsAsFactors = F)
  # Packages
  library(pacman)
library(knitr)
  p_load(ggridges)

# Data work ------------------------------------------------------------------------------
  # Set seed
  set.seed(12345)
  # Sample size
  n <- 1e5
  # Exogenous
  e_good <- tibble(
    x = runif(n = n, min = -4, max = 4),
    e = rnorm(n)
  ) %>% mutate(x = round(x))
  # Endogenous
  e_bad <- tibble(
    x = runif(n = n, min = -4, max = 4),
    e = rnorm(n) + 0.5 * x
  ) %>% mutate(x = round(x))

# Figures: Joint densities ---------------------------------------------------------------
  # The joint plot: good
  joint_good <- ggplot(data = e_good, aes(x = e)) +
    geom_density() +
    theme_pander()
  # The joint plot: bad
  joint_bad <- ggplot(data = e_bad, aes(x = e)) +
    geom_density() +
    theme_pander()

# Figures: Conditional densities ---------------------------------------------------------
  cond_good <- ggplot(data = e_good, aes(x = e, y = as.factor(x))) +
    geom_density_ridges_gradient(
      aes(fill = after_stat(x)),
      color = "white",
      scale = 2.5,
      size = 0.2
    ) +
    # geom_vline(xintercept = 0, alpha = 0.3) +
    scale_fill_viridis(option = "magma") +
    xlab("e") +
    ylab("X") +
    theme_pander(base_family = "Fira Sans Book", base_size = 18) +
    theme(
      legend.position = "none",
      axis.title.y = element_text(angle = 0, vjust = 0.5, family = "MathJax_Math", size = 22),
      axis.title.x = element_text(family = "MathJax_Math", size = 22)
    )
  cond_bad <- ggplot(data = e_bad, aes(x = e, y = as.factor(x))) +
    geom_density_ridges_gradient(
      aes(fill = after_stat(x)),
      color = "white",
      scale = 2.5,
      size = 0.2
    ) +
    # geom_vline(xintercept = 0, alpha = 0.3) +
    scale_fill_viridis(option = "magma") +
    xlab("e") +
    ylab("X") +
    theme_pander(base_family = "Fira Sans Book", base_size = 18) +
    theme(
      legend.position = "none",
      axis.title.y = element_text(angle = 0, vjust = 0.5, family = "MathJax_Math", size = 22),
      axis.title.x = element_text(family = "MathJax_Math", size = 22)
    )
```

Exogeneidad Estricta se Incumple, _i.e._, $\mathop{E}\left[ \epsilon \mid X \right] \neq 0$


## S3. Colinealidad Imperfecta {-}

\[
\text{Rango}(X) = K
\]

Para que el modelo estÃ© identificado, debe cumplirse que el nÃºmero de observaciones sea mayor que el nÃºmero de regresores: \( n > K \).

Violaciones comunes:

1. Regresor constante: \( X_j = c \)
2. Dos variables idÃ©nticas: \( X_j = X_k \)
3. CombinaciÃ³n lineal exacta: \( X_3 = X_1 + X_2 \) *Trampa de las variables binarias*

Ejemplo de matriz con rango 3:

\[
A = \begin{bmatrix}
1 & 2 & 3 \\
3 & 5 & 7 \\
4 & 6 & 5 \\
\end{bmatrix}
\quad \Rightarrow \text{Rango}(A) = 3
\]

Ejemplo de matriz con rango **menor a 3**:

\[
B = \begin{bmatrix}
1 & 3 & 1 \\
3 & 8 & 2 \\
2 & 9 & 5 \\
\end{bmatrix}
\quad \Rightarrow \text{Rango}(B) \neq 3
\]

> La tercera columna de \( B \) es combinaciÃ³n lineal de las otras dos:  
> \( C_3 = -2 \cdot C_1 + C_2 \)

Wooldridge (2003) aclara que este supuesto **permite que los regresores estÃ©n correlacionados**, siempre que no haya una relaciÃ³n lineal exacta entre ellos.

## S4. Perturbaciones EsfÃ©ricas {-}

Este supuesto se compone de dos condiciones:

 ðŸ”¹ Homocedasticidad

\[
\text{Var}(\epsilon_i \mid X) = \sigma^2 \quad \forall i
\]

La dispersiÃ³n del tÃ©rmino de error es constante para todos los individuos. Esto significa que la varianza de los errores no depende de los regresores.


ðŸ”¹ No autocorrelaciÃ³n

\[
\text{Cov}(\epsilon_i, \epsilon_j \mid X) = 0 \quad \text{para } i \neq j
\]

Los errores no estÃ¡n correlacionados entre sÃ­. Es especialmente relevante en series de tiempo, pero tambiÃ©n puede violarse en datos de corte transversal (e.g., por correlaciÃ³n espacial).



ðŸ”¸ ImplicaciÃ³n conjunta

Cuando se cumplen homocedasticidad y no autocorrelaciÃ³n:

\[
\text{Var}(\epsilon \mid X) = \sigma^2 I
\]

La matriz de varianzas-covarianzas de los errores es **escalar y diagonal**, tambiÃ©n llamada **matriz esfÃ©rica**.



 ðŸ§  DerivaciÃ³n paso a paso {-}

\[
\text{Var}(\epsilon \mid X) = \mathbb{E}[\epsilon \epsilon' \mid X] - \mathbb{E}[\epsilon \mid X] \cdot \mathbb{E}[\epsilon' \mid X]
\]

Por el supuesto de exogeneidad estricta (S2), sabemos que:

\[
\mathbb{E}[\epsilon \mid X] = 0 \quad \Rightarrow \quad \text{Var}(\epsilon \mid X) = \mathbb{E}[\epsilon \epsilon' \mid X]
\]

Entonces, la matriz resultante es:

\[
\text{Var}(\epsilon \mid X) =
\begin{bmatrix}
\mathbb{E}[\epsilon_1^2 \mid X] & \mathbb{E}[\epsilon_1 \epsilon_2 \mid X] & \cdots & \mathbb{E}[\epsilon_1 \epsilon_n \mid X] \\
\mathbb{E}[\epsilon_2 \epsilon_1 \mid X] & \mathbb{E}[\epsilon_2^2 \mid X] & \cdots & \mathbb{E}[\epsilon_2 \epsilon_n \mid X] \\
\vdots & \vdots & \ddots & \vdots \\
\mathbb{E}[\epsilon_n \epsilon_1 \mid X] & \mathbb{E}[\epsilon_n \epsilon_2 \mid X] & \cdots & \mathbb{E}[\epsilon_n^2 \mid X]
\end{bmatrix}
\]

Aplicando los supuestos:

- \( \text{Var}(\epsilon_i \mid X) = \sigma^2 \)
- \( \text{Cov}(\epsilon_i, \epsilon_j \mid X) = 0 \) para \( i \neq j \)

\[
\Rightarrow \text{Var}(\epsilon \mid X) =
\begin{bmatrix}
\sigma^2 & 0 & \cdots & 0 \\
0 & \sigma^2 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & \sigma^2
\end{bmatrix}
= \sigma^2 I
\]


> Este supuesto es necesario para garantizar la eficiencia del estimador MCO bajo los supuestos clÃ¡sicos (Teorema de Gauss-Markov).



## S5. Regresores No EstocÃ¡sticos {-}

Este supuesto establece que la matriz de regresores \( X \) **no es aleatoria**: sus valores permanecen fijos en repeticiones del experimento o entre muestras.

\[
X = \text{constante} \quad \text{(no varÃ­a entre muestras)}
\]

 ðŸ”¹ Â¿QuÃ© significa?

Aunque en la prÃ¡ctica \( X \) proviene de una muestra aleatoria, asumir que es no estocÃ¡stica permite tratarlo como fijo en la teorÃ­a. Esto implica que cualquier inferencia o estimaciÃ³n se **condiciona sobre \( X \)**.

 âœ… Ventajas teÃ³ricas

- Simplifica la demostraciÃ³n de propiedades como insesgamiento y varianza mÃ­nima.
- Permite eliminar la distinciÃ³n entre:
  - valor esperado **condicional**: \( \mathbb{E}[\hat{\beta} \mid X] \)
  - y valor esperado **incondicional**: \( \mathbb{E}[\hat{\beta}] \)

 âš ï¸ En la prÃ¡ctica...

Este supuesto rara vez se cumple literalmente, ya que \( X \) normalmente proviene de una muestra aleatoria. Sin embargo, es comÃºn en teorÃ­a clÃ¡sica porque:

- No afecta la validez del MCO si se asume que \( X \) es **independiente de** \( \epsilon \).
- Se puede relajar en contextos de modelos mÃ¡s generales (paneles, variables instrumentales, etc.).

> En modelos con regresores estocÃ¡sticos, se requiere en cambio que \( \mathbb{E}[\epsilon \mid X] = 0 \), lo que recupera el supuesto de exogeneidad estricta (S2).




## S6. Normalidad del Error {-}

\[
\epsilon \mid X \sim \mathcal{N}(0, \sigma^2 I)
\]

Este supuesto establece que los errores, **condicionales a los regresores**, siguen una distribuciÃ³n normal multivariada con media cero y matriz de varianza-covarianza esfÃ©rica \( \sigma^2 I \).

 ðŸŽ¯ Â¿Para quÃ© sirve?

Este supuesto **no es necesario** para que el estimador de MÃ­nimos Cuadrados Ordinarios (MCO) sea:

- Insesgado (S2 ya garantiza eso),
- Eficiente entre estimadores lineales insesgados (por el Teorema de Gauss-Markov).

Sin embargo, **sÃ­ es crucial** para que se cumpla la distribuciÃ³n exacta de ciertos estadÃ­sticos en muestras pequeÃ±as.



 âœ… Aplicaciones de la normalidad:

- Validez de las pruebas **t** para significancia individual.
- Validez de las pruebas **F** para restricciones conjuntas.
- ConstrucciÃ³n exacta de intervalos de confianza para \( \beta \).



 ðŸ§  Â¿QuÃ© pasa en muestras grandes?

Gracias al **Teorema Central del LÃ­mite** y **La Ley de los Grandes NÃºmeros*, incluso si \( \epsilon \) no es normal, el estimador \( \hat{\beta} \) tenderÃ¡ a seguir una distribuciÃ³n normal asintÃ³tica:

\[
\hat{\beta} \overset{approx}{\sim} \mathcal{N}\left(\beta, \sigma^2 (X'X)^{-1}\right)
\]

> Por eso, la normalidad puede **relajarse** si \( n \) es suficientemente grande.




## Glosario de SÃ­mbolos {-}

| SÃ­mbolo | Significado                         |
|---------|-------------------------------------|
| \( Y_i \) | Variable dependiente               |
| \( X_{ij} \) | Regresor j para observaciÃ³n i    |
| \( \beta_j \) | ParÃ¡metro poblacional           |
| \( \epsilon_i \) | Error poblacional               |
| \( n \) | NÃºmero de observaciones              |
| \( k \) | NÃºmero de regresores (sin constante) |


## ðŸ“˜ Preguntas de repaso {-}

ðŸ“˜ 1. Conceptuales

Defina brevemente los siguientes tÃ©rminos:

- EconometrÃ­a teÃ³rica  
- EconometrÃ­a aplicada

Â¿QuÃ© papel juega cada uno de los seis supuestos del modelo clÃ¡sico de regresiÃ³n lineal en garantizar las propiedades del estimador de MCO?


ðŸ§® 2. ClasificaciÃ³n de modelos

 Clasifique los siguientes modelos como **lineales en parÃ¡metros** o **no lineales**:

1. \( y_i = \beta_0 + \beta_1 x_i + \epsilon_i \)  
2. \( \log(y_i) = \beta_0 + \beta_1 \log(x_i) + \epsilon_i \)  
3. \( y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \epsilon_i \)  
4. \( y_i = \frac{\beta_0}{1 + e^{-\beta_1 x_i}} + \epsilon_i \)  
5. \( y_i = \alpha + \theta^{x_i} + \epsilon_i \)


 ðŸ“ 3. InterpretaciÃ³n de la pendiente

Interprete el coeficiente \( \beta_1 \) en los siguientes modelos de regresiÃ³n lineal simple:

1. \( y_i = \beta_0 + \beta_1 x_i + \epsilon_i \)  
2. \( \log(y_i) = \beta_0 + \beta_1 x_i + \epsilon_i \)  
3. \( y_i = \beta_0 + \beta_1 \log(x_i) + \epsilon_i \)  
4. \( \log(y_i) = \beta_0 + \beta_1 \log(x_i) + \epsilon_i \)

> En cada caso, explique quÃ© representa un aumento marginal en \( x_i \), y si los efectos son absolutos, porcentuales o elÃ¡sticos.

### ðŸŽ¥ Recursos audiovisuales {-}

<div style="margin-bottom: 1em;">
  <strong>Â¿QuÃ© hacen los economistas? (Video 1)</strong><br>
  <iframe width="560" height="315" src="https://www.youtube.com/embed/iiYKRD8ochA" 
          frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" 
          allowfullscreen></iframe>
</div>

<div style="margin-bottom: 1em;">
  <strong>An Uneven Paying Field (Video 2)</strong><br>
  <iframe width="560" height="315" src="https://www.youtube.com/embed/ZWSv-7PjHIM" 
          frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" 
          allowfullscreen></iframe>
</div>



