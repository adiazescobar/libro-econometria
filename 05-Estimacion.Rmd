# Regresi√≥n por M√≠nimos Cuadrados Ordinarios (MCO)

## Modelo {-}
Antes de lanzarnos a minimizar errores, hagamos una pausa para ver c√≥mo funciona esto con √°lgebra lineal, que es el coraz√≥n de MCO.
Tenemos una regresi√≥n con n observaciones y k variables explicativas (incluyendo la constante). Entonces:


- \( y \): vector de resultados observados (\( n \times 1 \))
- \( X \): matriz de variables explicativas (\( n \times k \))
- \( \beta \): vector de coeficientes (\( k \times 1 \))
- \( \varepsilon \): vector de errores aleatorios (\( n \times 1 \))


El modelo lineal es:

$$
y = X\beta + \varepsilon
$$

En notaci√≥n matricial, esto se expresa como:

$$
\begin{bmatrix}
y_1 \\
y_2 \\
\vdots \\
y_n
\end{bmatrix}
=
\begin{bmatrix}
1 & x_{11} & \cdots & x_{1k} \\
1 & x_{21} & \cdots & x_{2k} \\
\vdots & \vdots & \ddots & \vdots \\
1 & x_{n1} & \cdots & x_{nk}
\end{bmatrix}
\begin{bmatrix}
\beta_0 \\
\beta_1 \\
\vdots \\
\beta_k
\end{bmatrix}
+
\begin{bmatrix}
\varepsilon_1 \\
\varepsilon_2 \\
\vdots \\
\varepsilon_n
\end{bmatrix}
$$


## ¬øQu√© hace MCO? {-}


La regresi√≥n por M√≠nimos Cuadrados Ordinarios (MCO) es una de las herramientas m√°s fundamentales y utilizadas en econometr√≠a. Su objetivo es simple pero poderoso: encontrar la mejor recta (o hiperplano) que se ajusta a los datos observados, minimizando los errores que cometemos al predecir.

¬øQu√© quiere decir ‚Äúminimizar errores‚Äù?

Supongamos que tenemos una relaci√≥n entre una variable dependiente \( y \) y un conjunto de variables explicativas \( X \). Queremos encontrar un vector de par√°metros \( \beta \) que explique esa relaci√≥n lo mejor posible. El problema es que los datos reales no siguen perfectamente la recta que proponemos. Siempre hay un error o residuo que capta lo que no podemos explicar.

Pero aunque no conocemos los verdaderos errores (denotados por \( \varepsilon \)), s√≠ podemos calcular cu√°nto nos estamos equivocando si asumimos un valor hipot√©tico de \( \beta \), al que llamaremos \( \tilde{\beta} \). El error estimado para cada observaci√≥n \( i \) ser√≠a:

$$
\tilde{\epsilon}_i = y_i - X_i \tilde{\beta}
$$

Y si elevamos estos errores al cuadrado (para que los negativos no se cancelen con los positivos) y los sumamos para todas las observaciones, obtenemos lo que llamamos la **Suma de los Residuos al Cuadrado (SRC)** es:

$$
SRC(\tilde{\beta}) = \sum_{i=1}^n (y_i - X_i \tilde{\beta})^2
$$

En notaci√≥n matricial:

$$
SRC(\tilde{\beta}) = (y - X\tilde{\beta})'(y - X\tilde{\beta})
$$

La idea de MCO es encontrar un valor de \( \tilde{\beta} \) que minimice esta suma de residuos al cuadrado. En otras palabras, queremos encontrar el mejor ajuste posible a nuestros datos. En la siguiente grafica veremos c√≥mo se ve esta funci√≥n objetivo y c√≥mo el valor √≥ptimo de \( \tilde{\beta} \) se encuentra en el m√≠nimo de la curva.


```{r funcion_objetivo_mco, echo=FALSE, fig.align='center', message=FALSE, warning=FALSE}
# Simular datos
set.seed(123)
n <- 50
x <- runif(n, 0, 10)
epsilon <- rnorm(n, 0, 2)
y <- 1 + 0.5 * x + epsilon

# Estimar modelo real
modelo <- lm(y ~ x)
beta_hat <- coef(modelo)[2]  # coeficiente de x

# Secuencia de valores hipot√©ticos
beta_tilde <- seq(0, 1, length.out = 200)

# Calcular SRC
SRC <- sapply(beta_tilde, function(b) {
  y_hat <- 1 + b * x
  sum((y - y_hat)^2)
})

# Dataframe
df <- data.frame(beta_tilde = beta_tilde, SRC = SRC)

# Gr√°fico
library(ggplot2)
ggplot(df, aes(x = beta_tilde, y = SRC)) +
  geom_line(color = "#1f78b4", size = 1.3) +
  geom_vline(xintercept = beta_hat, color = "#e41a1c", linetype = "dashed", size = 1) +
  geom_point(aes(x = beta_hat, y = min(SRC)), color = "#e41a1c", size = 3) +
  annotate("text",
           x = beta_hat + 0.05,
           y = min(SRC) + 20,
           label = paste0("M√≠nimo en \\u03B2\u0302 = ", round(beta_hat, 3)),
           color = "#e41a1c", size = 4.2, hjust = 0) +
  labs(
    title = "Funci√≥n objetivo de MCO",
    x = "Valores Hip√≥teticos de Beta (beta tilde)",
    y = "SRC: Suma de los residuos al cuadrado"
  ) +
  theme_minimal(base_size = 13) +
  theme(plot.subtitle = element_text(margin = margin(b = 10)))
```

Cuando tenemos m√°s de una variable explicativa, MCO sigue buscando el punto que minimiza la suma de los residuos al cuadrado (SRC), pero ahora en una superficie 3D. En el gr√°fico a continuaci√≥n, mostramos c√≥mo la SRC var√≠a con diferentes combinaciones de valores hipot√©ticos para \( \tilde{\beta}_1 \) y \( \tilde{\beta}_2 \).

La superficie muestra los errores para cada combinaci√≥n, y el m√≠nimo (el punto m√°s bajo) representa los valores estimados \( (\hat{\beta}_1, \hat{\beta}_2) \).

```{r funcion_objetivo_3d, echo=FALSE, message=FALSE, warning=FALSE}
# Cargar paquetes
library(plotly)

# Simular datos
set.seed(123)
n <- 100
x1 <- runif(n, 0, 10)
x2 <- runif(n, 5, 15)
epsilon <- rnorm(n, 0, 2)
y <- 2 + 1.5 * x1 - 0.8 * x2 + epsilon

# Valores reales
modelo <- lm(y ~ x1 + x2)
beta1_hat <- coef(modelo)[2]
beta2_hat <- coef(modelo)[3]

# Grilla de valores
beta1_vals <- seq(0, 3, length.out = 60)
beta2_vals <- seq(-2, 1, length.out = 60)

# Calcular SRC en cada punto de la grilla
SRC_matrix <- outer(beta1_vals, beta2_vals, Vectorize(function(b1, b2) {
  y_hat <- 2 + b1 * x1 + b2 * x2  # intercepto fijo en 2
  sum((y - y_hat)^2)
}))

# Gr√°fico 3D con plotly
plot_ly(
  x = ~beta1_vals,
  y = ~beta2_vals,
  z = ~SRC_matrix
) %>%
  add_surface(colorscale = "Viridis") %>%
  layout(
    title = "Funci√≥n objetivo de MCO con dos regresores",
    scene = list(
      xaxis = list(title = "Œ≤‚ÇÅ"),
      yaxis = list(title = "Œ≤‚ÇÇ"),
      zaxis = list(title = "SRC")
    )
  )
```


## ¬øC√≥mo se encuentra el vector \(\hat{\beta}\)? {-}

Queremos minimizar la SRC:

$$
\hat{\beta} = \underset{\tilde{\beta}}{\arg\min} \; (y - X\tilde{\beta})'(y - X\tilde{\beta})
$$

Expandiendo:

$$
SRC(\tilde{\beta}) = y'y - 2\tilde{\beta}'X'y + \tilde{\beta}'X'X\tilde{\beta}
$$

Para minimizar esta funci√≥n deerivamos con respecto a \( \tilde{\beta} \):

$$
\frac{\partial SRC}{\partial \tilde{\beta}} = -2X'y + 2X'X\tilde{\beta} = 0
$$

Reordenando, llegamos a las **Ecuaciones normales de MCO**:

$$
X'X \hat{\beta} = X'y
$$

Si la matriz  \( X'X \) es invertible (lo que requiere, por ejemplo, que no haya multicolinealidad perfecta), entonces podemos despejar \( \hat{\beta} \) directamente

$$
\hat{\beta} = (X'X)^{-1}X'y
$$


## ¬øEs un m√≠nimo? {-}

S√≠, y lo podemos verificar con la segunda derivada (matriz Hessiana) es:

$$
\frac{\partial^2 SRC}{\partial \tilde{\beta} \partial \tilde{\beta}'} = 2X'X
$$


Como \( X'X \) es semidefinida positiva (o positiva definida si no hay colinealidad), eso nos garantiza que el punto que encontramos con la primera derivada es en efecto un m√≠nimo.

üß† **Recordatorio:** Para funciones cuadr√°ticas, si la matriz que acompa√±a al t√©rmino cuadr√°tico (en este caso \( X'X \)) es **positiva definida**, entonces el punto cr√≠tico que encontramos resolviendo la condici√≥n de primer orden **es un m√≠nimo global**.

<div class="box-cerebro">

### üß† ¬øQu√© significa que una matriz sea semidefinida positiva? {-}

Una matriz sim√©trica \( A \) es:

- **Positiva semidefinida (PSD)** si para cualquier vector no nulo \( z \), se cumple:

  \[
  z'Az \geq 0
  \]

- **Positiva definida (PD)** si para cualquier vector no nulo \( z \), se cumple:

  \[
  z'Az > 0
  \]

En el contexto de m√≠nimos cuadrados, la matriz \( X'X \) es siempre **sim√©trica** y al menos **semidefinida positiva**. Ser√° **positiva definida** si las columnas de \( X \) no tienen colinealidad perfecta (es decir, no hay combinaci√≥n lineal exacta entre las variables).


### ¬øY por qu√© importa esto? {-}

Porque si una funci√≥n cuadr√°tica tiene la forma:

\[
f(\beta) = \beta'A\beta + b'\beta + c
\]

entonces \( f(\beta) \) tiene un m√≠nimo en el punto donde su derivada es cero **si** \( A \) es positiva definida (o semidefinida positiva en algunos casos). Esto es justo lo que ocurre en MCO.

üìå En otras palabras: el hecho de que \( X'X \) sea positiva definida es lo que garantiza que el estimador \( \hat{\beta} \) **minimiza** la suma de los residuos al cuadrado, y no la maximiza o genera un punto de silla.

</div>



## Interpretaci√≥n en t√©rminos de contraparte muestral {-}

Podemos escribir:

$$
\hat{\beta} = \left( \frac{X'X}{n} \right)^{-1} \left( \frac{X'y}{n} \right)
$$

Esto sugiere que estamos usando promedios muestrales. Bajo ciertas condiciones:

$$
\hat{\beta} \to E[X'X]^{-1}E[X'y]
$$

---

## Supuestos clave empleados hasta ac√° {-}

- **S1**: Linealidad en los par√°metros
- **S2**: \( E[\varepsilon_i \mid X_i] = 0 \)
- **S3**: \( X'X \) invertible (no colinealidad perfecta)


## Diferencia entre la regresi√≥n simple y la regresi√≥n m√∫ltiple {-}

Vamos a ver diferentes casos de regresi√≥n, desde la m√°s simple hasta la m√°s compleja, para entender la anatom√≠a de la regresi√≥n y c√≥mo MCO se adapta a cada uno.

### Regresi√≥n sin variables explicativas {-}

En este caso, el modelo es simplemente:

\[
y = \beta + \varepsilon
\]

Aqu√≠, \( \beta \) es el √∫nico par√°metro a estimar. El estimador de MCO es simplemente el promedio de \( y \):

\[
\hat{\beta} = \frac{1}{n} \sum_{i=1}^{n} y_i
\]

### Regresi√≥n simple con una variable explicativa {-}

En este caso, el modelo es:

\[
y = \beta_0 + \beta_1 x + \varepsilon
\]

Aqu√≠, tenemos dos par√°metros a estimar: \( \beta_0 \) (intercepto) y \( \beta_1 \) (pendiente). El estimador de MCO se calcula como:

\[
\hat{\beta} = (X'X)^{-1}X'y
\]

<div class="box-tarea">

### üìù Pausa {-}

Demostrar que el estimador de MCO para \( \beta_0 \) y \( \beta_1 \) en la regresi√≥n simple son:

\[
\hat{\beta}_1 = \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sum (x_i - \bar{x})^2}
\]

\[
\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}
\]

Debes partir de la formulaci√≥n matricial de MCO y aplicar √°lgebra paso a paso. üí™

Recuerda las siguientes propiedades de la sumatoria

*Promedio y suma*
Si 
\[
\bar{X} = \frac{1}{n} \sum_{i=1}^{n} X_i
\quad \Rightarrow \quad
n \bar{X} = \sum_{i=1}^{n} X_i
\]

*Suma de cuadrados centrados*
\[
\sum_{i=1}^{n} (X_i - \bar{X})^2 = \sum_{i=1}^{n} X_i^2 - n \bar{X}^2
\]

Demostraci√≥n:
\[
\sum_{i=1}^{n} (X_i - \bar{X})^2 = \sum X_i^2 - 2 \bar{X} \sum X_i + n \bar{X}^2
\]
\[
= \sum X_i^2 - 2n \bar{X}^2 + n \bar{X}^2
= \sum X_i^2 - n \bar{X}^2
\]

*Covarianza muestral*
\[
\sum_{i=1}^{n} (X_i - \bar{X})(Y_i - \bar{Y}) = \sum X_i (Y_i - \bar{Y}) - \bar{X} \sum (Y_i - \bar{Y})
\]

\[
= \sum X_i Y_i - \bar{Y} \sum X_i - \bar{X} \cdot 0
= \sum X_i Y_i - n \bar{Y} \bar{X}
= \sum X_i (Y_i - \bar{Y})
\]

</div>

### Regresi√≥n m√∫ltiple con m√∫ltiples variables explicativas {-}
En este caso, el modelo es:
\[
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_k x_k + \varepsilon
\]
Aqu√≠, tenemos \( k+1 \) par√°metros a estimar. El estimador de MCO sigue siendo:
\[
\hat{\beta} = (X'X)^{-1}X'y
\]

En el siguiente cap√≠tulo entenederemos mejor la anatom√≠a de la regresi√≥n m√∫ltiple para entender perfectamente ¬øcu√°l es la diferencia entre la regresi√≥n simple y la m√∫ltiple?



## Ap√©ndice {-}

### √Ålgebra... mucha √°lgebra {-}

Partimos del modelo de regresi√≥n simple en forma matricial:

$$
y = X\beta + \varepsilon
$$

Donde:

- \( y \) es un vector columna de dimensi√≥n \( n \times 1 \),
- \( X \) es una matriz \( n \times 2 \), con una columna de unos y una columna con la variable explicativa \( X_1 \), es decir: \( X = [1 \;\; X_1] \),
- \( \varepsilon \) es el vector de perturbaciones,
- \( \beta \) es un vector \( 2 \times 1 \) con los par√°metros a estimar.

El estimador de MCO es:

$$
\begin{bmatrix}
\hat{\beta}_0 \\
\hat{\beta}_1
\end{bmatrix}
=
\underbrace{(X'X)^{-1}}_a \cdot \underbrace{X'y}_b
$$


C√°lculo de \( X'X \)

$$
X'X =
\begin{bmatrix}
1 & 1 & \cdots & 1 \\
X_{11} & X_{12} & \cdots & X_{1n}
\end{bmatrix}
\begin{bmatrix}
1 & X_{11} \\
1 & X_{12} \\
\vdots & \vdots \\
1 & X_{1n}
\end{bmatrix}
=
\begin{bmatrix}
n & \sum X_{1i} \\
\sum X_{1i} & \sum X_{1i}^2
\end{bmatrix}
$$

Su inversa es:

$$
(X'X)^{-1} =
\frac{1}{n \sum X_{1i}^2 - (\sum X_{1i})^2}
\begin{bmatrix}
\sum X_{1i}^2 & -\sum X_{1i} \\
-\sum X_{1i} & n
\end{bmatrix}
$$



### C√°lculo de \( X'y \)

$$
X'y =
\begin{bmatrix}
1 & 1 & \cdots & 1 \\
X_{11} & X_{12} & \cdots & X_{1n}
\end{bmatrix}
\begin{bmatrix}
y_1 \\
y_2 \\
\vdots \\
y_n
\end{bmatrix}
=
\begin{bmatrix}
\sum y_i \\
\sum X_{1i} y_i
\end{bmatrix}
$$

 Producto final

Sustituyendo en la f√≥rmula del estimador:

$$
\begin{aligned}
\begin{bmatrix}
\hat{\beta}_0 \\
\hat{\beta}_1
\end{bmatrix}
&=
\frac{1}{n \sum X_{1i}^2 - (\sum X_{1i})^2}
\begin{bmatrix}
\sum X_{1i}^2 & -\sum X_{1i} \\
-\sum X_{1i} & n
\end{bmatrix}
\begin{bmatrix}
\sum y_i \\
\sum X_{1i} y_i
\end{bmatrix} \\
&=
\frac{1}{n \sum X_{1i}^2 - (\sum X_{1i})^2}
\begin{bmatrix}
\sum X_{1i}^2 \sum y_i - \sum X_{1i} \sum X_{1i} y_i \\
- \sum X_{1i} \sum y_i + n \sum X_{1i} y_i
\end{bmatrix}
\end{aligned}
$$



**Estimador de \( \hat{\beta}_1 \)**

$$
\begin{aligned}
\hat{\beta}_1 &= 
\frac{-\sum X_{1i} \sum y_i + n \sum X_{1i} y_i}{n \sum X_{1i}^2 - (\sum X_{1i})^2} \\
&= \frac{\sum (X_{1i} - \bar{X}_1)(y_i - \bar{y})}{\sum (X_{1i} - \bar{X}_1)^2}
\end{aligned}
$$

**Estimador de \( \hat{\beta}_0 \)**

$$
\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{X}_1
$$



## Regresi√≥n m√∫ltiple {-}

La matriz \( X'X \) contiene la covarianza entre las variables explicativas. Si las variables no est√°n correlacionadas, no se necesita regresi√≥n m√∫ltiple.

Asumimos:

- \( \sum X_{ji} = 0 \) para todas las variables (centradas en cero)
- \( \sum X_{ji} X_{hi} = 0 \) para todo \( j \ne h \) (no correlaci√≥n entre regresores)

Entonces \( X'X \) se convierte en:

$$
X'X =
\begin{bmatrix}
n & 0 & \cdots & 0 \\
0 & \sum X_{1i}^2 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & \sum X_{ki}^2
\end{bmatrix}
$$

Y su inversa:

$$
(X'X)^{-1} =
\begin{bmatrix}
1/n & 0 & \cdots & 0 \\
0 & 1/\sum X_{1i}^2 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & 1/\sum X_{ki}^2
\end{bmatrix}
$$

El estimador de MCO en este caso es:

$$
\hat{\beta} =
\begin{bmatrix}
\frac{\sum y}{n} \\
\frac{\sum X_1 y}{\sum X_1^2} \\
\vdots \\
\frac{\sum X_k y}{\sum X_k^2}
\end{bmatrix}
$$

---

**Conclusi√≥n**: si las variables explicativas est√°n centradas y no est√°n correlacionadas entre s√≠, entonces el estimador de MCO para cada coeficiente coincide con el estimador de regresi√≥n simple. Solo cuando hay correlaci√≥n entre regresores es necesaria la regresi√≥n m√∫ltiple para obtener efectos parciales.


## Estimador de MCO en R empleando matrices {-}

```{r mco-matrices-R, message=FALSE, warning=FALSE}
# Simular datos
set.seed(123)
n <- 100
x <- runif(n, 0, 10)
epsilon <- rnorm(n, 0, 1.5)
y <- 2 + 0.7 * x + epsilon

# Crear matrices
X <- cbind(1, x)   # matriz de dise√±o con constante
Y <- matrix(y, ncol = 1)

# Estimador de MCO
XtX_inv <- solve(t(X) %*% X)
XtY <- t(X) %*% Y
b_hat <- XtX_inv %*% XtY

# Valores ajustados y residuos
Y_hat <- X %*% b_hat
e <- Y - Y_hat

# Varianza del error y errores est√°ndar
n <- nrow(X)
k <- ncol(X)
s2 <- sum(e^2) / (n - k)
var_cov_matrix <- s2 * XtX_inv
se <- sqrt(diag(var_cov_matrix))

# Resultados
b_hat
se
```

## Estimador de MCO en Stata usando MATA {-}

```stata
clear all
set more off
sysuse auto, clear

* Generamos la constante
gen cons = 1

* Entramos a MATA
mata
// Creamos las matrices X y Y
st_view(Y=., ., "price")
st_view(X=., ., ("cons", "weight"))

n = rows(X)
k = cols(X)
df = n - k

// Estimador de MCO
b = invsym(X'X)*X'Y

// Valores ajustados y residuos
Y_hat = X*b
e = Y - Y_hat

// Suma de residuos al cuadrado
SRC = e'e

// Varianza del error
s2 = SRC / df
s = sqrt(s2)

// Matriz de varianzas y covarianzas
V = s2 * invsym(X'X)
se = sqrt(diagonal(V))

// Resultados
b
se
end

* Comparaci√≥n con comando base
reg price weight

````
## Estimador de MCO en Python usando NumPy {-}

```python

# Estimador de MCO en Python usando matrices (numpy)
import numpy as np

# Simular datos
np.random.seed(123)
n = 100
x = np.random.uniform(0, 10, size=n)
epsilon = np.random.normal(0, 1.5, size=n)
y = 2 + 0.7 * x + epsilon

# Crear matriz X con constante
X = np.column_stack((np.ones(n), x))  # n x 2
Y = y.reshape(-1, 1)  # n x 1

# Estimador MCO: beta_hat = (X'X)^(-1) X'Y
XtX_inv = np.linalg.inv(X.T @ X)
XtY = X.T @ Y
beta_hat = XtX_inv @ XtY

# Valores ajustados y residuos
Y_hat = X @ beta_hat
e = Y - Y_hat

# Varianza del error
k = X.shape[1]
s2 = (e.T @ e) / (n - k)
var_cov_matrix = s2[0, 0] * XtX_inv
se = np.sqrt(np.diag(var_cov_matrix))

# Resultados
print("Coeficientes (beta_hat):")
print(beta_hat.flatten())

print("\nErrores est√°ndar:")
print(se)

import matplotlib.pyplot as plt

plt.scatter(x, y, label='Datos', alpha=0.6)
plt.plot(x, Y_hat, color='red', label='Ajuste MCO')
plt.xlabel('x')
plt.ylabel('y')
plt.title('Regresi√≥n lineal por MCO')
plt.legend()
plt.grid(True)
plt.show()
```

<div class="box-repaso">

### üìò Preguntas de repaso {-}

A continuaci√≥n encontrar√°s algunas preguntas conceptuales y computacionales para reforzar lo aprendido sobre el estimador de M√≠nimos Cuadrados Ordinarios (MCO). Puedes intentar resolverlas en papel o programando en R, Stata o Python.

#### ‚úçÔ∏è Interpretaci√≥n de coeficientes {-}

Suponga que ha estimado la siguiente regresi√≥n:

\[
\widehat{\text{salario}} = \beta_0 + \beta_1 \cdot \text{educaci√≥n}
\]

donde:

- salario = salario por hora (en pesos)
- educaci√≥n = a√±os de educaci√≥n completados

Para cada uno de los siguientes coeficientes estimados, interprete el resultado en t√©rminos econ√≥micos:

1. \( \hat{\beta}_1 = 2000 \)
2. \( \hat{\beta}_1 = 0 \)
3. \( \hat{\beta}_1 = -500 \)
4. \( \hat{\beta}_1 = 0.15 \)
5. \( \hat{\beta}_1 = 10 \)
6. \( \hat{\beta}_0 = 2500 \)

---

#### üß† √Ålgebra y transformaciones {-}

7. ¬øQu√© ocurre con el estimador de MCO si multiplicamos la variable dependiente por una constante \( c \)?  
   *Demuestre usando la expresi√≥n matricial \( \hat{\beta} = (X'X)^{-1} X'y \).*

8. ¬øQu√© ocurre si multiplicamos una de las variables explicativas por una constante \( c \)?  
   *¬øQu√© sucede con el coeficiente estimado y con el resto de los coeficientes?*

9. Si a la variable dependiente se le suma una constante, ¬øqu√© ocurre con los estimadores?

10. Si todas las variables explicativas tienen media cero, ¬øcu√°l es el valor estimado del intercepto? ¬øPor qu√©?


#### üíª Preguntas de programaci√≥n {-}

11. Implemente una funci√≥n en R que reciba dos vectores \( x \) e \( y \), y devuelva \( \hat{\beta}_0 \), \( \hat{\beta}_1 \) y los errores est√°ndar, usando solo √°lgebra matricial.

12. En Stata, estime una regresi√≥n por MCO con `reg` y luego reproduzca todos los pasos usando `mata`. Compare los resultados y verifique que los residuos sean iguales.

13. Escriba una funci√≥n en Python que tome \( X \) e \( Y \) como matrices de NumPy y devuelva:
    - \( \hat{\beta} \),
    - la matriz de varianza-covarianza,
    - los errores est√°ndar,
    - y el \( R^2 \).

---

#### üìö Para reflexionar {-}

14. ¬øPor qu√© es importante que la matriz \( X'X \) sea positiva definida? ¬øQu√© ocurre si no lo es?

15. ¬øQu√© implicaciones tiene usar variables altamente correlacionadas en un modelo de regresi√≥n m√∫ltiple? ¬øC√≥mo lo podr√≠as detectar?

</div>


