

```{r setup-poblacion, include=FALSE}
options(htmltools.dir.version = FALSE)
library(pacman)
p_load(broom, latex2exp, ggplot2, ggthemes, viridis, dplyr, magrittr, knitr, parallel)
library(ggplot2)
library(ggtext)

# Color institucional
red_pink <- "#e64173"

# Opciones globales de knitr
opts_chunk$set(
  comment = "#>",
  fig.align = "center",
  fig.height = 9,
  fig.width  = 7,
  fig.asp = 1,
 out.width = "75%",
  warning = FALSE,
  message = FALSE
)

# Temas ggplot auxiliares
theme_empty <- theme_bw() + theme(
  line          = element_blank(),
  rect          = element_blank(),
  strip.text    = element_blank(),
  axis.text     = element_blank(),
  plot.title = element_text(size = rel(1), face = "bold", margin = margin(0,0,15,0), hjust = 0),
  axis.title = element_text(size = rel(0.85), face = "bold"),
  plot.margin   = margin(0,0,-1,-1, unit = "lines"),
  legend.position = "none")



theme_simple <- theme_bw() + theme(
  line          = element_blank(),
  panel.grid    = element_blank(),
  rect          = element_blank(),
  strip.text    = element_blank(),
  axis.text.x   = element_text(size = 14),
  axis.text.y   = element_blank(),
  axis.ticks    = element_blank(),
  plot.title    = element_blank(),
  axis.title    = element_blank(),
  legend.position = "none"
)
```
# Regresi√≥n lineal
## üéØ Objetivo del cap√≠tulo  {-}

En este capitulo vamos a: 
1. Entender qu√© es una regresi√≥n lineal y c√≥mo se ve gr√°ficamente.
2. Aprender c√≥mo se calcula la mejor l√≠nia con m√≠nimos cuadrados ordinarios (MCO)
3. Explorar qu√© hace un buen estiamdor y c√≥mo evaluarlo

## üîç ¬øQu√© significa encontrar la ‚Äúmejor l√≠nea‚Äù? {-}

Antes de hablar de estimaciones, pensemos en c√≥mo se generan los datos:

> Supondremos que hay un **modelo poblacional** o proceso generador de datos:

$$ y_i = \beta_0 + \beta_1 x_i + \epsilon_i $$

- \( y_i \): variable dependiente (lo que queremos explicar)
- \( x_i \): variable independiente
- \( \beta_0, \beta_1 \): par√°metros poblacionales
- \( \epsilon_i \): **t√©rmino de error**: todo lo que afecta a \( y_i \) y no est√° en \( x_i \)

El t√©rmino \( \epsilon_i \) captura factores no observados, errores de medici√≥n, y variaci√≥n aleatoria. Es fundamental porque incluso si tuvi√©ramos los valores verdaderos de \( \beta_0 \) y \( \beta_1 \), seguir√≠amos sin poder predecir perfectamente \( y_i \) debido a este componente.

En la pr√°ctica, estimamos los par√°metros a partir de una muestra. Esto nos da una versi√≥n estimada del modelo:

$$ \hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_i $$

Y calculamos los **residuos** (errores estimados):

$$ \hat{\epsilon}_i = y_i - \hat{y}_i $$

Queremos encontrar la l√≠nea que prediga \( y_i \) con la menor cantidad posible de errores. Eso significa minimizar:

$$ \text{SRC} = \sum_{i = 1}^{n} \hat{\epsilon}_i^2 $$

Esto se conoce como el **criterio de m√≠nimos cuadrados**.

### üé® Ilustremos esto con un ejemplo visual {-}

Creemos unos nuevos datos para ilustrar esto.

```{R, ols vs lines 1, echo = F, dev = "svg", fig.height = 6}
# Generate a population
set.seed(98731)
n <- 100
pop_df <- tibble(
  x = rnorm(n, mean = 10, sd = 2),
  y = 6 + 0.2 * x + rnorm(n, sd = 1)
) 
lm0 <- lm(y ~ x, data = pop_df)

# The plot
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
  labs(
    title = "Grafica 1. Gr√°fico de dispersi√≥n de la poblaci√≥n",
    subtitle = "Relaci√≥n entre Y y X"
  ) +
theme_empty
```

La linea de regresi√≥n es igual a $\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x$ donde \hat{\beta}_0$ y $\hat{\beta}_1$ son los par√°metros estimados de la regresi√≥n. En este caso, $\hat{\beta}_0 = 6$ y $\hat{\beta}_1 = 0.2$. Para cada una de las observaciones podemos encontrar el y estimado $\hat{y}_i$. En la siguiente figura, la l√≠nea naranja representa la l√≠nea de regresi√≥n estimada.

```{R, vs lines 2, echo = F, dev = "svg", fig.height = 6}
# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- 6
b1 <- 0.2
# The plot
ggplot(data = pop_df, aes(x = x, y = y)) +
# geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1)), size = 0.5, alpha = 0.2) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
geom_abline(intercept = b0, slope = b1, color = "orange", size = 2, alpha = 0.9) +
labs(
  title = "Gr√°fico 2. L√≠nea de regresi√≥n estimada",
  subtitle = "Relaci√≥n entre Y y X"
) +
theme_empty
```

Para cada una de las observaciones podemos calcular los errores: $\epsilon_i = y_i - \hat{y}_i$, como se observa en el siguiente gr√°fico.


```{R, ols vs lines 3, echo = F, dev = "svg", fig.height = 6}
# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- 6
b1 <- 0.2
# The plot
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1)), size = 0.5, alpha = 0.2) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
geom_abline(intercept = b0, slope = b1, color = "orange", size = 2, alpha = 0.9) +
labs(
  title = "Gr√°fico 3. Errores de la regresi√≥n",
  subtitle = "Relaci√≥n entre Y y X"
) +
theme_empty
```

Ahora podemos probar con otras lineas y ver c√≥mo se comportan los errores. En el siguiente grafico, la l√≠nea de regresi√≥n estimada es $\hat{y} = 3 + 0.2 x$. Es evidente que los errores estiamdos son m√°s grandes que los errores estimados en el gr√°fico anterior.

```{R, ols vs lines 4, echo = F, dev = "svg", fig.height = 6}
# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- 3
b1 <- 0.2
# The plot
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1)), size = 0.5, alpha = 0.2) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
geom_abline(intercept = b0, slope = b1, color = "orange", size = 2, alpha = 0.9) +
labs(
  title = "Gr√°fico 4. L√≠nea de regresi√≥n propuesta",
  subtitle = "Relaci√≥n entre Y y X"
) +
  theme_empty
```

Probemos ahora con una l√≠nea de regresi√≥n estimada que no se ajusta a los datos, $\hat{y} = 10 - 0.8 x$. En este caso, los errores son a√∫n m√°s grandes.

```{R, ols vs lines 5, echo = F, dev = "svg", fig.height = 6}
# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- 10
b1 <- -0.8
# The plot
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1)), size = 0.5, alpha = 0.2) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
geom_abline(intercept = b0, slope = b1, color = "orange", size = 2, alpha = 0.9) +
labs(
  title = "Gr√°fico 5. L√≠nea de regresi√≥n propuesta",
) +
theme_empty
```

Recuerda que SRC es igual a: $\left(\sum e_i^2\right)$: Errores m√°s grandes reciben penalizaciones m√°s grandes.

```{R, ols vs lines 6, echo = F, dev = "svg", fig.height = 6}
# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- 10
b1 <- -0.8
# The plot
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1), color = (y - y_hat(x, b0, b1))^2), size = 0.5, alpha = 0.8) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
geom_abline(intercept = b0, slope = b1, color = "orange", size = 2, alpha = 0.9) +
scale_color_viridis(option = "cividis", direction = -1) +
labs(
  title = "Gr√°fico 6. Errores de la regresi√≥n",
  subtitle = "Relaci√≥n entre Y y X"
) +
theme_empty
```

La estimaci√≥n de MCO es la combinaci√≥n de $\hat{\beta}_0$ y $\hat{\beta}_1$ que minimiza la SRC

```{R, ols vs lines 7, echo = F, dev = "svg", fig.height = 6}
# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- lm0$coefficients[1]
b1 <- lm0$coefficients[2]
# The plot
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1), color = (y - y_hat(x, b0, b1))^2), size = 0.5, alpha = 0.8) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
geom_abline(intercept = b0, slope = b1, color = red_pink, size = 2, alpha = 0.9) +
scale_color_viridis(option = "cividis", direction = -1) +
theme_empty
```


## MCO {-}

### Formalmente {-}

En una regresi√≥n lineal simple, el estimador de MCO proviene de escoger  $\hat{\beta}_0$ y $\hat{\beta}_1$ que minimice la suma de residuos al cuadrado (SRC), _i.e._,

$$ \min_{\hat{\beta}_0,\, \hat{\beta}_1} \text{SRC} $$

donde
$$ \text{SRC} = \sum_{i = 1}^{n} (y_i - \hat{y}_i)^2 = \sum_{i = 1}^{n} (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)^2 $$
El estimador de MCO es el valor de $\hat{\beta}_0$ y $\hat{\beta}_1$ que minimiza la SRC.

pero nosotros sabemos que $\text{SRC} = \sum_i \tilde{\epsilon_i}^2$. Now use the definitions of $\tilde{\epsilon_i}$ and $\hat{y}$.

$$
\begin{aligned}
  \tilde{\epsilon_i}^2 &= \left( y_i - \hat{y}_i \right)^2 = \left( y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i \right)^2 \\
  &= y_i^2 - 2 y_i \hat{\beta}_0 - 2 y_i \hat{\beta}_1 x_i + \hat{\beta}_0^2 + 2 \hat{\beta}_0 \hat{\beta}_1 x_i + \hat{\beta}_1^2 x_i^2
\end{aligned}
$$

**Recordatorio:** Minimizar una funci√≥n multivariada requiere (**1**) que las primeras derivadas sean iguales a cero (las *condiciones de primer orden*) y (**2**) las condiciones de segundo orden (concavidad).

Nos estamos acercando. Necesitamos  **minimizar la SRC**. 

$$ \text{SRE} = \sum_i \tilde{e_i}^2 = \sum_i \left( y_i^2 - 2 y_i \hat{\beta}_0 - 2 y_i \hat{\beta}_1 x_i + \hat{\beta}_0^2 + 2 \hat{\beta}_0 \hat{\beta}_1 x_i + \hat{\beta}_1^2 x_i^2 \right) $$

For the first-order conditions of minimization, we now take the first derivates of SSE with respect to $\hat{\beta}_0$ and $\hat{\beta}_1$.

$$
\begin{aligned}
  \dfrac{\partial \text{SRC}}{\partial \hat{\beta}_0} &= \sum_i \left( 2 \hat{\beta}_0 + 2 \hat{\beta}_1 x_i - 2 y_i \right) = 2n \hat{\beta}_0 + 2 \hat{\beta}_1 \sum_i x_i - 2 \sum_i y_i \\
  &= 2n \hat{\beta}_0 + 2n \hat{\beta}_1 \overline{x} - 2n \overline{y}
\end{aligned}
$$

donde $\overline{x} = \frac{\sum x_i}{n}$ y $\overline{y} = \frac{\sum y_i}{n}$ son medias muestrales de $x$ y $y$ (de tama√±o $n$).

Las condiciones de primer orden establecen que las derivadas deben ser iguales a cero: 
$$ \dfrac{\partial \text{SSE}}{\partial \hat{\beta}_0} = 2n \hat{\beta}_0 + 2n \hat{\beta}_1 \overline{x} - 2n \overline{y} = 0 $$

Lo que implica

$$ \hat{\beta}_0 = \overline{y} - \hat{\beta}_1 \overline{x} $$
Ahora para $\hat{\beta}_1$.

Tomemos la derivada de la SRC con respecto a $\hat{\beta}_1$

$$
\begin{aligned}
  \dfrac{\partial \text{SSE}}{\partial \hat{\beta}_1} &= \sum_i \left( 2 \hat{\beta}_0 x_i + 2 \hat{\beta}_1 x_i^2 - 2 y_i x_i \right) = 2 \hat{\beta}_0 \sum_i x_i + 2 \hat{\beta}_1 \sum_i x_i^2 - 2 \sum_i y_i x_i \\
  &= 2n \hat{\beta}_0 \overline{x} + 2 \hat{\beta}_1 \sum_i x_i^2 - 2 \sum_i y_i x_i
\end{aligned}
$$
Igualarlo a cero 

$$ \dfrac{\partial \text{SSE}}{\partial \hat{\beta}_1} = 2n \hat{\beta}_0 \overline{x} + 2 \hat{\beta}_1 \sum_i x_i^2 - 2 \sum_i y_i x_i = 0 $$
y reemplazarlo  $\hat{\beta}_0$, _i.e._, $\hat{\beta}_0 = \overline{y} - \hat{\beta}_1 \overline{x}$. Thus,

$$
 2n \left(\overline{y} - \hat{\beta}_1 \overline{x}\right) \overline{x} + 2 \hat{\beta}_1 \sum_i x_i^2 - 2 \sum_i y_i x_i = 0
$$
Continuando

$$ 2n \left(\overline{y} - \hat{\beta}_1 \overline{x}\right) \overline{x} + 2 \hat{\beta}_1 \sum_i x_i^2 - 2 \sum_i y_i x_i = 0 $$
$$ 2n \overline{y}\,\overline{x} - 2n \hat{\beta}_1 \overline{x}^2 + 2 \hat{\beta}_1 \sum_i x_i^2 - 2 \sum_i y_i x_i = 0 $$
$$ \implies 2 \hat{\beta}_1 \left( \sum_i x_i^2 - n \overline{x}^2 \right) = 2 \sum_i y_i x_i - 2n \overline{y}\,\overline{x} $$

$$ \implies \hat{\beta}_1 = \dfrac{\sum_i y_i x_i - 2n \overline{y}\,\overline{x}}{\sum_i x_i^2 - n \overline{x}^2} = \dfrac{\sum_i (x_i - \overline{x})(y_i - \overline{y})}{\sum_i (x_i - \overline{x})^2} $$
LISTOO!

Ahora tenemos nuestros lindos estimadores

$$ \hat{\beta}_1 = \dfrac{\sum_i (x_i - \overline{x})(y_i - \overline{y})}{\sum_i (x_i - \overline{x})^2} $$
and the intercept

$$ \hat{\beta}_0 = \overline{y} - \hat{\beta}_1 \overline{x} $$

Ya sabes de d√≥nde proviene la parte de *m√≠nimos cuadrados* en el t√©rmino "m√≠nimos cuadrados ordinarios". üéä

Ahora pasamos a las propiedades (impl√≠citas) de los  M√≠nimos Cuadrados Ordinarios (MCO / OLS).



## üìä Propiedades y supuestos {-}

### ¬øQu√© hace a un buen estimador? {-}

Antes de hablar de propiedades del estimador de MCO, recordemos algunas herramientas fundamentales de estad√≠stica.

### üìà Repaso: Funciones de densidad

Las **funciones de densidad de probabilidad** (FDP, o PDF en ingl√©s) describen la probabilidad de que una **variable aleatoria continua** tome valores dentro de un intervalo dado. La probabilidad total bajo la curva es 1.

Ejemplo: para una variable normal est√°ndar, la probabilidad de que tome un valor entre -2 y 0 es:

$$ \mathop{\text{P}}\left(-2 \leq X \leq 0\right) = 0.48 $$

```{r, example: pdf, echo=FALSE, dev="svg", fig.height=3.5}
tmp <- tibble(x = seq(-4, 4, 0.01), y = dnorm(x))
tmp <- rbind(tmp, tibble(x = seq(4, -4, -0.01), y = 0))

ggplot(data = tmp, aes(x, y)) +
  geom_polygon(fill = "grey85") +
  geom_polygon(data = tmp %>% filter(between(x, -2, 0)), fill = red_pink) +
  geom_hline(yintercept = 0, color = "black") +
  labs(
    title = "Gr√°fico 1. Funci√≥n de densidad de probabilidad",
  ) +
  theme_simple
```

Otro ejemplo cl√°sico es la probabilidad de que una variable aleatoria normal est√°ndar tome un valor entre -1.96 y 1.96: $\mathop{\text{P}}\left(-1.96 \leq X \leq 1.96\right) = 0.95$

```{R, example: pdf 2, echo = F, dev = "svg", fig.height = 3.5}
# Generate data for density's polygon
tmp <- tibble(x = seq(-4, 4, 0.01), y = dnorm(x))
tmp <- rbind(tmp, tibble(x = seq(4, -4, -0.01), y = 0))
# Plot it
ggplot(data = tmp, aes(x, y)) +
geom_polygon(fill = "grey85") +
geom_polygon(data = tmp %>% filter(between(x, -1.96, 1.96)), fill = red_pink) +
geom_hline(yintercept = 0, color = "black") +
labs(
  title = "Gr√°fico 2. Funci√≥n de densidad de probabilidad",
) +
theme_simple
```
O la probabilidad de que una variable aleatoria normal est√°ndar tome un valor mayor a 2: $\mathop{\text{P}}\left(X > 2\right) = 0.023$

```{R, example: pdf 3, echo = F, dev = "svg", fig.height = 3.5}
# Generate data for density's polygon
tmp <- tibble(x = seq(-4, 4, 0.01), y = dnorm(x))
tmp <- rbind(tmp, tibble(x = seq(4, -4, -0.01), y = 0))
# Plot it
ggplot(data = tmp, aes(x, y)) +
geom_polygon(fill = "grey85") +
geom_polygon(data = tmp %>% filter(between(x, 2, Inf)), fill = red_pink) +
geom_hline(yintercept = 0, color = "black") +
theme_simple
```

### ü§î ¬øQu√© propiedades buscamos en un estimador? {-}

Imaginemos que intentamos estimar un par√°metro verdadero \( \beta \), y tenemos tres m√©todos distintos. Cada uno produce una distribuci√≥n diferente para \( \hat{\beta} \).


```{r competing_pdfs, echo = FALSE, fig.height = 4.5}
# Cargar paquetes (por si el chunk se ejecuta solo)
library(tibble); library(dplyr); library(ggplot2)

# Datos para cada densidad -----------------------------
xs <- seq(-7.5, 7.5, 0.01)

d1 <- tibble(x = xs, y = dnorm(x, mean = 1,  sd = 1))   |>
  bind_rows(tibble(x = rev(xs), y = 0))
d2 <- tibble(x = xs, y = dunif(x, min  = -2.5, max = 1.5)) |>
  bind_rows(tibble(x = rev(xs), y = 0))
d3 <- tibble(x = xs, y = dnorm(x, mean = 0,  sd = 2.5)) |>
  bind_rows(tibble(x = rev(xs), y = 0))

# Gr√°fico ----------------------------------------------
ggplot() +
  geom_polygon(data = d1, aes(x = x, y = y),
               fill = "orange",       alpha = 0.80) +
  geom_polygon(data = d2, aes(x = x, y = y),
               fill = red_pink,       alpha = 0.65) +
  geom_polygon(data = d3, aes(x = x, y = y),
               fill = "darkslategray", alpha = 0.60) +
  geom_hline(yintercept = 0, colour = "black") +
  geom_vline(xintercept = 0, linetype = "dashed", linewidth = 1) +
  scale_x_continuous(breaks = 0, labels = expression(beta)) +   # <-- cambio
  theme_simple +
  theme(axis.text.x = element_text(size = 20))

```

**Pregunta:** ¬øQu√© propiedades podr√≠an ser importantes para un estimador?

**Propiedad 1. Insesgamiento**  
Es decir, si repiti√©ramos el experimento muchas veces, ¬øel estimador tiende a acercarse al valor verdadero del par√°metro que estamos tratando de estimar?

El sesgo mide si el estimador se acerca al valor real en promedio:

> üß™ **¬øQu√© significa "repetir el experimento"?**  
> En este contexto, *repetir el experimento* puede entenderse de tres formas, todas v√°lidas para pensar en la incertidumbre de un estimador:
>
> 1. **Cambiar la muestra**: imaginar que tomamos muchas muestras aleatorias distintas de la poblaci√≥n.  
> 2. **Mantener fija la muestra, pero cambiar los errores**: incluso si los valores de \( x_i \) no cambian, los valores de \( y_i \) pueden variar si asumimos que los errores \( \epsilon_i \) son aleatorios. Recuerda que \( y_i \) sigue un proceso generador de datos subyacente.  
> 3. **Cambiar ambos simult√°neamente**: es el caso m√°s com√∫n en simulaciones ‚Äî se sortean tanto los \( x_i \) como los \( \epsilon_i \).
>
> En cualquiera de los tres escenarios, obtendr√≠amos distintos valores de \( \hat{\beta} \). Eso nos permite construir una **distribuci√≥n muestral** del estimador y analizar propiedades como el sesgo.
>
> ‚ö†Ô∏è **Importante:** cuando hablamos de ‚Äúrepetir el experimento‚Äù, no queremos decir que volvamos a observar a las *mismas* personas varias veces con diferentes valores de \( x \) (por ejemplo, d√°ndoles distintos a√±os de educaci√≥n).  
> Lo que estamos haciendo es imaginar escenarios hipot√©ticos en los que la muestra o los errores cambian, y ver c√≥mo eso afecta al estimador.  
> Estos experimentos no se pueden realizar en la realidad con una misma persona, pero s√≠ los podemos simular por computadora o analizar te√≥ricamente.

**M√°s formalmente:** ¬øLa media de la distribuci√≥n del estimador es igual al par√°metro que estima?

En promedio (despu√©s de *muchas* repeticiones), ¬øel estimador tiende hacia el valor correcto?
**M√°s formalmente:** ¬øLa media de la distribuci√≥n del estimador es igual al par√°metro que estima?
$$ \mathop{\text{Sesgo}}_\beta \left( \hat{\beta} \right) = \mathop{\boldsymbol{E}}\left[ \hat{\beta} \right] - \beta $$
**Estimador Insesagado:** $\mathop{\boldsymbol{E}}\left[ \hat{\beta} \right] = \beta$

```{r unbiased_pdf, echo = FALSE, dev = "svg"}
ggplot(tmp, aes(x, y)) +
  geom_polygon(fill = red_pink, alpha = 0.9) +
  geom_hline(yintercept = 0, colour = "black") +
  geom_vline(xintercept = 0, linetype = "dashed", linewidth = 1) +
  scale_x_continuous(breaks = 0, labels = expression(beta)) +  # <- cambio clave
  theme_simple +
  theme(axis.text.x = element_text(size = 40))
```


**Estimador Sesagado:** $\mathop{\boldsymbol{E}}\left[ \hat{\beta} \right] \neq \beta$

```{R, biased pdf, echo = F, dev = "svg"}
ggplot(data = tmp, aes(x, y)) +
geom_polygon(aes(x = x + 2), fill = "darkslategray", alpha = 0.9) +
geom_hline(yintercept = 0, color = "black") +
geom_vline(xintercept = 0, size = 1, linetype = "dashed") +
scale_x_continuous(breaks = 0, labels = expression(beta)) + 
theme_simple +
theme(axis.text.x = element_text(size = 40))
```

**Propiedad 2: Varianza**

Tambi√©n queremos que nuestras estimaciones **no var√≠en demasiado de una muestra a otra**. En otras palabras: **queremos un estimador que sea estable**, no que en cada muestra nos d√© un valor completamente distinto.

La **varianza** mide cu√°nta variaci√≥n hay en las estimaciones \( \hat{\beta} \) que obtenemos al repetir el experimento (cambiando la muestra, los errores, o ambos):

$$
\text{Var} \left( \hat{\beta} \right) = \mathbb{E} \left[ \left( \hat{\beta} - \mathbb{E}[\hat{\beta}] \right)^2 \right]
$$

Un estimador con **menor varianza** produce resultados m√°s consistentes entre muestras. Esto lo hace m√°s confiable, incluso si no es perfecto.

> üéØ *Queremos que nuestras estimaciones est√©n ‚Äúconcentradas‚Äù cerca del valor esperado, no dispersas como tiros al aire.*

Veamos un ejemplo visual de c√≥mo la varianza afecta a las distribuciones de los estimadores.

```{r, variance pdf, echo = FALSE, dev = "svg", fig.height = 5}
d4 <- tibble(x = seq(-7.5, 7.5, 0.01), y = dnorm(x, mean = 0, sd = 1)) %>%
  rbind(., tibble(x = seq(7.5, -7.5, -0.01), y = 0))
d5 <- tibble(x = seq(-7.5, 7.5, 0.01), y = dnorm(x, mean = 0, sd = 2)) %>%
  rbind(., tibble(x = seq(7.5, -7.5, -0.01), y = 0))
ggplot() +
  geom_polygon(data = d4, aes(x, y), fill = red_pink, alpha = 0.9) +
  geom_polygon(data = d5, aes(x, y), fill = "darkslategray", alpha = 0.8) +
  geom_hline(yintercept = 0, color = "black") +
  geom_vline(xintercept = 0, size = 1, linetype = "dashed") +
  scale_x_continuous(breaks = 0, labels = expression(beta)) + 
  theme_simple +
  theme(axis.text.x = element_text(size = 20))
```

La curva rosada representa un estimador con baja varianza: la mayor√≠a de los valores de \( \hat{\beta} \) est√°n cerca de \( \beta \). Mientras que la curva gris oscuro representa un estimador con alta varianza: sus valores est√°n m√°s dispersos. A igualdad de sesgo, preferimos el estimador que tenga menor varianza.

### üéØ El trade-off: sesgo vs. varianza {-}

Hasta ahora hablamos del sesgo y de la varianza por separado.   Pero muchas veces, mejorar uno implica empeorar el otro.  Esto se conoce como el ** trade-off entre sesgo y varianza**.

> ¬øDeber√≠amos aceptar un poco de sesgo si eso nos permite reducir mucho la varianza?

En econometr√≠a, solemos preferir estimadores **insesgados** (o al menos **consistentes**), porque valoramos la interpretaci√≥n causal y te√≥rica de los par√°metros.   Pero en otras disciplinas, como el aprendizaje autom√°tico o la predicci√≥n estad√≠stica, es com√∫n aceptar un peque√±o sesgo si con ello se logra una gran reducci√≥n en la varianza y, en consecuencia, una mejor predicci√≥n promedio.

ve√°mos esta idea:

```{r, variance bias, echo = FALSE, dev = "svg"}
d4 <- tibble(x = seq(-7.5, 7.5, 0.01), y = dnorm(x, mean = 0.3, sd = 1)) %>%
  rbind(., tibble(x = seq(7.5, -7.5, -0.01), y = 0))
d5 <- tibble(x = seq(-7.5, 7.5, 0.01), y = dnorm(x, mean = 0, sd = 2)) %>%
  rbind(., tibble(x = seq(7.5, -7.5, -0.01), y = 0))
ggplot() +
  geom_polygon(data = d4, aes(x, y), fill = red_pink, alpha = 0.9) +
  geom_polygon(data = d5, aes(x, y), fill = "darkslategray", alpha = 0.8) +
  geom_hline(yintercept = 0, color = "black") +
  geom_vline(xintercept = 0, size = 1, linetype = "dashed") +
  scale_x_continuous(breaks = 0, labels = expression(beta)) + 
  theme_simple +
  theme(axis.text.x = element_text(size = 20))
```

### Propiedad 3: Consistencia {-}

La **consistencia** es una propiedad clave que nos dice qu√© pasa con el estimador cuando la muestra es cada vez m√°s grande.

> Intuitivamente, un estimador es **consistente** si, al aumentar el tama√±o de la muestra, sus valores se acercan cada vez m√°s al valor verdadero del par√°metro \( \beta \).

Esto nos da confianza de que, con datos suficientes, estaremos muy cerca del valor correcto.

Formalmente, un estimador \( \hat{\beta} \) es consistente si:

$$
\hat{\beta} \xrightarrow{p} \beta \quad \text{cuando } n \to \infty
$$

Esto se lee como: ‚Äú\( \hat{\beta} \) converge en probabilidad a \( \beta \)‚Äù.  
Es decir, **la probabilidad de que \( \hat{\beta} \) se aleje mucho de \( \beta \) se hace cada vez m√°s peque√±a a medida que usamos muestras m√°s grandes.**

---

Ve√°mos lo que ocurre cuando la muestra crece:

```{r, consistency pdf, echo = FALSE, dev = "svg"}
d6 <- tibble(x = seq(-7.5, 7.5, 0.01), y = dnorm(x, mean = 0, sd = 3)) %>%
  rbind(., tibble(x = seq(7.5, -7.5, -0.01), y = 0))
d7 <- tibble(x = seq(-7.5, 7.5, 0.01), y = dnorm(x, mean = 0, sd = 2)) %>%
  rbind(., tibble(x = seq(7.5, -7.5, -0.01), y = 0))
d8 <- tibble(x = seq(-7.5, 7.5, 0.01), y = dnorm(x, mean = 0, sd = 1)) %>%
  rbind(., tibble(x = seq(7.5, -7.5, -0.01), y = 0))
ggplot() +
  geom_polygon(data = d6, aes(x, y), fill = "orange", alpha = 0.7) +
  geom_polygon(data = d7, aes(x, y), fill = "darkslategray", alpha = 0.8) +
  geom_polygon(data = d8, aes(x, y), fill = red_pink, alpha = 0.9) +
  geom_hline(yintercept = 0, color = "black") +
  geom_vline(xintercept = 0, size = 1, linetype = "dashed") +
  scale_x_continuous(breaks = 0, labels = expression(beta)) + 
  theme_simple +
  theme(axis.text.x = element_text(size = 20))
```

La curva naranja representa una estimaci√≥n con mucha incertidumbre (muestra peque√±a). La curva gris oscura representa una muestra de tama√±o mediano. Mientras que la curva rosada muestra c√≥mo la estimaci√≥n se concentra alrededor de \( \beta \) con una muestra grande.

üéØ Un estimador consistente se ‚Äúafina‚Äù con m√°s datos: no solo mejora su varianza, sino que tiende a decir la verdad.

### Propiedad 4: Eficiencia {-}

La **eficiencia** combina las ideas de sesgo y varianza.

> Entre todos los estimadores **insesgados**, el m√°s eficiente es aquel que tiene la **menor varianza** posible.  
> Es decir, si dos estimadores son igual de ‚Äúcorrectos en promedio‚Äù, preferimos el que sea m√°s **estable**.

La eficiencia no se refiere a un √∫nico estimador, sino a una **comparaci√≥n entre estimadores**.

---

Formalmente, un estimador \( \hat{\beta} \) es eficiente si:

$$
\text{Var}(\hat{\beta}) \leq \text{Var}(\hat{\beta}')
$$

para cualquier otro estimador \( \hat{\beta}' \) que tambi√©n sea insesgado.

> Esto significa que **ning√∫n otro estimador insesgado** tiene una varianza menor que \( \hat{\beta} \).

---

üí° En el contexto de m√≠nimos cuadrados ordinarios (MCO), cuando se cumplen ciertos supuestos (los del teorema de Gauss-Markov), el estimador \( \hat{\beta}_{\text{MCO}} \) es el **Mejor Estimador Lineal Insesgado**, tambi√©n conocido como **MELI**:

> ‚úîÔ∏è Mejor ‚Üí tiene la menor varianza  
> ‚úîÔ∏è Estimador Lineal ‚Üí combinaci√≥n lineal de los datos  
> ‚úîÔ∏è Insesgado ‚Üí \( \mathbb{E}[\hat{\beta}] = \beta \)

---

> üéì En resumen, un estimador eficiente **es tan preciso como permite la informaci√≥n disponible en los datos**, sin sacrificar insesgamiento.


### Resumen de las propiedades {-}
| Propiedad     | Descripci√≥n                                                                                   |
|---------------|-----------------------------------------------------------------------------------------------|
| Insesgamiento  | El estimador no se aleja sistem√°ticamente del valor verdadero.                                 |
| Varianza      | El estimador tiene poca variaci√≥n entre muestras.                                             |
| Consistencia  | A medida que aumenta el tama√±o de la muestra, el estimador converge al valor verdadero.       |
| Eficiencia    | El estimador tiene la menor varianza posible entre todos los estimadores insesgados.         |
---

### üß† Nota de cierre: c√≥mo interpretar cada propiedad {-}

Cada propiedad que vimos tiene un enfoque ligeramente distinto sobre c√≥mo pensar la incertidumbre:

- **Sesgo**: ¬øEn promedio (tras repetir el experimento), el estimador acierta?
- **Varianza**: ¬øQu√© tanto cambia el estimador de una muestra a otra?
- **Consistencia**: ¬øEl estimador se acerca al valor verdadero si usamos una muestra m√°s grande del mismo experimento?
- **Eficiencia**: ¬øEste estimador es mejor (m√°s preciso) que otros estimadores insesgados disponibles?

> üîÅ Las primeras dos propiedades (sesgo y varianza) se entienden a trav√©s de *repeticiones hipot√©ticas* del experimento.  
> üìà La consistencia se analiza observando lo que ocurre cuando crece el tama√±o muestral.  
> ‚öñÔ∏è La eficiencia es una comparaci√≥n entre estimadores, **dado que todos sean insesgados**.

Estas ideas son fundamentales para entender c√≥mo evaluar y justificar un estimador en econometr√≠a.

---

## üìò Preguntas de repaso {-}

1. Verdadero o falso

- **(V/F)** Un estimador puede ser insesgado pero tener alta varianza.
- **(V/F)** La consistencia se refiere a repetir el experimento muchas veces.
- **(V/F)** Un estimador eficiente siempre es consistente.
- **(V/F)** Si un estimador es insesgado y eficiente, no puede ser mejorado bajo los supuestos del modelo.

2. Selecci√≥n m√∫ltiple {-}

**¬øCu√°l de las siguientes afirmaciones es correcta respecto a la eficiencia?**

A. Es una propiedad absoluta de un estimador.  
B. Se refiere a qu√© tan cerca est√° \( \hat{\beta} \) del promedio de los datos.  
C. Compara la varianza entre estimadores insesgados.  
D. Es sin√≥nimo de consistencia.  

**¬øQu√© pasa con un estimador consistente cuando el tama√±o muestral crece?**

A. Se vuelve insesgado autom√°ticamente.  
B. Se aleja del valor verdadero.  
C. Su varianza se hace infinita.  
D. Se aproxima al valor verdadero con alta probabilidad.  

3. Respuesta abierta

1. Explica con tus palabras qu√© significa que un estimador sea insesgado. ¬øPor qu√© esta propiedad es importante en econometr√≠a?**

2. ¬øPor qu√© puede ser √∫til, en algunos contextos, aceptar un estimador sesgado? Da un ejemplo donde podr√≠a ser preferible.**

3. ¬øEn qu√© se diferencia el concepto de varianza del de eficiencia? ¬øPueden dos estimadores tener la misma varianza pero distinta eficiencia?**

4. Sup√≥n que tienes dos estimadores:   - A es insesgado pero tiene alta varianza.   - B tiene un peque√±o sesgo pero varianza muy baja.  ¬øCu√°l elegir√≠as para un problema donde la prioridad es predecir bien el valor de \( y \)?  ¬øCambiar√≠a tu respuesta si el objetivo fuera estimar un efecto causal? Justifica tu elecci√≥n.