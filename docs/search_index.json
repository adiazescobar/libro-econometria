[["index.html", "Ãlgebra Matricial para EconometrÃ­a CapÃ­tulo 2: RegresiÃ³n lineal 1 InformaciÃ³n general DescripciÃ³n del curso Material bibliogrÃ¡fico EvaluaciÃ³n Programa semanal Recursos adicionales InclusiÃ³n Integridad acadÃ©mica", " Ãlgebra Matricial para EconometrÃ­a CapÃ­tulo 2: RegresiÃ³n lineal Ana MarÃ­a DÃ­az 15 July 2025 1 InformaciÃ³n general Campo Detalle Curso EconometrÃ­a Avanzada (CodÂ 1420) Docente AnaÂ MarÃ­aÂ DÃ­az Oficina / AtenciÃ³n SÃ©ptimo piso EdificioÂ 20Â | LunesÂ 9â€“11Â a.m. (o por Teams) Sitio web http://adiazescobar.com Correo a.diaze@javeriana.edu.co Prerequisito EconometrÃ­aÂ I Horario de clase MartesÂ yÂ JuevesÂ 11:00â€“13:00Â | SalonesÂ 67â€‘208 yÂ 67â€‘314 Monitor MiguelÂ ÃngelÂ CortÃ©sÂ â€” horarios y oficina por definir DescripciÃ³n del curso El objetivo principal es proporcionar herramientas para el anÃ¡lisis economÃ©trico de datos de corte transversal, series de tiempo y panel. Se revisa el modelo clÃ¡sico de regresiÃ³n lineal, las consecuencias de violar sus supuestos, modelos para variables dependientes discretas o limitadas y tÃ©cnicas bÃ¡sicas de series de tiempo y panel. Al finalizar, el estudiante podrÃ¡ ejecutar regresiones mÃºltiples, diagnosticar problemas comunes y aplicar soluciones apropiadas. Material bibliogrÃ¡fico Libro obligatorio Verbeek, MarnoÂ (2004). A Guide to Modern Econometrics. Wiley. Libros recomendados Greene, WilliamÂ (2003). Econometric Analysis. Prentice Hall. Wooldridge, JeffreyÂ (2003). Introductory Econometrics: A Modern Approach. Thomson. Wooldridge, JeffreyÂ (2002). Econometric Analysis of Cross Section and Panel Data. MITÂ Press. Montenegro, ÃlvaroÂ (2009). Series de Tiempo. Javegraf, PUJ. Stock, J.Â &amp;Â Watson, M.Â (2006). Introduction to Econometrics. Addisonâ€‘Wesley. Hayashi, FumioÂ (2000). Econometrics. Princeton UP. Angrist, J.D.Â &amp;Â Pischke, J.S.Â (2009). Mostly Harmless Econometrics. Princeton UP. Cameron, A.C.Â &amp;Â Trivedi, P.K.Â (2009). Microeconometrics Using Stata. StataÂ Press. StataÂ 11 Time Series Reference Manual. StataÂ Press. RosalesÂ R.Â etÂ al.Â (2010). Fundamentos de EconometrÃ­a Intermedia. CEDE. EvaluaciÃ³n Porcentaje Actividad 25Â % ParcialÂ 1 teÃ³rico 25Â % ParcialÂ 2 teÃ³rico 3Â % Talleres en clase 7Â % MonitorÃ­as 15Â % Trabajo final 25Â % Examen final +0.5Â (para el mejor) Videoâ€‘bono examen final Los exÃ¡menes son con libro cerrado y sin dispositivos electrÃ³nicos. El incumplimiento se sanciona segÃºn el reglamento de integridad acadÃ©mica. Programa semanal Semana Tema principal Lecturas clave 1 Supuestos del MCRL VerbeekÂ cap.Â 1â€‘2 (oblig.) Â | HayashiÂ cap.Â 1; WooldridgeÂ cap.Â 1â€‘2 (opc.) 2 RegresiÃ³n simple vs mÃºltiple; Teorema FWL VerbeekÂ cap.Â 1â€‘2 Â | The Stata JournalÂ (2013)Â 13(1):Â 92â€‘106 3 Propiedades de MCO en muestras finitas; Teorema Gaussâ€‘Markov VerbeekÂ cap.Â 2 4 Inferencia y predicciÃ³n; Propiedades asintÃ³ticas de MCO VerbeekÂ cap.Â 3 5 Primer parcial â€” 6 No linealidad; Multicolinealidad VerbeekÂ cap.Â 3â€‘4 7 Heterocedasticidad VerbeekÂ cap.Â 4 8 Endogeneidad: simultaneidad, omitidas, mediciÃ³n VerbeekÂ cap.Â 5 9 Variables instrumentales, MCO2E, GMM VerbeekÂ cap.Â 5 10 Modelos LPM, logit y probit VerbeekÂ cap.Â 7 11 MÃ¡ximo verosimilitud; DID, RD, duraciÃ³n, cuantÃ­lica (opc.) â€” 12 Semana Santa / Receso â€” 13 Segundo parcial â€” 14 Series de tiempo: conceptos bÃ¡sicos VerbeekÂ cap.Â 8 15 AR, MA y VAR estacionarios VerbeekÂ cap.Â 9 16 Datos de panel: pooled, between, FE, RE VerbeekÂ cap.Â 10 17 Examen final â€” Recursos adicionales BenÂ Lambert â€“ Econometrics on YouTube Mastering Econometrics (MRU) AEA Journal of Economic Perspectives â€“ Classroom Google Dataset Search Stata Cheat Sheets Seeing Theory â€“ Visual Probability InclusiÃ³n Este curso da la bienvenida a personas de todas las edades, orÃ­genes, creencias, etnias, gÃ©neros, identidades, orientaciones sexuales y capacidades. Se espera un ambiente respetuoso e inclusivo. Integridad acadÃ©mica La Universidad Javeriana fomenta la honestidad y establece sanciones por fraude o plagio segÃºn el reglamento de estudiantes. Cualquier uso no autorizado de materiales durante evaluaciones se considera falta grave. "],["repaso.html", "2 Repaso Construimos una poblaciÃ³n de juguete Â¿Y si mi muestra es mala? Â¿Y si mantengo fija la muestra? ğŸ“˜ Preguntas de repaso", " 2 Repaso Â¿QuÃ© estudia la econometrÃ­a? La econometrÃ­a es la herramienta que usamos para entender el mundo usando datos. Nos ayuda a responder preguntas como: Â¿cuÃ¡nto gana una persona segÃºn su nivel educativo? Â¿CÃ³mo influye la experiencia laboral en el salario? Â¿CuÃ¡l es el impacto de una polÃ­tica pÃºblica sobre el empleo? Pero aquÃ­ hay un reto importante: casi nunca podemos observar a toda la poblaciÃ³n. En vez de eso, trabajamos con una muestra. Usamos esta muestra para hacer inferencias sobre cÃ³mo funciona el mundo real, ese que no podemos ver completamente. En este capÃ­tulo vamos a entender, paso a paso, por quÃ© eso genera incertidumbre â€”y por quÃ© esa incertidumbre es una parte inevitable (Â¡y valiosa!) del anÃ¡lisis economÃ©trico. El proceso generador de datos Supongamos que el salario de un individuo, \\(y_i\\), depende de forma lineal de su nivel educativo, \\(x_i\\): \\[ y_i \\;=\\; \\beta_0 \\;+\\; \\beta_1\\,x_i \\;+\\; u_i, \\] donde \\(u_i\\) recoge todo lo que no observamos (habilidad, contactos, suerteâ€¦). A esta ecuaciÃ³n la llamaremos Proceso Generador de Datos (PGD) o modelo poblacional. El problema es que no podemos observar \\(u_i\\) porque es un tÃ©rmino de error. Ni tenemos acceso a todos los individuos de la poblaciÃ³n. Â¿QuÃ© hacemos entonces? En la prÃ¡ctica, tomamos una muestra aleatoria de individuos y observamos sus salarios y aÃ±os de educaciÃ³n \\((y_i,\\,x_i)\\), el termino de error \\(u_i\\) permanece oculto. Con una muestra aleatoria de tamaÃ±o \\(n\\), estimamos los parÃ¡metros \\(\\beta_0\\) y \\(\\beta_1\\) de la siguiente manera: \\[ y_i \\;=\\; \\hat{\\beta}_0 + \\hat{\\beta}_1\\,x_i + e_i, \\qquad \\hat{y}_i \\;=\\; \\hat{\\beta}_0 + \\hat{\\beta}_1\\,x_i, \\] Donde \\(\\hat{\\beta}_0\\) y \\(\\hat{\\beta}_1\\) son los estimadores de los parÃ¡metros poblacionales \\(\\beta_0\\) y \\(\\beta_1\\), y \\(e_i\\) es el tÃ©rmino de error muestral. A la recta que obtenemos se le llama modelo muestral. La direfencia entre este modelo y el PGD es precisamente lo que genera incertidumbre en nuestras estimaciones. Es decir que tenemos dos fuentes de incertidumbre, la muestra que compone nuestros datos y el tÃ©rmino de error \\(u_i\\) que no podemos observar. Para entender todo esto mejor, vamos primero a enfocarnos en la muestra que tenemos y cÃ³mo podemos usarla para estimar el PGD. Luego veremos cÃ³mo la incertidumbre afecta nuestras estimaciones y por quÃ© es importante. Construimos una poblaciÃ³n de juguete Para ilustrar el proceso generador de datos y la incertidumbre, vamos a crear una poblaciÃ³n de juguete. Esta poblaciÃ³n serÃ¡ un conjunto de 100 individuos con caracterÃ­sticas especÃ­ficas. Luego tomaremos muestras aleatorias de esta poblaciÃ³n y realizaremos regresiones para ver cÃ³mo se comportan nuestras estimaciones en comparaciÃ³n con el PGD real. Vamos a crear un mundo ficticio con 100 individuos. A cada uno le asignamos: \\(x\\) (aÃ±os de educaciÃ³n) sigue una normal con mediaÂ 5 y desviaciÃ³nÂ 1.5. \\(y\\) depende linealmente de \\(x\\) con pendienteÂ 0.5 y un tÃ©rmino aleatorio \\(u\\sim N(0,1)\\). La relaciÃ³n verdadera en la poblaciÃ³n El modelo poblacional que usamos, es decir el PGD, es: \\(y = 3 + 0.5x\\): AsÃ­ que en promedio los salarios de los individuos aumentan en 0.5 por cada aÃ±o adicional de educaciÃ³n. Esta es la verdad de nuestra poblaciÃ³n simulada. Obtenemos que los coeficientes son muy similares a los que usamos para generar la poblaciÃ³n: \\[ y_i = 2.53 + 0.57 x_i + u_i \\] Esto significa que el modelo poblacional es: \\[ y_i = \\beta_0 + \\beta_1 x_i + u_i \\] Sin embargo, esa linea estÃ¡ fuera de nuestro alcance porque requerirÃ­a encuestar a todos los egresados. Podemos estimar la relaciÃ³n entre \\(y\\) y \\(x\\) en una muestra aleatoria de individuos. Comencemos tomando 30 graduados al azar de nuestro grupo de 100 individuos: Estimemos la relaciÃ³n que existe entre \\(y\\) y \\(x\\) en esta muestra de 30 individuos. En la siguiente grÃ¡fica, la lÃ­nea roja es el modelo poblacional y la lÃ­nea negra discontinua es el modelo muestral. Ahora encontramos unos coeficientes estimados que son diferentes a los del modelo poblacional: PGD Modelo Poblacional \\(y_i = 2.53 + 0.57 x_i + u_i\\) Modelo muestral \\(\\hat{y}_i = 2.36 + 0.61 x_i\\) Tomemos otros 30 individuos al azar de la poblaciÃ³n y veamos cÃ³mo se comporta la regresiÃ³n. Ahora encontramos los siguientes coeficientes estimados: PGD Modelo Poblacional \\(y_i = 2.53 + 0.57 x_i + u_i\\) Modelo muestral \\(\\hat{y}_i = 2.79 + 0.56 x_i\\) Podemos ver que los coeficientes estimados son diferentes a los del modelo poblacional y tambiÃ©n diferentes entre sÃ­. Esto es normal, porque cada muestra aleatoria puede dar lugar a diferentes estimaciones. Tomemos una tercera muestra aleatoria de 30 individuos y veamos cÃ³mo se comporta la regresiÃ³n. Ahora encontramos los siguientes coeficientes estimados: PGD Modelo Poblacional \\(y_i = 2.53 + 0.57 x_i + u_i\\) Modelo muestral \\(\\hat{y}_i = 3.21 + 0.45 x_i\\) Siguen siendo diferentes a los del modelo poblacional y tambiÃ©n diferentes entre sÃ­. A veces se parece mucho, a veces no tanto. La razÃ³n es simple; cada muestra incluye un conjunto diferente de personas y eso cambia los resultados. Ahora repitamos esto 10,000 veces. Este ejercicio se conoce como Ejercicio de Monte Carlo. Vamos a tomar 10,000 muestras aleatorias de 30 individuos de nuestra poblaciÃ³n y estimar los coeficientes de regresiÃ³n para cada muestra. Luego, graficaremos todas las lÃ­neas de regresiÃ³n obtenidas para ver cÃ³mo se distribuyen en relaciÃ³n con la lÃ­nea poblacional. Â¿Lo interesante? Aunque cada recta individual es distinta, en promedio todas convergen hacia la recta verdadera (la primera que estimamos). En resumen, en promedio las lÃ­neas de regresiÃ³n se ajustan muy bien a la lÃ­nea de la poblaciÃ³n. Sin embargo, las lÃ­neas individuales (muestras) pueden desviarse significativamente. Las diferencias entre las muestras individuales y la poblaciÃ³n generan incertidumbre para el econometrista. ğŸ‘‰ Este resultado es tranquilizador: aunque nuestras estimaciones varÃ­an de muestra a muestra, en promedio nos acercamos a la verdad. Esto es lo que se conoce como insesgamiento del estimador MCO. Eso implica que cuando estimamos los coeficientes de regresiÃ³n, no podemos estar seguros de que nuestros estimadores sean exactamente iguales a los parÃ¡metros poblacionales. En cambio, obtenemos estimaciones que son variables aleatorias. En otras palabras, \\(\\hat{\\beta}\\) en sÃ­ mismo es una variable aleatoria, dependiente de la muestra aleatoria. Cuando tomamos una muestra y realizamos una regresiÃ³n, no sabemos si es una muestra â€˜buenaâ€™ ( \\(\\hat{\\beta}\\) estÃ¡ cerca de \\(\\beta\\)) o una muestra â€˜malaâ€™ (nuestra muestra difiere significativamente de la poblaciÃ³n). Mantener un registro de esta incertidumbre es clave para el anÃ¡lisis economÃ©trico. Nos permite entender la precisiÃ³n de nuestras estimaciones y cÃ³mo podemos mejorar nuestro modelo. Â¿Y si mi muestra es mala? â“ Pregunta del lector: Â¿QuÃ© pasa si me toca una muestra mala? Â¿CÃ³mo lo sÃ©? Â¿Se puede hacer algo para reducir esa incertidumbre? Una muestra mala es una muestra que, por puro azar, no representa bien a la poblaciÃ³n. Esto puede pasar incluso si tomamos la muestra correctamente. En esos casos, los estimadores como \\(\\hat{\\beta}_1\\) pueden estar lejos de su valor verdadero \\(\\beta_1\\), y nuestras conclusiones podrÃ­an ser engaÃ±osas. Â¿CÃ³mo saber si me tocÃ³ una muestra mala? No podemos saberlo con certeza, porque no conocemos la verdad poblacional. Pero hay seÃ±ales que nos pueden alertar: Errores estÃ¡ndar grandes: indican mucha variabilidad en la estimaciÃ³n. Intervalos de confianza anchos: reflejan gran incertidumbre. Signos o tamaÃ±os inesperados en los coeficientes: pueden deberse a una muestra no representativa. Pruebas de diagnÃ³stico del modelo: pueden revelar si los supuestos no se cumplen (residuos no normales, heterocedasticidad, etc.). Â¿Se puede reducir la incertidumbre muestral? Â¡SÃ­! Estas son algunas estrategias comunes: Aumentar el tamaÃ±o de la muestra (\\(n\\)) Entre mÃ¡s observaciones, mÃ¡s cerca estarÃ¡ \\(\\hat{\\beta}\\) de \\(\\beta\\) (por la ley de los grandes nÃºmeros). Mejorar el diseÃ±o muestral Muestreos estratificados, por conglomerados o con pesos pueden hacer las estimaciones mÃ¡s precisas. Controlar por variables relevantes Incluir mÃ¡s covariables reduce la varianza al explicar mejor el comportamiento de \\(y\\). Usar estimadores eficientes o robustos Si hay heterocedasticidad, los errores estÃ¡ndar robustos o el uso de mÃ©todos como MCO ponderado pueden mejorar la precisiÃ³n. âœ… La importancia de la fuente de los datos Una forma muy eficaz de minimizar el riesgo de una muestra sesgada es usar datos de fuentes confiables y con buen diseÃ±o muestral. Por ejemplo, confiar en los datos del DANE en Colombia o de institutos nacionales de estadÃ­stica en otros paÃ­ses es una prÃ¡ctica fundamental. Estas instituciones diseÃ±an cuidadosamente sus encuestas (como la ECH, ENUT o ENDS) para asegurar que sean representativas de la poblaciÃ³n. Si el muestreo estÃ¡ bien hecho desde el inicio, el margen de error se reduce y nuestras inferencias serÃ¡n mucho mÃ¡s confiables. ğŸ’¡ Mensaje clave: La incertidumbre no es un error: es una caracterÃ­stica natural del trabajo con datos. Lo importante no es eliminarla, sino medirla bien, comunicarla con claridad y tenerla en cuenta al tomar decisiones. Â¿Y si mantengo fija la muestra? Hasta ahora nos enfocamos en la incertidumbre que surge por el azar de la muestra. Ahora exploramos otra fuente igual de importante: la variabilidad del tÃ©rmino de error , incluso si la muestra es fija. Hasta ahora vimos que la incertidumbre puede surgir del hecho de que trabajamos con una muestra: cada subconjunto aleatorio de la poblaciÃ³n genera estimadores ligeramente diferentes. Pero hay una segunda fuente de incertidumbre: el tÃ©rmino de error \\(u_i\\). Recordemos el modelo: \\[ y_i = \\beta_0 + \\beta_1 x_i + u_i \\] Aunque tuviÃ©ramos toda la poblaciÃ³n (o una muestra perfecta), no podrÃ­amos predecir perfectamente \\(y_i\\) porque el valor de \\(u_i\\) sigue siendo desconocido. El termino de error \\(u_i\\) representa todo lo que influye en \\(y_i\\) pero no estÃ¡ capturado por \\(x_i\\). Por ejemplo, en un modelo donde \\(y_i\\) es el salario y \\(x_i\\) es la educaciÃ³n: Habilidades intrÃ­nsecas Redes de contacto Experiencia laboral previa Suerte (Â¡sÃ­, tambiÃ©n cuenta!) Todo eso se concentra en \\(u_i\\), que es no observable, pero no irrelevante. Incluso si estimamos \\(\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i\\) con una muestra fija: Nuestra recta de regresiÃ³n capta la tendencia promedio. Pero los valores observados de \\(y_i\\) se dispersan alrededor de la recta por culpa de \\(u_i\\). Esto se ve asÃ­: Al repetir el proceso de muestreo muchas veces, cada muestra tendrÃ¡ su propia recta de regresiÃ³n, pero todas estarÃ¡n dispersas alrededor de la recta poblacional. Vamos a hacer lo siguiente: Tomamos una sola muestra fija de 30 individuos de la poblaciÃ³n. Mantenemos sus valores de \\(x_i\\) constantes. Re-generamos el tÃ©rmino de error \\(u_i \\sim N(0,1)\\) varias veces. Calculamos nuevos valores de \\(y_i = 3 + 0.5x_i + u_i\\) en cada iteraciÃ³n. Estimamos una regresiÃ³n para cada muestra simulada. Esto nos permite ver cÃ³mo las estimaciones varÃ­an Ãºnicamente por culpa del tÃ©rmino de error, manteniendo fija la muestra. AquÃ­ vemos que, aunque la muestra es fija, las rectas de regresiÃ³n se dispersan alrededor de la recta poblacional. Esto es porque el tÃ©rmino de error \\(u_i\\) introduce variabilidad en los valores de \\(y_i\\). â˜ï¸ Una aclaraciÃ³n importante sobre el insesgamiento Al repetir el proceso de muestreo muchas veces, cada muestra tendrÃ¡ su propia recta de regresiÃ³n. Pero ojo: aunque esas rectas se vean â€œalrededorâ€ de la recta poblacional en nuestras simulaciones, esto no siempre ocurre en la vida real. El hecho de que las estimaciones se agrupen alrededor de los verdaderos valores poblacionales depende de que se cumplan los supeustos de modelo de regresiÃ³n lineal. Por ejemplo: Si el tÃ©rmino de error \\(u_i\\) estÃ¡ correlacionado con \\(x_i\\) (por ejemplo, porque omitimos una variable relevante), entonces nuestro estimador de \\(\\beta_1\\) estarÃ¡ sesgado. Si hay errores de mediciÃ³n, mala especificaciÃ³n del modelo o selecciÃ³n no aleatoria, tambiÃ©n se viola el insesgamiento. ğŸ¯ Â¿CuÃ¡ndo es cierto que nuestras estimaciones â€œse agrupanâ€ alrededor del verdadero \\(\\beta_1\\)? Cuando se cumplen los supuestos del modelo clÃ¡sico de regresiÃ³n lineal, en particular exogeneidad estricta o independencia del tÃ©rmino de error \\(u_i\\) respecto a las variables explicativas \\(x_i\\). En algunos libros se conoce como esperanza condicional igual cero: \\[ \\mathbb{E}[u_i \\mid x_i] = 0 \\] ğŸ” Si este supuestos se cumplen, entonces nuestro estimador de MÃ­nimos Cuadrados Ordinarios (MCO) es insesgado: \\[ \\mathbb{E}[\\hat{\\beta}_1] = \\beta_1 \\] Es decir, si repitiÃ©ramos el experimento de muestreo muchas veces, el promedio de nuestras estimaciones convergerÃ­a al verdadero valor poblacional. Pero si no se cumplen, como veremos mÃ¡s adelante, podemos tener: Estimadores sesgados Estimadores inconsistentes Intervalos de confianza y pruebas de hipÃ³tesis invÃ¡lidos ğŸ’¬ Entonces, Â¿las simulaciones que hicimos son â€œrealistasâ€? SÃ­â€¦ bajo los supuestos del modelo clÃ¡sico. En nuestras simulaciones controlamos todo: sabemos exactamente cÃ³mo se genera \\(y_i\\), y aseguramos que \\(u_i\\) sea independiente de \\(x_i\\). Por eso nuestras estimaciones tienden a agruparse cerca de la recta poblacional. Pero en el mundo real, los datos no vienen con etiqueta de â€œsupuestos cumplidosâ€. Por eso uno de los grandes desafÃ­os del anÃ¡lisis economÃ©trico es diagnosticar y justificar si los supuestos se cumplen. Y si no se cumplen, buscar soluciones: variables instrumentales, variables omitidas, diseÃ±os cuasiexperimentales, diseÃ±os experimentales, etc. ğŸ’¡ ConclusiÃ³n clave: El tÃ©rmino de error no desaparece, incluso cuando tenemos una muestra grande o bien diseÃ±ada. Por eso, cualquier estimaciÃ³n puntual (\\(\\hat{\\beta}_1\\)) debe ir acompaÃ±ada de una medida de incertidumbre, como el error estÃ¡ndar o un intervalo de confianza. Esto nos prepara para el siguiente paso: la inferencia estadÃ­stica. ğŸ“˜ Preguntas de repaso Â¿QuÃ© diferencia hay entre el modelo poblacional y el modelo muestral? Â¿CuÃ¡l de los dos observamos y cuÃ¡l inferimos? Â¿Por quÃ© decimos que el tÃ©rmino de error \\(u_i\\) es una fuente de incertidumbre, incluso si la muestra estÃ¡ fija? Â¿QuÃ© condiciones deben cumplirse para que el estimador de MÃ­nimos Cuadrados Ordinarios (MCO) sea insesgado? Â¿QuÃ© significa que un estimador sea insesgado â€œen promedioâ€? Â¿Eso garantiza que cualquier muestra nos darÃ¡ un buen resultado? Â¿QuÃ© implicaciones tiene usar una muestra mal diseÃ±ada o no representativa? En la simulaciÃ³n Monte Carlo, Â¿por quÃ© las rectas de regresiÃ³n estimadas con diferentes muestras se agrupan alrededor de la recta poblacional? Â¿QuÃ© observas cuando repetimos la estimaciÃ³n con una misma muestra, pero re-generamos el tÃ©rmino de error? Â¿QuÃ© se mantiene constante y quÃ© varÃ­a? En tus propias palabras, Â¿por quÃ© no podemos predecir perfectamente \\(y_i\\) aunque conozcamos bien \\(x_i\\)? Â¿Por quÃ© se llama â€œMonte Carloâ€ a este mÃ©todo de simulaciÃ³n? Â¿QuÃ© relaciÃ³n tiene con el azar? Â¿QuÃ© riesgos implica asumir que los supuestos del modelo clÃ¡sico se cumplen cuando en realidad no lo hacen? Â¿QuÃ© estrategias puedes usar si sospechas que \\(u_i\\) estÃ¡ correlacionado con \\(x_i\\)? Menciona al menos dos. Â¿Crees que todas las fuentes oficiales de datos (como el DANE) garantizan muestras perfectamente representativas? Â¿QuÃ© condiciones lo permitirÃ­an? âœï¸ Opcional para prÃ¡ctica adicional: simula tu propia poblaciÃ³n de juguete con una relaciÃ³n negativa entre \\(x\\) y \\(y\\), y repite el ejercicio de Monte Carlo. Â¿QuÃ© cambia? Â¿QuÃ© se mantiene? # Paso 1: Crear muestra fija de x set.seed(123) n &lt;- 30 x &lt;- rnorm(n, mean = 5, sd = 1.5) # Paso 2: Definir nÃºmero de simulaciones B &lt;- 1000 # Paso 3: Simular y almacenar coeficientes library(tibble) library(purrr) library(broom) sim &lt;- map_dfr(1:B, function(i) { u &lt;- rnorm(n, 0, 1) y &lt;- 3 + 0.5 * x + u model &lt;- lm(y ~ x) tidy(model)[2, c(&quot;term&quot;, &quot;estimate&quot;)] }) # Paso 4: Visualizar distribuciÃ³n del estimador library(ggplot2) ggplot(sim, aes(x = estimate)) + geom_histogram(bins = 40, fill = &quot;steelblue&quot;, color = &quot;white&quot;) + geom_vline(xintercept = 0.5, color = &quot;red&quot;, linetype = &quot;dashed&quot;) + labs( title = &quot;DistribuciÃ³n de \\\\(\\\\hat{\\\\beta}_1\\\\) con muestra fija&quot;, x = &quot;Estimaciones de \\\\(\\\\hat{\\\\beta}_1\\\\)&quot;, y = &quot;Frecuencia&quot; ) clear all set seed 123 set obs 30 * Paso 1: Crear muestra fija de x gen x = rnormal(5, 1.5) * Paso 2: Guardar la muestra base tempfile base save `base&#39; * Paso 3: Preparar almacenamiento de resultados tempname resultados postfile `resultados&#39; b1 using results_sim.dta, replace * Paso 4: SimulaciÃ³n Monte Carlo local reps = 1000 forvalues i = 1/`reps&#39; { use `base&#39;, clear gen u = rnormal(0,1) gen y = 3 + 0.5*x + u regress y x post `resultados&#39; (_b[x]) } postclose `resultados&#39; * Paso 5: Usar y graficar los resultados use results_sim.dta, clear histogram b1, width(0.02) normal /// title(&quot;DistribuciÃ³n de coeficientes con muestra fija&quot;) /// xtitle(&quot;Estimaciones de _b[x]&quot;) ytitle(&quot;Frecuencia&quot;) "],["regresiÃ³n-lineal.html", "3 RegresiÃ³n lineal ğŸ¯ Objetivo del capÃ­tulo ğŸ” Â¿QuÃ© significa encontrar la â€œmejor lÃ­neaâ€? MCO ğŸ“Š Propiedades y supuestos ğŸ“˜ Preguntas de repaso", " 3 RegresiÃ³n lineal ğŸ¯ Objetivo del capÃ­tulo En este capitulo vamos a: 1. Entender quÃ© es una regresiÃ³n lineal y cÃ³mo se ve grÃ¡ficamente. 2. Aprender cÃ³mo se calcula la mejor lÃ­nia con mÃ­nimos cuadrados ordinarios (MCO) 3. Explorar quÃ© hace un buen estiamdor y cÃ³mo evaluarlo ğŸ” Â¿QuÃ© significa encontrar la â€œmejor lÃ­neaâ€? Antes de hablar de estimaciones, pensemos en cÃ³mo se generan los datos: Supondremos que hay un modelo poblacional o proceso generador de datos: \\[ y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i \\] \\(y_i\\): variable dependiente (lo que queremos explicar) \\(x_i\\): variable independiente \\(\\beta_0, \\beta_1\\): parÃ¡metros poblacionales \\(\\epsilon_i\\): tÃ©rmino de error: todo lo que afecta a \\(y_i\\) y no estÃ¡ en \\(x_i\\) El tÃ©rmino \\(\\epsilon_i\\) captura factores no observados, errores de mediciÃ³n, y variaciÃ³n aleatoria. Es fundamental porque incluso si tuviÃ©ramos los valores verdaderos de \\(\\beta_0\\) y \\(\\beta_1\\), seguirÃ­amos sin poder predecir perfectamente \\(y_i\\) debido a este componente. En la prÃ¡ctica, estimamos los parÃ¡metros a partir de una muestra. Esto nos da una versiÃ³n estimada del modelo: \\[ \\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i \\] Y calculamos los residuos (errores estimados): \\[ \\hat{\\epsilon}_i = y_i - \\hat{y}_i \\] Queremos encontrar la lÃ­nea que prediga \\(y_i\\) con la menor cantidad posible de errores. Eso significa minimizar: \\[ \\text{SRC} = \\sum_{i = 1}^{n} \\hat{\\epsilon}_i^2 \\] Esto se conoce como el criterio de mÃ­nimos cuadrados. ğŸ¨ Ilustremos esto con un ejemplo visual Creemos unos nuevos datos para ilustrar esto. La linea de regresiÃ³n es igual a \\(\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x\\) donde _0$ y \\(\\hat{\\beta}_1\\) son los parÃ¡metros estimados de la regresiÃ³n. En este caso, \\(\\hat{\\beta}_0 = 6\\) y \\(\\hat{\\beta}_1 = 0.2\\). Para cada una de las observaciones podemos encontrar el y estimado \\(\\hat{y}_i\\). En la siguiente figura, la lÃ­nea naranja representa la lÃ­nea de regresiÃ³n estimada. Para cada una de las observaciones podemos calcular los errores: \\(\\epsilon_i = y_i - \\hat{y}_i\\), como se observa en el siguiente grÃ¡fico. Ahora podemos probar con otras lineas y ver cÃ³mo se comportan los errores. En el siguiente grafico, la lÃ­nea de regresiÃ³n estimada es \\(\\hat{y} = 3 + 0.2 x\\). Es evidente que los errores estiamdos son mÃ¡s grandes que los errores estimados en el grÃ¡fico anterior. Probemos ahora con una lÃ­nea de regresiÃ³n estimada que no se ajusta a los datos, \\(\\hat{y} = 10 - 0.8 x\\). En este caso, los errores son aÃºn mÃ¡s grandes. Recuerda que SRC es igual a: \\(\\left(\\sum e_i^2\\right)\\): Errores mÃ¡s grandes reciben penalizaciones mÃ¡s grandes. La estimaciÃ³n de MCO es la combinaciÃ³n de \\(\\hat{\\beta}_0\\) y \\(\\hat{\\beta}_1\\) que minimiza la SRC MCO Formalmente En una regresiÃ³n lineal simple, el estimador de MCO proviene de escoger \\(\\hat{\\beta}_0\\) y \\(\\hat{\\beta}_1\\) que minimice la suma de residuos al cuadrado (SRC), i.e., \\[ \\min_{\\hat{\\beta}_0,\\, \\hat{\\beta}_1} \\text{SRC} \\] donde \\[ \\text{SRC} = \\sum_{i = 1}^{n} (y_i - \\hat{y}_i)^2 = \\sum_{i = 1}^{n} (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)^2 \\] El estimador de MCO es el valor de \\(\\hat{\\beta}_0\\) y \\(\\hat{\\beta}_1\\) que minimiza la SRC. pero nosotros sabemos que \\(\\text{SRC} = \\sum_i \\tilde{\\epsilon_i}^2\\). Now use the definitions of \\(\\tilde{\\epsilon_i}\\) and \\(\\hat{y}\\). \\[ \\begin{aligned} \\tilde{\\epsilon_i}^2 &amp;= \\left( y_i - \\hat{y}_i \\right)^2 = \\left( y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i \\right)^2 \\\\ &amp;= y_i^2 - 2 y_i \\hat{\\beta}_0 - 2 y_i \\hat{\\beta}_1 x_i + \\hat{\\beta}_0^2 + 2 \\hat{\\beta}_0 \\hat{\\beta}_1 x_i + \\hat{\\beta}_1^2 x_i^2 \\end{aligned} \\] Recordatorio: Minimizar una funciÃ³n multivariada requiere (1) que las primeras derivadas sean iguales a cero (las condiciones de primer orden) y (2) las condiciones de segundo orden (concavidad). Nos estamos acercando. Necesitamos minimizar la SRC. \\[ \\text{SRE} = \\sum_i \\tilde{e_i}^2 = \\sum_i \\left( y_i^2 - 2 y_i \\hat{\\beta}_0 - 2 y_i \\hat{\\beta}_1 x_i + \\hat{\\beta}_0^2 + 2 \\hat{\\beta}_0 \\hat{\\beta}_1 x_i + \\hat{\\beta}_1^2 x_i^2 \\right) \\] For the first-order conditions of minimization, we now take the first derivates of SSE with respect to \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\). \\[ \\begin{aligned} \\dfrac{\\partial \\text{SRC}}{\\partial \\hat{\\beta}_0} &amp;= \\sum_i \\left( 2 \\hat{\\beta}_0 + 2 \\hat{\\beta}_1 x_i - 2 y_i \\right) = 2n \\hat{\\beta}_0 + 2 \\hat{\\beta}_1 \\sum_i x_i - 2 \\sum_i y_i \\\\ &amp;= 2n \\hat{\\beta}_0 + 2n \\hat{\\beta}_1 \\overline{x} - 2n \\overline{y} \\end{aligned} \\] donde \\(\\overline{x} = \\frac{\\sum x_i}{n}\\) y \\(\\overline{y} = \\frac{\\sum y_i}{n}\\) son medias muestrales de \\(x\\) y \\(y\\) (de tamaÃ±o \\(n\\)). Las condiciones de primer orden establecen que las derivadas deben ser iguales a cero: \\[ \\dfrac{\\partial \\text{SSE}}{\\partial \\hat{\\beta}_0} = 2n \\hat{\\beta}_0 + 2n \\hat{\\beta}_1 \\overline{x} - 2n \\overline{y} = 0 \\] Lo que implica \\[ \\hat{\\beta}_0 = \\overline{y} - \\hat{\\beta}_1 \\overline{x} \\] Ahora para \\(\\hat{\\beta}_1\\). Tomemos la derivada de la SRC con respecto a \\(\\hat{\\beta}_1\\) \\[ \\begin{aligned} \\dfrac{\\partial \\text{SSE}}{\\partial \\hat{\\beta}_1} &amp;= \\sum_i \\left( 2 \\hat{\\beta}_0 x_i + 2 \\hat{\\beta}_1 x_i^2 - 2 y_i x_i \\right) = 2 \\hat{\\beta}_0 \\sum_i x_i + 2 \\hat{\\beta}_1 \\sum_i x_i^2 - 2 \\sum_i y_i x_i \\\\ &amp;= 2n \\hat{\\beta}_0 \\overline{x} + 2 \\hat{\\beta}_1 \\sum_i x_i^2 - 2 \\sum_i y_i x_i \\end{aligned} \\] Igualarlo a cero \\[ \\dfrac{\\partial \\text{SSE}}{\\partial \\hat{\\beta}_1} = 2n \\hat{\\beta}_0 \\overline{x} + 2 \\hat{\\beta}_1 \\sum_i x_i^2 - 2 \\sum_i y_i x_i = 0 \\] y reemplazarlo \\(\\hat{\\beta}_0\\), i.e., \\(\\hat{\\beta}_0 = \\overline{y} - \\hat{\\beta}_1 \\overline{x}\\). Thus, \\[ 2n \\left(\\overline{y} - \\hat{\\beta}_1 \\overline{x}\\right) \\overline{x} + 2 \\hat{\\beta}_1 \\sum_i x_i^2 - 2 \\sum_i y_i x_i = 0 \\] Continuando \\[ 2n \\left(\\overline{y} - \\hat{\\beta}_1 \\overline{x}\\right) \\overline{x} + 2 \\hat{\\beta}_1 \\sum_i x_i^2 - 2 \\sum_i y_i x_i = 0 \\] \\[ 2n \\overline{y}\\,\\overline{x} - 2n \\hat{\\beta}_1 \\overline{x}^2 + 2 \\hat{\\beta}_1 \\sum_i x_i^2 - 2 \\sum_i y_i x_i = 0 \\] \\[ \\implies 2 \\hat{\\beta}_1 \\left( \\sum_i x_i^2 - n \\overline{x}^2 \\right) = 2 \\sum_i y_i x_i - 2n \\overline{y}\\,\\overline{x} \\] \\[ \\implies \\hat{\\beta}_1 = \\dfrac{\\sum_i y_i x_i - 2n \\overline{y}\\,\\overline{x}}{\\sum_i x_i^2 - n \\overline{x}^2} = \\dfrac{\\sum_i (x_i - \\overline{x})(y_i - \\overline{y})}{\\sum_i (x_i - \\overline{x})^2} \\] LISTOO! Ahora tenemos nuestros lindos estimadores \\[ \\hat{\\beta}_1 = \\dfrac{\\sum_i (x_i - \\overline{x})(y_i - \\overline{y})}{\\sum_i (x_i - \\overline{x})^2} \\] and the intercept \\[ \\hat{\\beta}_0 = \\overline{y} - \\hat{\\beta}_1 \\overline{x} \\] Ya sabes de dÃ³nde proviene la parte de mÃ­nimos cuadrados en el tÃ©rmino â€œmÃ­nimos cuadrados ordinariosâ€. ğŸŠ Ahora pasamos a las propiedades (implÃ­citas) de los MÃ­nimos Cuadrados Ordinarios (MCO / OLS). ğŸ“Š Propiedades y supuestos Â¿QuÃ© hace a un buen estimador? Antes de hablar de propiedades del estimador de MCO, recordemos algunas herramientas fundamentales de estadÃ­stica. 3.0.1 ğŸ“ˆ Repaso: Funciones de densidad Las funciones de densidad de probabilidad (FDP, o PDF en inglÃ©s) describen la probabilidad de que una variable aleatoria continua tome valores dentro de un intervalo dado. La probabilidad total bajo la curva es 1. Ejemplo: para una variable normal estÃ¡ndar, la probabilidad de que tome un valor entre -2 y 0 es: \\[ \\mathop{\\text{P}}\\left(-2 \\leq X \\leq 0\\right) = 0.48 \\] Otro ejemplo clÃ¡sico es la probabilidad de que una variable aleatoria normal estÃ¡ndar tome un valor entre -1.96 y 1.96: \\(\\mathop{\\text{P}}\\left(-1.96 \\leq X \\leq 1.96\\right) = 0.95\\) O la probabilidad de que una variable aleatoria normal estÃ¡ndar tome un valor mayor a 2: \\(\\mathop{\\text{P}}\\left(X &gt; 2\\right) = 0.023\\) ğŸ¤” Â¿QuÃ© propiedades buscamos en un estimador? Imaginemos que intentamos estimar un parÃ¡metro verdadero \\(\\beta\\), y tenemos tres mÃ©todos distintos. Cada uno produce una distribuciÃ³n diferente para \\(\\hat{\\beta}\\). Pregunta: Â¿QuÃ© propiedades podrÃ­an ser importantes para un estimador? Propiedad 1. Insesgamiento Es decir, si repitiÃ©ramos el experimento muchas veces, Â¿el estimador tiende a acercarse al valor verdadero del parÃ¡metro que estamos tratando de estimar? El sesgo mide si el estimador se acerca al valor real en promedio: ğŸ§ª Â¿QuÃ© significa â€œrepetir el experimentoâ€? En este contexto, repetir el experimento puede entenderse de tres formas, todas vÃ¡lidas para pensar en la incertidumbre de un estimador: Cambiar la muestra: imaginar que tomamos muchas muestras aleatorias distintas de la poblaciÃ³n. Mantener fija la muestra, pero cambiar los errores: incluso si los valores de \\(x_i\\) no cambian, los valores de \\(y_i\\) pueden variar si asumimos que los errores \\(\\epsilon_i\\) son aleatorios. Recuerda que \\(y_i\\) sigue un proceso generador de datos subyacente. Cambiar ambos simultÃ¡neamente: es el caso mÃ¡s comÃºn en simulaciones â€” se sortean tanto los \\(x_i\\) como los \\(\\epsilon_i\\). En cualquiera de los tres escenarios, obtendrÃ­amos distintos valores de \\(\\hat{\\beta}\\). Eso nos permite construir una distribuciÃ³n muestral del estimador y analizar propiedades como el sesgo. âš ï¸ Importante: cuando hablamos de â€œrepetir el experimentoâ€, no queremos decir que volvamos a observar a las mismas personas varias veces con diferentes valores de \\(x\\) (por ejemplo, dÃ¡ndoles distintos aÃ±os de educaciÃ³n). Lo que estamos haciendo es imaginar escenarios hipotÃ©ticos en los que la muestra o los errores cambian, y ver cÃ³mo eso afecta al estimador. Estos experimentos no se pueden realizar en la realidad con una misma persona, pero sÃ­ los podemos simular por computadora o analizar teÃ³ricamente. MÃ¡s formalmente: Â¿La media de la distribuciÃ³n del estimador es igual al parÃ¡metro que estima? En promedio (despuÃ©s de muchas repeticiones), Â¿el estimador tiende hacia el valor correcto? MÃ¡s formalmente: Â¿La media de la distribuciÃ³n del estimador es igual al parÃ¡metro que estima? \\[ \\mathop{\\text{Sesgo}}_\\beta \\left( \\hat{\\beta} \\right) = \\mathop{\\boldsymbol{E}}\\left[ \\hat{\\beta} \\right] - \\beta \\] Estimador Insesagado: \\(\\mathop{\\boldsymbol{E}}\\left[ \\hat{\\beta} \\right] = \\beta\\) Estimador Sesagado: \\(\\mathop{\\boldsymbol{E}}\\left[ \\hat{\\beta} \\right] \\neq \\beta\\) Propiedad 2: Varianza TambiÃ©n queremos que nuestras estimaciones no varÃ­en demasiado de una muestra a otra. En otras palabras: queremos un estimador que sea estable, no que en cada muestra nos dÃ© un valor completamente distinto. La varianza mide cuÃ¡nta variaciÃ³n hay en las estimaciones \\(\\hat{\\beta}\\) que obtenemos al repetir el experimento (cambiando la muestra, los errores, o ambos): \\[ \\text{Var} \\left( \\hat{\\beta} \\right) = \\mathbb{E} \\left[ \\left( \\hat{\\beta} - \\mathbb{E}[\\hat{\\beta}] \\right)^2 \\right] \\] Un estimador con menor varianza produce resultados mÃ¡s consistentes entre muestras. Esto lo hace mÃ¡s confiable, incluso si no es perfecto. ğŸ¯ Queremos que nuestras estimaciones estÃ©n â€œconcentradasâ€ cerca del valor esperado, no dispersas como tiros al aire. Veamos un ejemplo visual de cÃ³mo la varianza afecta a las distribuciones de los estimadores. La curva rosada representa un estimador con baja varianza: la mayorÃ­a de los valores de \\(\\hat{\\beta}\\) estÃ¡n cerca de \\(\\beta\\). Mientras que la curva gris oscuro representa un estimador con alta varianza: sus valores estÃ¡n mÃ¡s dispersos. A igualdad de sesgo, preferimos el estimador que tenga menor varianza. ğŸ¯ El trade-off: sesgo vs.Â varianza Hasta ahora hablamos del sesgo y de la varianza por separado. Pero muchas veces, mejorar uno implica empeorar el otro. Esto se conoce como el ** trade-off entre sesgo y varianza**. Â¿DeberÃ­amos aceptar un poco de sesgo si eso nos permite reducir mucho la varianza? En econometrÃ­a, solemos preferir estimadores insesgados (o al menos consistentes), porque valoramos la interpretaciÃ³n causal y teÃ³rica de los parÃ¡metros. Pero en otras disciplinas, como el aprendizaje automÃ¡tico o la predicciÃ³n estadÃ­stica, es comÃºn aceptar un pequeÃ±o sesgo si con ello se logra una gran reducciÃ³n en la varianza y, en consecuencia, una mejor predicciÃ³n promedio. veÃ¡mos esta idea: Propiedad 3: Consistencia La consistencia es una propiedad clave que nos dice quÃ© pasa con el estimador cuando la muestra es cada vez mÃ¡s grande. Intuitivamente, un estimador es consistente si, al aumentar el tamaÃ±o de la muestra, sus valores se acercan cada vez mÃ¡s al valor verdadero del parÃ¡metro \\(\\beta\\). Esto nos da confianza de que, con datos suficientes, estaremos muy cerca del valor correcto. Formalmente, un estimador \\(\\hat{\\beta}\\) es consistente si: \\[ \\hat{\\beta} \\xrightarrow{p} \\beta \\quad \\text{cuando } n \\to \\infty \\] Esto se lee como: â€œ\\(\\hat{\\beta}\\) converge en probabilidad a \\(\\beta\\)â€. Es decir, la probabilidad de que \\(\\hat{\\beta}\\) se aleje mucho de \\(\\beta\\) se hace cada vez mÃ¡s pequeÃ±a a medida que usamos muestras mÃ¡s grandes. VeÃ¡mos lo que ocurre cuando la muestra crece: La curva naranja representa una estimaciÃ³n con mucha incertidumbre (muestra pequeÃ±a). La curva gris oscura representa una muestra de tamaÃ±o mediano. Mientras que la curva rosada muestra cÃ³mo la estimaciÃ³n se concentra alrededor de \\(\\beta\\) con una muestra grande. ğŸ¯ Un estimador consistente se â€œafinaâ€ con mÃ¡s datos: no solo mejora su varianza, sino que tiende a decir la verdad. Propiedad 4: Eficiencia La eficiencia combina las ideas de sesgo y varianza. Entre todos los estimadores insesgados, el mÃ¡s eficiente es aquel que tiene la menor varianza posible. Es decir, si dos estimadores son igual de â€œcorrectos en promedioâ€, preferimos el que sea mÃ¡s estable. La eficiencia no se refiere a un Ãºnico estimador, sino a una comparaciÃ³n entre estimadores. Formalmente, un estimador \\(\\hat{\\beta}\\) es eficiente si: \\[ \\text{Var}(\\hat{\\beta}) \\leq \\text{Var}(\\hat{\\beta}&#39;) \\] para cualquier otro estimador \\(\\hat{\\beta}&#39;\\) que tambiÃ©n sea insesgado. Esto significa que ningÃºn otro estimador insesgado tiene una varianza menor que \\(\\hat{\\beta}\\). ğŸ’¡ En el contexto de mÃ­nimos cuadrados ordinarios (MCO), cuando se cumplen ciertos supuestos (los del teorema de Gauss-Markov), el estimador \\(\\hat{\\beta}_{\\text{MCO}}\\) es el Mejor Estimador Lineal Insesgado, tambiÃ©n conocido como MELI: âœ”ï¸ Mejor â†’ tiene la menor varianza âœ”ï¸ Estimador Lineal â†’ combinaciÃ³n lineal de los datos âœ”ï¸ Insesgado â†’ \\(\\mathbb{E}[\\hat{\\beta}] = \\beta\\) ğŸ“ En resumen, un estimador eficiente es tan preciso como permite la informaciÃ³n disponible en los datos, sin sacrificar insesgamiento. Resumen de las propiedades Propiedad DescripciÃ³n Insesgamiento El estimador no se aleja sistemÃ¡ticamente del valor verdadero. Varianza El estimador tiene poca variaciÃ³n entre muestras. Consistencia A medida que aumenta el tamaÃ±o de la muestra, el estimador converge al valor verdadero. Eficiencia El estimador tiene la menor varianza posible entre todos los estimadores insesgados. ğŸ§  Nota de cierre: cÃ³mo interpretar cada propiedad Cada propiedad que vimos tiene un enfoque ligeramente distinto sobre cÃ³mo pensar la incertidumbre: Sesgo: Â¿En promedio (tras repetir el experimento), el estimador acierta? Varianza: Â¿QuÃ© tanto cambia el estimador de una muestra a otra? Consistencia: Â¿El estimador se acerca al valor verdadero si usamos una muestra mÃ¡s grande del mismo experimento? Eficiencia: Â¿Este estimador es mejor (mÃ¡s preciso) que otros estimadores insesgados disponibles? ğŸ” Las primeras dos propiedades (sesgo y varianza) se entienden a travÃ©s de repeticiones hipotÃ©ticas del experimento. ğŸ“ˆ La consistencia se analiza observando lo que ocurre cuando crece el tamaÃ±o muestral. âš–ï¸ La eficiencia es una comparaciÃ³n entre estimadores, dado que todos sean insesgados. Estas ideas son fundamentales para entender cÃ³mo evaluar y justificar un estimador en econometrÃ­a. ğŸ“˜ Preguntas de repaso Verdadero o falso (V/F) Un estimador puede ser insesgado pero tener alta varianza. (V/F) La consistencia se refiere a repetir el experimento muchas veces. (V/F) Un estimador eficiente siempre es consistente. (V/F) Si un estimador es insesgado y eficiente, no puede ser mejorado bajo los supuestos del modelo. SelecciÃ³n mÃºltiple {-} Â¿CuÃ¡l de las siguientes afirmaciones es correcta respecto a la eficiencia? A. Es una propiedad absoluta de un estimador. B. Se refiere a quÃ© tan cerca estÃ¡ \\(\\hat{\\beta}\\) del promedio de los datos. C. Compara la varianza entre estimadores insesgados. D. Es sinÃ³nimo de consistencia. Â¿QuÃ© pasa con un estimador consistente cuando el tamaÃ±o muestral crece? A. Se vuelve insesgado automÃ¡ticamente. B. Se aleja del valor verdadero. C. Su varianza se hace infinita. D. Se aproxima al valor verdadero con alta probabilidad. Respuesta abierta Explica con tus palabras quÃ© significa que un estimador sea insesgado. Â¿Por quÃ© esta propiedad es importante en econometrÃ­a?** Â¿Por quÃ© puede ser Ãºtil, en algunos contextos, aceptar un estimador sesgado? Da un ejemplo donde podrÃ­a ser preferible.** Â¿En quÃ© se diferencia el concepto de varianza del de eficiencia? Â¿Pueden dos estimadores tener la misma varianza pero distinta eficiencia?** SupÃ³n que tienes dos estimadores: - A es insesgado pero tiene alta varianza. - B tiene un pequeÃ±o sesgo pero varianza muy baja. Â¿CuÃ¡l elegirÃ­as para un problema donde la prioridad es predecir bien el valor de \\(y\\)? Â¿CambiarÃ­a tu respuesta si el objetivo fuera estimar un efecto causal? Justifica tu elecciÃ³n. "],["Ã¡lgebra-matricial-para-econometrÃ­a.html", "4 Ãlgebra Matricial para EconometrÃ­a Matrices Vectores Operaciones con matrices Determinantes Matriz inversa Rango de una matriz Sistemas de ecuaciones lineales Matrices cuadradas especiales Derivadas de una funciÃ³n multidimensional ğŸ“˜ Preguntas de repaso", " 4 Ãlgebra Matricial para EconometrÃ­a Antes de introducir los supuestos fundamentales del modelo de regresiÃ³n lineal, es importante repasar algunos conceptos clave del Ã¡lgebra matricial. Este lenguaje permite expresar de forma compacta y elegante muchos de los resultados economÃ©tricos, facilitando la comprensiÃ³n de los modelos lineales y sus propiedades. Matrices Una matriz \\(A \\in \\mathbb{R}^{m \\times n}\\) es un conjunto de elementos \\(a_{ij}\\), donde \\(i = 1, \\ldots, m\\) (filas) y \\(j = 1, \\ldots, n\\) (columnas), organizados de la siguiente manera: \\[ A = \\begin{pmatrix} a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1n} \\\\ a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{m1} &amp; a_{m2} &amp; \\cdots &amp; a_{mn} \\end{pmatrix} \\] La dimensiÃ³n o orden de la matriz es \\(m \\times n\\), lo que indica que tiene \\(m\\) filas y \\(n\\) columnas. Cuando \\(m = n\\), se dice que la matriz es cuadrada; si \\(m \\neq n\\), es una matriz rectangular. Las matrices se representan con letras mayÃºsculas en negrita, como \\(\\mathbf{A}\\), y sus elementos con letras minÃºsculas con subÃ­ndices, como \\(a_{ij}\\). Los elementos de una matriz pueden ser nÃºmeros reales, \\(a_{ij} \\in \\mathbb{R}\\). Ejemplo. La matriz \\[ B = \\begin{pmatrix} 3 &amp; 1 &amp; 7 \\\\ 2 &amp; 4 &amp; 5 \\end{pmatrix} \\] es una matriz rectangular de orden \\(2 \\times 3\\). Tiene 2 filas y 3 columnas. El elemento en la fila 2 y columna 3 es \\(b_{23} = 5\\). Traspuesta de una matriz La traspuesta de una matriz \\(A = [a_{ij}]\\) de dimensiÃ³n \\(m \\times n\\) es otra matriz \\(A&#39; = [a_{ji}]\\) de dimensiÃ³n \\(n \\times m\\), obtenida al intercambiar filas por columnas. Es decir, la primera fila de \\(A\\) se convierte en la primera columna de \\(A&#39;\\), la segunda fila en la segunda columna, y asÃ­ sucesivamente. \\[ A&#39; = \\begin{pmatrix} a_{11} &amp; a_{21} &amp; \\cdots &amp; a_{m1} \\\\ a_{12} &amp; a_{22} &amp; \\cdots &amp; a_{m2} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{1n} &amp; a_{2n} &amp; \\cdots &amp; a_{mn} \\end{pmatrix} \\] Ejemplo. Sea la matriz \\[ A = \\begin{pmatrix} 6 &amp; 5 &amp; 7 &amp; 4 \\\\ 5 &amp; 4 &amp; 2 &amp; 5 \\\\ 1 &amp; 1 &amp; 11 &amp; 1 \\end{pmatrix} \\] que es de orden \\(3 \\times 4\\). Su traspuesta es: \\[ A&#39; = \\begin{pmatrix} 6 &amp; 5 &amp; 1 \\\\ 5 &amp; 4 &amp; 1 \\\\ 7 &amp; 2 &amp; 11 \\\\ 4 &amp; 5 &amp; 1 \\end{pmatrix} \\] Esta nueva matriz es de orden \\(4 \\times 3\\). Vectores Un vector columna es una matriz de orden \\(m \\times 1\\), es decir, una matriz que solo tiene una columna: \\[ \\mathbf{a} = \\begin{pmatrix} a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_m \\end{pmatrix} \\] Se denota con una letra minÃºscula en negrita y se puede escribir de forma abreviada como \\(\\mathbf{a} = [a_i]\\). Cada elemento \\(a_i\\) indica la posiciÃ³n del componente dentro del vector. Un vector fila, en cambio, es una matriz de orden \\(1 \\times m\\), es decir, solo tiene una fila: \\[ \\mathbf{a}&#39; = \\begin{pmatrix} a_1 &amp; a_2 &amp; \\cdots &amp; a_m \\end{pmatrix} \\] La traspuesta de un vector columna es un vector fila, y viceversa. En lÃ­nea, se escribe \\(\\mathbf{a} = (a_1, a_2, \\ldots, a_m)&#39;\\) para indicar que es columna, usando la notaciÃ³n de traspuesta. Ejemplo Sea el vector columna \\[ \\mathbf{v} = \\begin{pmatrix} 2 \\\\ -1 \\\\ 4 \\end{pmatrix} \\quad \\text{de orden } 3 \\times 1, \\quad \\text{su traspuesta es } \\mathbf{v}&#39; = \\begin{pmatrix} 2 &amp; -1 &amp; 4 \\end{pmatrix} \\] Producto escalar DefiniciÃ³n. Sean \\(\\mathbf{a} = (a_1, \\ldots, a_m)&#39;\\) y \\(\\mathbf{b} = (b_1, \\ldots, b_m)&#39;\\) dos vectores columna del mismo orden \\(m \\times 1\\), su producto escalar es: \\[ \\mathbf{a}&#39;\\mathbf{b} = \\sum_{i=1}^m a_i b_i = a_1b_1 + a_2b_2 + \\cdots + a_m b_m \\] Ejemplo Si \\(\\mathbf{a} = (1, 2, 3)&#39;\\) y \\(\\mathbf{b} = (4, 5, 6)&#39;\\), entonces: \\[ \\mathbf{a}&#39;\\mathbf{b} = 1 \\cdot 4 + 2 \\cdot 5 + 3 \\cdot 6 = 4 + 10 + 18 = 32 \\] Norma y normalizaciÃ³n DefiniciÃ³n. La norma de un vector \\(\\mathbf{x}\\) se define como: \\[ \\|\\mathbf{x}\\| = \\sqrt{\\mathbf{x}&#39;\\mathbf{x}} = \\sqrt{x_1^2 + x_2^2 + \\cdots + x_m^2} \\] El vector normalizado es: \\[ \\frac{\\mathbf{x}}{\\|\\mathbf{x}\\|} \\] Ortogonalidad DefiniciÃ³n. Dos vectores \\(\\mathbf{a}\\) y \\(\\mathbf{b}\\) son ortogonales (se denota \\(\\mathbf{a} \\perp \\mathbf{b}\\)) si: \\[ \\mathbf{a}&#39;\\mathbf{b} = 0 \\] \\[ \\bar{y} = \\frac{\\mathbf{i}&#39;\\mathbf{y}}{\\mathbf{i}&#39;\\mathbf{i}} \\] Operaciones con matrices Igualdad de matrices Dos matrices \\(A = [a_{ij}]\\) y \\(B = [b_{ij}]\\) de igual orden \\(m \\times n\\) son iguales si: \\[ a_{ij} = b_{ij}, \\quad \\text{para todo } i = 1, \\ldots, m; \\; j = 1, \\ldots, n \\] Suma y resta de matrices La suma de dos matrices del mismo orden es la matriz \\(C = A + B = [c_{ij}]\\) donde: \\[ c_{ij} = a_{ij} + b_{ij} \\] Propiedades: Conmutativa: \\(A + B = B + A\\) Asociativa: \\((A + B) + C = A + (B + C)\\) Elemento neutro: \\(A + 0 = A\\) Opuesto: \\(A + (-A) = 0\\) Ejemplo: \\[ A = \\begin{pmatrix} 6 &amp; 5 &amp; 7 &amp; 4 \\\\ 5 &amp; 4 &amp; 2 &amp; 5 \\\\ 1 &amp; 1 &amp; 11 &amp; 1 \\end{pmatrix}, \\quad B = \\begin{pmatrix} 7 &amp; 11 &amp; 2 &amp; 9 \\\\ 5 &amp; 8 &amp; 8 &amp; 1 \\\\ 6 &amp; 10 &amp; 8 &amp; 10 \\end{pmatrix} \\] \\[ A + B = \\begin{pmatrix} 13 &amp; 16 &amp; 9 &amp; 13 \\\\ 10 &amp; 12 &amp; 10 &amp; 6 \\\\ 7 &amp; 11 &amp; 19 &amp; 11 \\end{pmatrix} \\] Resta: se define como \\(A - B = A + (-B)\\) MultiplicaciÃ³n por un escalar \\[ \\lambda A = [\\lambda a_{ij}] \\] Ejemplo: \\[ 2A = \\begin{pmatrix} 12 &amp; 10 &amp; 14 &amp; 8 \\\\ 10 &amp; 8 &amp; 4 &amp; 10 \\\\ 2 &amp; 2 &amp; 22 &amp; 2 \\end{pmatrix} \\] MultiplicaciÃ³n de matrices Sean \\(A \\in \\mathbb{R}^{m \\times n}\\) y \\(B \\in \\mathbb{R}^{n \\times p}\\), el producto \\(AB \\in \\mathbb{R}^{m \\times p}\\) se define por: \\[ c_{ij} = \\sum_{k=1}^{n} a_{ik} b_{kj} \\] Propiedades: Asociativa: \\((AB)C = A(BC)\\) Distributiva: \\(A(B + C) = AB + AC\\) No conmutativa: en general \\(AB \\neq BA\\) Ejemplo: \\[ A = \\begin{pmatrix} 6 &amp; 5 &amp; 7 &amp; 4 \\\\ 5 &amp; 4 &amp; 2 &amp; 5 \\\\ 1 &amp; 1 &amp; 11 &amp; 1 \\end{pmatrix}, \\quad B&#39; = \\begin{pmatrix} 7 &amp; 5 &amp; 6 \\\\ 11 &amp; 8 &amp; 10 \\\\ 2 &amp; 8 &amp; 8 \\\\ 9 &amp; 1 &amp; 10 \\end{pmatrix} \\] \\[ F = A B&#39; = \\begin{pmatrix} 147 &amp; 130 &amp; 182 \\\\ 128 &amp; 78 &amp; 136 \\\\ 49 &amp; 102 &amp; 114 \\end{pmatrix} \\] TransposiciÃ³n de matrices Ya definida en la secciÃ³n anterior. Propiedades clave: \\((A&#39;)&#39; = A\\) \\((A + B)&#39; = A&#39; + B&#39;\\) \\((AB)&#39; = B&#39;A&#39;\\) Traza de una matriz La traza de una matriz cuadrada es la suma de los elementos de su diagonal principal: \\[ \\text{tr}(A) = \\sum_{i=1}^{n} a_{ii} \\] Propiedades: \\(\\text{tr}(A) = \\text{tr}(A&#39;)\\) \\(\\text{tr}(A + B) = \\text{tr}(A) + \\text{tr}(B)\\) \\(\\text{tr}(AB) = \\text{tr}(BA)\\) Ejemplo: \\[ \\text{tr}(F) = 147 + 78 + 114 = 339 \\] Determinantes Para matrices cuadradas \\(A \\in \\mathbb{R}^{n \\times n}\\), el determinante se denota \\(|A|\\). Para \\(2 \\times 2\\): \\[ |A| = \\begin{vmatrix} a &amp; b \\\\ c &amp; d \\end{vmatrix} = ad - bc \\] Para \\(3 \\times 3\\): \\[ |A| = a_{11}a_{22}a_{33} + a_{12}a_{23}a_{31} + a_{13}a_{21}a_{32} - a_{13}a_{22}a_{31} - a_{12}a_{21}a_{33} - a_{11}a_{23}a_{32} \\] Ejemplo: \\[ G = \\begin{pmatrix} 1 &amp; 1 &amp; 3 \\\\ 1 &amp; 1 &amp; 0 \\\\ 3 &amp; 1 &amp; 2 \\end{pmatrix} \\Rightarrow |G| = -6 \\] Matriz inversa Una matriz cuadrada \\(A\\) es invertible si existe \\(A^{-1}\\) tal que: \\[ A A^{-1} = A^{-1} A = I \\] Se calcula como: \\[ A^{-1} = \\frac{1}{|A|} \\cdot \\text{adj}(A) \\] donde \\(\\text{adj}(A)\\) es la matriz adjunta (traspuesta de los cofactores). Propiedades: \\((A^{-1})^{-1} = A\\) \\((AB)^{-1} = B^{-1} A^{-1}\\) \\((A&#39;)^{-1} = (A^{-1})&#39;\\) Ejemplo: \\[ G^{-1} = \\frac{1}{-6} \\begin{pmatrix} 2 &amp; 1 &amp; -3 \\\\ -2 &amp; -7 &amp; 3 \\\\ -2 &amp; 2 &amp; 0 \\end{pmatrix} \\] Rango de una matriz El rango de una matriz es el nÃºmero mÃ¡ximo de filas (o columnas) linealmente independientes. Definiciones: Vectores son linealmente dependientes si \\(c_1a_1 + \\cdots + c_n a_n = 0\\) con \\(c_i \\neq 0\\) Son independientes si la Ãºnica combinaciÃ³n que da cero es con todos los \\(c_i = 0\\) Propiedades: \\(\\text{rang}(AB) \\leq \\min\\{\\text{rang}(A), \\text{rang}(B)\\}\\) Si \\(A\\) es invertible: \\(\\text{rang}(AB) = \\text{rang}(B)\\) \\(\\text{rang}(A) = \\text{rang}(A A&#39;) = \\text{rang}(A&#39; A)\\) Sistemas de ecuaciones lineales Un sistema de ecuaciones lineales con \\(m\\) ecuaciones y \\(n\\) incÃ³gnitas se puede escribir de la forma: \\[ \\begin{aligned} a_{11}x_1 + a_{12}x_2 + \\cdots + a_{1n}x_n &amp;= b_1 \\\\ a_{21}x_1 + a_{22}x_2 + \\cdots + a_{2n}x_n &amp;= b_2 \\\\ &amp;\\vdots \\\\ a_{m1}x_1 + a_{m2}x_2 + \\cdots + a_{mn}x_n &amp;= b_m \\end{aligned} \\] En forma matricial, este sistema se escribe como: \\[ A \\mathbf{x} = \\mathbf{b} \\] donde \\[ A = \\begin{pmatrix} a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1n} \\\\ a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{m1} &amp; a_{m2} &amp; \\cdots &amp; a_{mn} \\end{pmatrix}, \\quad \\mathbf{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{pmatrix}, \\quad \\mathbf{b} = \\begin{pmatrix} b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_m \\end{pmatrix} \\] Sistema de Cramer DefiniciÃ³n. Un sistema de ecuaciones lineales se denomina sistema de Cramer si: La matriz \\(A\\) es cuadrada (\\(m = n\\)) La matriz \\(A\\) es no singular, es decir, \\(|A| \\neq 0\\) En este caso, el sistema tiene una Ãºnica soluciÃ³n dada por: \\[ \\mathbf{x} = A^{-1} \\mathbf{b} \\] Ejemplo numÃ©rico Considere el sistema: \\[ \\begin{aligned} 12x_1 + 20x_2 &amp;= 388 \\\\ 4x_1 + 17x_2 &amp;= 212 \\end{aligned} \\] En forma matricial: \\[ \\begin{pmatrix} 12 &amp; 20 \\\\ 4 &amp; 17 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} 388 \\\\ 212 \\end{pmatrix} \\] Paso 1: Calcular la inversa de \\(A\\) Primero calculamos el determinante: \\[ |A| = 12 \\cdot 17 - 20 \\cdot 4 = 204 - 80 = 124 \\] Luego, la matriz de cofactores traspuesta (adjunta): \\[ \\text{adj}(A) = \\begin{pmatrix} 17 &amp; -20 \\\\ -4 &amp; 12 \\end{pmatrix} \\] Entonces, \\[ A^{-1} = \\frac{1}{124} \\begin{pmatrix} 17 &amp; -20 \\\\ -4 &amp; 12 \\end{pmatrix} \\] Paso 2: Multiplicar \\(A^{-1} \\mathbf{b}\\) \\[ \\mathbf{x} = A^{-1} \\mathbf{b} = \\frac{1}{124} \\begin{pmatrix} 17 &amp; -20 \\\\ -4 &amp; 12 \\end{pmatrix} \\begin{pmatrix} 388 \\\\ 212 \\end{pmatrix} \\] Calculamos el producto: \\[ \\begin{aligned} x_1 &amp;= \\frac{1}{124}(17 \\cdot 388 - 20 \\cdot 212) = \\frac{1}{124}(6596 - 4240) = \\frac{2356}{124} = 19 \\\\ x_2 &amp;= \\frac{1}{124}(-4 \\cdot 388 + 12 \\cdot 212) = \\frac{1}{124}(-1552 + 2544) = \\frac{992}{124} = 8 \\end{aligned} \\] SoluciÃ³n final: \\[ \\boxed{ x_1 = 19, \\quad x_2 = 8 } \\] Este procedimiento es vÃ¡lido siempre que la matriz \\(A\\) sea cuadrada y su determinante no sea cero. Si \\(|A| = 0\\), el sistema no tiene soluciÃ³n Ãºnica: puede ser incompatible o tener infinitas soluciones. Matrices cuadradas especiales Las siguientes matrices cuadradas tienen propiedades estructurales claves que facilitan el desarrollo de mÃ©todos economÃ©tricos. 1. Matriz diagonal Una matriz diagonal \\(A = [a_{ij}] \\in \\mathbb{R}^{m \\times m}\\) tiene ceros fuera de la diagonal principal: \\[ A = \\begin{pmatrix} a_{11} &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; a_{22} &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; a_{mm} \\end{pmatrix} = \\text{diag}(a_{11}, a_{22}, \\ldots, a_{mm}) \\] 2. Matriz identidad La matriz identidad \\(I_m\\) es una matriz diagonal con unos en la diagonal: \\[ I_m = \\begin{pmatrix} 1 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; 1 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; 1 \\end{pmatrix} \\] Propiedades: \\(AI_m = I_mA = A\\), \\(I_m^{-1} = I_m\\) 3. Matriz escalar Una matriz escalar es una matriz diagonal cuyos elementos en la diagonal son iguales a un mismo nÃºmero \\(\\lambda\\): \\[ A = \\lambda I_m \\] 4. Matriz triangular inferior Una matriz triangular inferior cumple: \\[ a_{ij} = 0 \\quad \\text{para todo } i &lt; j \\] \\[ A = \\begin{pmatrix} a_{11} &amp; 0 &amp; \\cdots &amp; 0 \\\\ a_{21} &amp; a_{22} &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{m1} &amp; a_{m2} &amp; \\cdots &amp; a_{mm} \\end{pmatrix} \\] 5. Matriz nula La matriz nula tiene todos sus elementos iguales a cero: \\[ 0 = \\begin{pmatrix} 0 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; 0 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; 0 \\end{pmatrix} \\] 6. Matriz simÃ©trica Una matriz \\(A \\in \\mathbb{R}^{m \\times m}\\) es simÃ©trica si: \\[ A = A&#39; \\quad \\text{es decir, } a_{ij} = a_{ji} \\] 7. Matriz idempotente Una matriz \\(A\\) es idempotente si: \\[ A^2 = A \\] Ejemplo clave: la matriz de proyecciÃ³n sobre el espacio generado por las columnas de \\(X\\): \\[ P = X(X&#39;X)^{-1}X&#39; \\] Cumple: \\(P = P&#39;\\) (simÃ©trica) \\(P^2 = P\\) (idempotente) 8. Matriz ortogonal Una matriz \\(Q\\) es ortogonal si: \\[ Q&#39;Q = QQ&#39; = I \\Rightarrow Q^{-1} = Q&#39; \\] Sus columnas (y filas) son vectores ortonormales. 9. Matrices de proyecciÃ³n: \\(P\\) y \\(M\\) En regresiÃ³n lineal, dos matrices juegan un rol fundamental: a) Matriz de proyecciÃ³n sobre el espacio columna de \\(X\\): \\[ P = X(X&#39;X)^{-1}X&#39; \\] Idempotente: \\(P^2 = P\\) SimÃ©trica: \\(P&#39; = P\\) Proyecta cualquier vector \\(y\\) sobre el espacio generado por las columnas de \\(X\\): \\(\\hat{y} = P y\\) b) Matriz de aniquilaciÃ³n o proyecciÃ³n ortogonal: \\[ M = I - P \\] Idempotente: \\(M^2 = M\\) SimÃ©trica: \\(M&#39; = M\\) Proyecta sobre el complemento ortogonal del espacio generado por \\(X\\): \\(e = M y\\) (residuos) Estas matrices son centrales para expresar la descomposiciÃ³n: \\[ y = \\hat{y} + e = P y + M y \\] donde \\(\\hat{y}\\) es la parte explicada por \\(X\\), y \\(e\\) es la parte no explicada (residuos). Derivadas de una funciÃ³n multidimensional Derivadas de una forma lineal Sea la forma lineal \\(\\mathbf{a}&#39;\\mathbf{x} = a_1x_1 + a_2x_2 + \\cdots + a_nx_n\\), una funciÃ³n escalar de \\(n\\) variables independientes \\(x_1, \\ldots, x_n\\). La derivada parcial con respecto a una variable \\(x_i\\) es simplemente: \\[ \\frac{\\partial \\mathbf{a}&#39;\\mathbf{x}}{\\partial x_i} = a_i \\] La derivada de \\(\\mathbf{a}&#39;\\mathbf{x}\\) con respecto al vector \\(\\mathbf{x}\\) es: \\[ \\frac{\\partial \\mathbf{a}&#39;\\mathbf{x}}{\\partial \\mathbf{x}} = \\begin{pmatrix} \\frac{\\partial \\mathbf{a}&#39;\\mathbf{x}}{\\partial x_1} \\\\ \\frac{\\partial \\mathbf{a}&#39;\\mathbf{x}}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial \\mathbf{a}&#39;\\mathbf{x}}{\\partial x_n} \\end{pmatrix} = \\begin{pmatrix} a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_n \\end{pmatrix} = \\mathbf{a} \\] De forma anÃ¡loga, la derivada de \\(\\mathbf{a}&#39;\\mathbf{x}\\) respecto de \\(\\mathbf{x}&#39;\\) es un vector fila: \\[ \\frac{\\partial \\mathbf{a}&#39;\\mathbf{x}}{\\partial \\mathbf{x}&#39;} = \\begin{pmatrix} a_1 &amp; a_2 &amp; \\cdots &amp; a_n \\end{pmatrix} = \\mathbf{a}&#39; \\] Derivadas de una forma cuadrÃ¡tica Sea la forma cuadrÃ¡tica \\(\\mathbf{x}&#39;A\\mathbf{x}\\), donde \\(A\\) es una matriz simÃ©trica. Esta puede escribirse como: \\[ \\mathbf{x}&#39;A\\mathbf{x} = \\sum_{i=1}^{n} a_{ii}x_i^2 + 2\\sum_{i=1}^{n-1}\\sum_{j=i+1}^{n} a_{ij}x_ix_j \\] La derivada de \\(\\mathbf{x}&#39;A\\mathbf{x}\\) con respecto al vector \\(\\mathbf{x}\\) es: \\[ \\frac{\\partial (\\mathbf{x}&#39;A\\mathbf{x})}{\\partial \\mathbf{x}} = 2A\\mathbf{x} \\] Esto es, un vector columna cuya i-Ã©sima componente es: \\[ \\frac{\\partial (\\mathbf{x}&#39;A\\mathbf{x})}{\\partial x_i} = 2(a_{i1}x_1 + a_{i2}x_2 + \\cdots + a_{in}x_n) \\] Derivadas de segundo orden (matriz Hessiana) La derivada segunda de \\(\\mathbf{x}&#39;A\\mathbf{x}\\) con respecto a \\(x_i\\) es: \\[ \\frac{\\partial^2 (\\mathbf{x}&#39;A\\mathbf{x})}{\\partial x_i^2} = 2a_{ii} \\] La derivada mixta con respecto a \\(x_i\\) y \\(x_j\\) es: \\[ \\frac{\\partial^2 (\\mathbf{x}&#39;A\\mathbf{x})}{\\partial x_i \\partial x_j} = 2a_{ij} \\] La matriz de segundas derivadas (Hessiana) es: \\[ \\frac{\\partial^2 (\\mathbf{x}&#39;A\\mathbf{x})}{\\partial \\mathbf{x} \\partial \\mathbf{x}&#39;} = 2A \\] Resumen Derivada de forma lineal: \\(\\frac{\\partial (\\mathbf{a}&#39;\\mathbf{x})}{\\partial \\mathbf{x}} = \\mathbf{a}\\) Derivada de forma cuadrÃ¡tica: \\(\\frac{\\partial (\\mathbf{x}&#39;A\\mathbf{x})}{\\partial \\mathbf{x}} = 2A\\mathbf{x}\\) Matriz Hessiana: \\(\\frac{\\partial^2 (\\mathbf{x}&#39;A\\mathbf{x})}{\\partial \\mathbf{x} \\partial \\mathbf{x}&#39;} = 2A\\) ğŸ“˜ Preguntas de repaso Sea \\(\\mathbf{i} = (1, 1, \\ldots, 1)&#39;\\) un vector \\(m \\times 1\\) de unos. Calcule \\(\\mathbf{i}&#39;\\mathbf{i}\\). Sean \\(\\mathbf{i} = (1, \\ldots, 1)&#39;\\) y \\(\\mathbf{y} = (y_1, \\ldots, y_m)&#39;\\). Calcule \\(\\mathbf{i}&#39;\\mathbf{y}\\). Demuestre que la media de las observaciones \\(y_1, \\ldots, y_m\\) puede expresarse como: "],["supuestos-del-modelo-clÃ¡sico-de-regresiÃ³n-lineal.html", "5 Supuestos del Modelo ClÃ¡sico de RegresiÃ³n Lineal Proceso Generador de Datos Tabla Resumen de Supuestos S1. Linealidad en los ParÃ¡metros S2. Exogeneidad Estricta S3. Colinealidad Imperfecta S4. Perturbaciones EsfÃ©ricas S5. Regresores No EstocÃ¡sticos S6. Normalidad del Error Glosario de SÃ­mbolos ğŸ“˜ Preguntas de repaso", " 5 Supuestos del Modelo ClÃ¡sico de RegresiÃ³n Lineal Proceso Generador de Datos El modelo de regresiÃ³n lineal parte de la siguiente estructura: \\[ Y_i = X_i \\beta + \\epsilon_i \\] Donde: - \\(Y_i\\): variable dependiente (observaciÃ³n i) - \\(X_i\\): vector fila con los regresores de la observaciÃ³n i - \\(\\beta\\): vector de parÃ¡metros poblacionales - \\(\\epsilon_i\\): error poblacional (componentes no observables) - \\(i = 1, 2, ..., n\\) Esta formulaciÃ³n describe el proceso generador de datos (PGD), base para los supuestos del MCO. Tabla Resumen de Supuestos Supuesto NotaciÃ³n ImplicaciÃ³n principal S1. Linealidad en los parÃ¡metros \\(y_i = X_i \\beta + \\epsilon_i\\) El modelo es lineal en los parÃ¡metros S2. Exogeneidad estricta \\(\\mathbb{E}[\\epsilon_i \\mid X] = 0\\) No hay correlaciÃ³n entre regresores y error S3. Colinealidad imperfecta \\(\\text{Rango}(X) = K\\) No hay multicolinealidad perfecta; modelo identificable S4. Perturbaciones esfÃ©ricas \\(\\text{Var}(\\epsilon_i \\mid X) = \\sigma^2\\), \\(\\text{Cov}(\\epsilon_i, \\epsilon_j \\mid X) = 0\\) Homocedasticidad y no autocorrelaciÃ³n S5. Regresores no estocÃ¡sticos \\(X\\) es fija en repetidas muestras Simplifica demostraciones teÃ³ricas S6. Normalidad \\(\\epsilon \\mid X \\sim \\mathcal{N}(0, \\sigma^2 I)\\) Solo necesaria para inferencia exacta S1. Linealidad en los ParÃ¡metros El valor esperado de \\(y\\) estÃ¡ relacionado linealmente con los regresores: \\[ \\mathbb{E}[Y_i \\mid X_i] = X_i \\beta \\] Esto permite distintas formas funcionales (lineales en parÃ¡metros): Lineal: \\(y_i = \\beta_1 + \\beta_2 x_i + \\epsilon_i\\) Log-log: \\(\\log(y_i) = \\beta_1 + \\beta_2 \\log(x_i) + \\epsilon_i\\) Log-lineal: \\(\\log(y_i) = \\beta_1 + \\beta_2 x_i + \\epsilon_i\\) Lineal-log: \\(y_i = \\beta_1 + \\beta_2 \\log(x_i) + \\epsilon_i\\) CuadrÃ¡tico: \\(y_i = \\beta_1 + \\beta_2 x_i + \\beta_3 x_i^2 + \\epsilon_i\\) Interactuado: \\(y_i = \\beta_1 + \\beta_2 x_1 + \\beta_3 x_2 + \\beta_4(x_1 x_2) + \\epsilon_i\\) S2. Exogeneidad Estricta \\[ \\mathbb{E}[\\epsilon_i \\mid X] = 0 \\] Esto implica que no existe relaciÃ³n sistemÃ¡tica entre los regresores y el tÃ©rmino de error. Ejemplos: \\(\\mathbb{E}[u \\mid X = 1] = 0\\) \\(\\mathbb{E}[u \\mid X_2 = \\text{Mujer}] = 0\\) DemostraciÃ³n (Ley de la esperanza iterada): \\[ \\mathbb{E}[\\epsilon_i] = \\mathbb{E}\\left[ \\mathbb{E}[\\epsilon_i \\mid X] \\right] = \\mathbb{E}[0] = 0 \\] Equivalencia: Si \\(\\mathbb{E}[\\epsilon_i \\mid X] = 0\\), entonces: \\[ \\text{Cov}(\\epsilon_i, X_j) = 0 \\quad \\forall j \\] Pero quÃ© quiere decir? Una forma de pensar en esta definiciÃ³n es: Para cualquier valor de \\(X\\), el valor esperado de los residuos debe ser igual a cero E.g., \\(\\mathop{E}\\left[ u \\mid X=1 \\right]=0\\) and \\(\\mathop{E}\\left[ u \\mid X=100 \\right]=0\\) E.g., \\(\\mathop{E}\\left[ u \\mid X_2=\\text{Mujer} \\right]=0\\) and \\(\\mathop{E}\\left[ u \\mid X_2=\\text{Hombre} \\right]=0\\) Note: \\(\\mathop{E}\\left[ u \\mid X \\right]=0\\) es mÃ¡s restrictivo que \\(\\mathop{E}\\left[ u \\right]=0\\) Graficamenteâ€¦ Exogeneidad Estricta se Incumple, i.e., \\(\\mathop{E}\\left[ \\epsilon \\mid X \\right] \\neq 0\\) S3. Colinealidad Imperfecta \\[ \\text{Rango}(X) = K \\] Para que el modelo estÃ© identificado, debe cumplirse que el nÃºmero de observaciones sea mayor que el nÃºmero de regresores: \\(n &gt; K\\). Violaciones comunes: Regresor constante: \\(X_j = c\\) Dos variables idÃ©nticas: \\(X_j = X_k\\) CombinaciÃ³n lineal exacta: \\(X_3 = X_1 + X_2\\) Trampa de las variables binarias Ejemplo de matriz con rango 3: \\[ A = \\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 3 &amp; 5 &amp; 7 \\\\ 4 &amp; 6 &amp; 5 \\\\ \\end{bmatrix} \\quad \\Rightarrow \\text{Rango}(A) = 3 \\] Ejemplo de matriz con rango menor a 3: \\[ B = \\begin{bmatrix} 1 &amp; 3 &amp; 1 \\\\ 3 &amp; 8 &amp; 2 \\\\ 2 &amp; 9 &amp; 5 \\\\ \\end{bmatrix} \\quad \\Rightarrow \\text{Rango}(B) \\neq 3 \\] La tercera columna de \\(B\\) es combinaciÃ³n lineal de las otras dos: \\(C_3 = -2 \\cdot C_1 + C_2\\) Wooldridge (2003) aclara que este supuesto permite que los regresores estÃ©n correlacionados, siempre que no haya una relaciÃ³n lineal exacta entre ellos. S4. Perturbaciones EsfÃ©ricas Este supuesto se compone de dos condiciones: ğŸ”¹ Homocedasticidad \\[ \\text{Var}(\\epsilon_i \\mid X) = \\sigma^2 \\quad \\forall i \\] La dispersiÃ³n del tÃ©rmino de error es constante para todos los individuos. Esto significa que la varianza de los errores no depende de los regresores. ğŸ”¹ No autocorrelaciÃ³n \\[ \\text{Cov}(\\epsilon_i, \\epsilon_j \\mid X) = 0 \\quad \\text{para } i \\neq j \\] Los errores no estÃ¡n correlacionados entre sÃ­. Es especialmente relevante en series de tiempo, pero tambiÃ©n puede violarse en datos de corte transversal (e.g., por correlaciÃ³n espacial). ğŸ”¸ ImplicaciÃ³n conjunta Cuando se cumplen homocedasticidad y no autocorrelaciÃ³n: \\[ \\text{Var}(\\epsilon \\mid X) = \\sigma^2 I \\] La matriz de varianzas-covarianzas de los errores es escalar y diagonal, tambiÃ©n llamada matriz esfÃ©rica. ğŸ§  DerivaciÃ³n paso a paso {-} \\[ \\text{Var}(\\epsilon \\mid X) = \\mathbb{E}[\\epsilon \\epsilon&#39; \\mid X] - \\mathbb{E}[\\epsilon \\mid X] \\cdot \\mathbb{E}[\\epsilon&#39; \\mid X] \\] Por el supuesto de exogeneidad estricta (S2), sabemos que: \\[ \\mathbb{E}[\\epsilon \\mid X] = 0 \\quad \\Rightarrow \\quad \\text{Var}(\\epsilon \\mid X) = \\mathbb{E}[\\epsilon \\epsilon&#39; \\mid X] \\] Entonces, la matriz resultante es: \\[ \\text{Var}(\\epsilon \\mid X) = \\begin{bmatrix} \\mathbb{E}[\\epsilon_1^2 \\mid X] &amp; \\mathbb{E}[\\epsilon_1 \\epsilon_2 \\mid X] &amp; \\cdots &amp; \\mathbb{E}[\\epsilon_1 \\epsilon_n \\mid X] \\\\ \\mathbb{E}[\\epsilon_2 \\epsilon_1 \\mid X] &amp; \\mathbb{E}[\\epsilon_2^2 \\mid X] &amp; \\cdots &amp; \\mathbb{E}[\\epsilon_2 \\epsilon_n \\mid X] \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\mathbb{E}[\\epsilon_n \\epsilon_1 \\mid X] &amp; \\mathbb{E}[\\epsilon_n \\epsilon_2 \\mid X] &amp; \\cdots &amp; \\mathbb{E}[\\epsilon_n^2 \\mid X] \\end{bmatrix} \\] Aplicando los supuestos: \\(\\text{Var}(\\epsilon_i \\mid X) = \\sigma^2\\) \\(\\text{Cov}(\\epsilon_i, \\epsilon_j \\mid X) = 0\\) para \\(i \\neq j\\) \\[ \\Rightarrow \\text{Var}(\\epsilon \\mid X) = \\begin{bmatrix} \\sigma^2 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\sigma^2 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; \\sigma^2 \\end{bmatrix} = \\sigma^2 I \\] Este supuesto es necesario para garantizar la eficiencia del estimador MCO bajo los supuestos clÃ¡sicos (Teorema de Gauss-Markov). S5. Regresores No EstocÃ¡sticos Este supuesto establece que la matriz de regresores \\(X\\) no es aleatoria: sus valores permanecen fijos en repeticiones del experimento o entre muestras. \\[ X = \\text{constante} \\quad \\text{(no varÃ­a entre muestras)} \\] ğŸ”¹ Â¿QuÃ© significa? Aunque en la prÃ¡ctica \\(X\\) proviene de una muestra aleatoria, asumir que es no estocÃ¡stica permite tratarlo como fijo en la teorÃ­a. Esto implica que cualquier inferencia o estimaciÃ³n se condiciona sobre \\(X\\). âœ… Ventajas teÃ³ricas Simplifica la demostraciÃ³n de propiedades como insesgamiento y varianza mÃ­nima. Permite eliminar la distinciÃ³n entre: valor esperado condicional: \\(\\mathbb{E}[\\hat{\\beta} \\mid X]\\) y valor esperado incondicional: \\(\\mathbb{E}[\\hat{\\beta}]\\) âš ï¸ En la prÃ¡cticaâ€¦ Este supuesto rara vez se cumple literalmente, ya que \\(X\\) normalmente proviene de una muestra aleatoria. Sin embargo, es comÃºn en teorÃ­a clÃ¡sica porque: No afecta la validez del MCO si se asume que \\(X\\) es independiente de \\(\\epsilon\\). Se puede relajar en contextos de modelos mÃ¡s generales (paneles, variables instrumentales, etc.). En modelos con regresores estocÃ¡sticos, se requiere en cambio que \\(\\mathbb{E}[\\epsilon \\mid X] = 0\\), lo que recupera el supuesto de exogeneidad estricta (S2). S6. Normalidad del Error \\[ \\epsilon \\mid X \\sim \\mathcal{N}(0, \\sigma^2 I) \\] Este supuesto establece que los errores, condicionales a los regresores, siguen una distribuciÃ³n normal multivariada con media cero y matriz de varianza-covarianza esfÃ©rica \\(\\sigma^2 I\\). ğŸ¯ Â¿Para quÃ© sirve? Este supuesto no es necesario para que el estimador de MÃ­nimos Cuadrados Ordinarios (MCO) sea: Insesgado (S2 ya garantiza eso), Eficiente entre estimadores lineales insesgados (por el Teorema de Gauss-Markov). Sin embargo, sÃ­ es crucial para que se cumpla la distribuciÃ³n exacta de ciertos estadÃ­sticos en muestras pequeÃ±as. âœ… Aplicaciones de la normalidad: Validez de las pruebas t para significancia individual. Validez de las pruebas F para restricciones conjuntas. ConstrucciÃ³n exacta de intervalos de confianza para \\(\\beta\\). ğŸ§  Â¿QuÃ© pasa en muestras grandes? Gracias al Teorema Central del LÃ­mite y **La Ley de los Grandes NÃºmeros*, incluso si \\(\\epsilon\\) no es normal, el estimador \\(\\hat{\\beta}\\) tenderÃ¡ a seguir una distribuciÃ³n normal asintÃ³tica: \\[ \\hat{\\beta} \\overset{approx}{\\sim} \\mathcal{N}\\left(\\beta, \\sigma^2 (X&#39;X)^{-1}\\right) \\] Por eso, la normalidad puede relajarse si \\(n\\) es suficientemente grande. Glosario de SÃ­mbolos SÃ­mbolo Significado \\(Y_i\\) Variable dependiente \\(X_{ij}\\) Regresor j para observaciÃ³n i \\(\\beta_j\\) ParÃ¡metro poblacional \\(\\epsilon_i\\) Error poblacional \\(n\\) NÃºmero de observaciones \\(k\\) NÃºmero de regresores (sin constante) ğŸ“˜ Preguntas de repaso ğŸ“˜ 1. Conceptuales Defina brevemente los siguientes tÃ©rminos: EconometrÃ­a teÃ³rica EconometrÃ­a aplicada Â¿QuÃ© papel juega cada uno de los seis supuestos del modelo clÃ¡sico de regresiÃ³n lineal en garantizar las propiedades del estimador de MCO? ğŸ§® 2. ClasificaciÃ³n de modelos Clasifique los siguientes modelos como lineales en parÃ¡metros o no lineales: \\(y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\\) \\(\\log(y_i) = \\beta_0 + \\beta_1 \\log(x_i) + \\epsilon_i\\) \\(y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\epsilon_i\\) \\(y_i = \\frac{\\beta_0}{1 + e^{-\\beta_1 x_i}} + \\epsilon_i\\) \\(y_i = \\alpha + \\theta^{x_i} + \\epsilon_i\\) ğŸ“ 3. InterpretaciÃ³n de la pendiente Interprete el coeficiente \\(\\beta_1\\) en los siguientes modelos de regresiÃ³n lineal simple: \\(y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\\) \\(\\log(y_i) = \\beta_0 + \\beta_1 x_i + \\epsilon_i\\) \\(y_i = \\beta_0 + \\beta_1 \\log(x_i) + \\epsilon_i\\) \\(\\log(y_i) = \\beta_0 + \\beta_1 \\log(x_i) + \\epsilon_i\\) En cada caso, explique quÃ© representa un aumento marginal en \\(x_i\\), y si los efectos son absolutos, porcentuales o elÃ¡sticos. ğŸ¥ Recursos audiovisuales Â¿QuÃ© hacen los economistas? (Video 1) An Uneven Paying Field (Video 2) "]]
