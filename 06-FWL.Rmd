# Anatom√≠a de la Regresi√≥n M√∫ltiple

## Matrices de Proyecci√≥n {-}

Supongamos que estamos estimando una regresi√≥n lineal m√∫ltiple del tipo:

\[
y = X\beta + \varepsilon
\]

donde:

- \( y \in \mathbb{R}^n \) es el vector de la variable dependiente
- \( X \in \mathbb{R}^{n \times k} \) es la matriz de regresores de rango completo
- \( \beta \in \mathbb{R}^k \) es el vector de par√°metros

La estimaci√≥n de m√≠nimos cuadrados se basa en proyectar el vector \( y \) sobre el **espacio columna de \( X \)**, denotado \( \mathcal{C}(X) \). Esta proyecci√≥n busca encontrar el punto m√°s cercano en ese subespacio a \( y \), minimizando la suma de los residuos al cuadrado.

Esta proyecci√≥n se logra mediante la **matriz de proyecci√≥n**:

\[
\mathbf{P}_X = X(X'X)^{-1}X'
\]

Y su complemento ortogonal (que proyecta sobre el espacio ortogonal a \( \mathcal{C}(X) \)) es:

\[
\mathbf{M}_X = I - \mathbf{P}_X
\]

\[
\hat{y} = \mathbf{P}_X y \quad \text{y} \quad \hat{\varepsilon} = \mathbf{M}_X y
\]

### üîç Intuici√≥n geom√©trica {-}

- \( y \): vector observado
- \( \hat{y} = \mathbf{P}_X y \): predicci√≥n, o "sombra" de \( y \) sobre el plano generado por \( X \)
- \( \hat{\varepsilon} = y - \hat{y} \): residuo, perpendicular al plano

El estimador de MCO se obtiene al minimizar:

\[
\min_{\beta} (y - X\beta)'(y - X\beta)
\]

La soluci√≥n es:

\[
\hat{\beta} = (X'X)^{-1}X'y
\]


### üìê Propiedades algebraicas clave {-}

| Propiedad       | \( \mathbf{P}_X \)                   | \( \mathbf{M}_X \)                   |
|----------------|--------------------------------------|--------------------------------------|
| Simetr√≠a       | \( \mathbf{P}_X' = \mathbf{P}_X \)   | \( \mathbf{M}_X' = \mathbf{M}_X \)   |
| Idempotencia   | \( \mathbf{P}_X^2 = \mathbf{P}_X \)  | \( \mathbf{M}_X^2 = \mathbf{M}_X \)  |
| Ortogonalidad  | \( \mathbf{P}_X \mathbf{M}_X = 0 \)  | \( \hat{y} \perp \hat{\varepsilon} \) |

Estas propiedades nos permitir√°n probar resultados como: ortogonalidad entre predicci√≥n y residuos, descomposici√≥n de la varianza total, y derivar el estimador de varianza del error.


### ‚ú® Visualizaci√≥n tridimensional de la proyecci√≥n {-}

```{r proyeccion-bonita, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
library(rgl)
library(htmlwidgets)
library(knitr)

# Asegurar integraci√≥n con knitr
setupKnitr()

# Crear escena (sin open3d)
bg3d("white")
par3d(windowRect = c(100, 100, 900, 900), zoom = 0.8)

# Espacio base
x1 <- c(1, 0, 0)
x2 <- c(0, 1, 0)
X <- cbind(x1, x2)
grid_x <- seq(-2, 2, length.out = 10)
grid_y <- seq(-2, 2, length.out = 10)
z_plane <- rep(0, 100)
y <- c(1, 1, 2)
P <- X %*% solve(t(X) %*% X) %*% t(X)
Py <- P %*% y

# Plano
surface3d(grid_x, grid_y, matrix(z_plane, 10), color = "#A7D3D3", alpha = 0.4)

# Vectores
lines3d(c(0, y[1]), c(0, y[2]), c(0, y[3]), col = "orangered", lwd = 4)
text3d(y[1], y[2], y[3], texts = "Y", col = "orangered", cex = 1.2)

lines3d(c(0, Py[1]), c(0, Py[2]), c(0, Py[3]), col = "darkgreen", lwd = 4)
text3d(Py[2], Py[2.5], Py[3], texts = "PxY", col = "darkgreen", cex = 1.2)

lines3d(c(Py[1], y[1]), c(Py[2], y[2]), c(Py[3], y[3]), col = "gray40", lwd = 3)
text3d((y[1]+Py[1])/2, (y[2]+Py[2])/2, (y[3]+Py[3])/2, texts = "Residuo", col = "gray40", cex = 1.2)

axes3d()
title3d("Proyecci√≥n de y sobre el espacio generado por X")

# Mostrar el gr√°fico interactivamente en el documento
rglwidget()

```


La figura muestra c√≥mo el vector \( y \) se proyecta sobre el espacio generado por las columnas de \( X \). La diferencia \( y - P_X y \) es ortogonal al plano y corresponde a los residuos.

## Teorema de Frisch-Waugh-Lovell (FWL) {-}

El **Teorema de Frisch-Waugh-Lovell (FWL)** es uno de los resultados m√°s importantes de la econometr√≠a, porque **nos permite entender qu√© significa realmente el coeficiente de una variable en una regresi√≥n m√∫ltiple**.

FWL nos dice que, si estamos interesados en el efecto de una variable \( X_r \) sobre \( y \), **controlando por otras variables** \( X_s \), **podemos obtener exactamente el mismo coeficiente** de una manera alternativa, usando **proyecciones**.

Esto es extremadamente √∫til porque:

- Da una interpretaci√≥n clara del coeficiente como un efecto "depurado".
- Permite construir regresiones parciales paso a paso.
- Ayuda a entender la mec√°nica interna de los modelos de regresi√≥n m√∫ltiples.

## üéØ Notaci√≥n y Motivaci√≥n {-}

Considera el siguiente modelo de regresi√≥n lineal:

\[
y = X_r \beta_r + X_s \beta_s + \varepsilon
\]

donde:

- \( y \in \mathbb{R}^{n \times 1} \) es la variable dependiente (por ejemplo, salario).
- \( X_r \in \mathbb{R}^{n \times 1} \) es la **variable de inter√©s** (por ejemplo, educaci√≥n).
- \( X_s \in \mathbb{R}^{n \times k} \) es el conjunto de **variables de control** (por ejemplo, experiencia, edad, g√©nero, etc.).
- \( \varepsilon \in \mathbb{R}^{n \times 1} \) es el error.

Esta forma tambi√©n puede escribirse m√°s compactamente as√≠:

\[
y = [X_r \quad X_s] \begin{bmatrix} \beta_r \\ \beta_s \end{bmatrix} + \varepsilon
\]

Donde la matriz \( X = [X_r \quad X_s] \in \mathbb{R}^{n \times (k+1)} \) contiene **todas** las variables explicativas, y el vector de par√°metros \( \beta \in \mathbb{R}^{k+1} \) contiene sus coeficientes respectivos.

## ‚ùì¬øCu√°l es el problema que resuelve el FWL? {-}

Imagina que quieres saber el **efecto de la educaci√≥n sobre el salario**, pero sabes que hay muchas otras variables (edad, experiencia, g√©nero, etc.) que tambi√©n afectan el salario. Entonces haces una regresi√≥n m√∫ltiple, y obtienes el coeficiente de educaci√≥n controlando por todo lo dem√°s.

El teorema FWL te dice: *"Ese coeficiente se puede obtener en tres pasos, sin necesidad de correr la regresi√≥n completa."* Y lo mejor: **el resultado ser√° exactamente igual**.

Eso es lo que veremos a continuaci√≥n en el paso a paso.


## ‚ú® Paso a paso del Teorema de Frisch-Waugh-Lovell (FWL) {-}

El Teorema de FWL nos dice que podemos obtener el coeficiente \( \beta_r \) de una regresi√≥n m√∫ltiple:

\[
y = X_r \beta_r + X_s \beta_s + \varepsilon
\]

realizando **tres regresiones parciales**, sin necesidad de incluir todos los regresores al mismo tiempo. A continuaci√≥n explicamos cada paso con todo el detalle necesario.


## üß© Paso 1: Proyectar \( y \) sobre \( X_s \) y obtener los residuos {-}

Primero, eliminamos de \( y \) la parte que puede ser explicada por los controles \( X_s \). Esto se hace **regresando \( y \) sobre \( X_s \)** y guardando los **residuos**. Es decir:

\[
\tilde{y} = M_s y = (I - P_s)y
\]

donde:

- \( P_s = X_s (X_s'X_s)^{-1} X_s' \) es la **matriz de proyecci√≥n** sobre el espacio generado por los controles \( X_s \).
- \( M_s = I - P_s \) es la **matriz de residuos** o complemento ortogonal: elimina todo lo que est√© explicado por \( X_s \).

üîç **¬øQu√© significa esto?**

- Estamos "limpiando" a \( y \), quit√°ndole la parte que se debe a los controles.
- El resultado \( \tilde{y} \) representa la **parte de \( y \) que es ortogonal a los controles**.

üìê **Interpretaci√≥n geom√©trica**: proyectamos \( y \) sobre el subespacio generado por \( X_s \) y nos quedamos con la componente perpendicular.


## üß© Paso 2: Proyectar \( X_r \) sobre \( X_s \) y obtener los residuos {-}

Ahora hacemos lo mismo con la variable de inter√©s \( X_r \): le quitamos la parte que puede explicarse con los controles.

\[
\tilde{X}_r = M_s X_r = (I - P_s)X_r
\]

üîç **¬øQu√© significa esto?**

- Estamos "depurando" a \( X_r \), eliminando cualquier relaci√≥n lineal con los controles.
- \( \tilde{X}_r \) es la **parte de \( X_r \) que no se puede predecir con \( X_s \)**.

üìê **Interpretaci√≥n geom√©trica**: proyectamos \( X_r \) sobre \( \mathcal{C}(X_s) \) y guardamos el residuo, que es ortogonal a ese subespacio.

## üß© Paso 3: Regresar \( \tilde{y} \) sobre \( \tilde{X}_r \) {-}

Finalmente, estimamos el coeficiente que relaciona las dos variables "limpias" o **depuradas**:

\[
\tilde{y} = \tilde{X}_r \cdot \gamma + u
\]

El estimador de MCO de esta regresi√≥n es:

\[
\hat{\gamma} = (\tilde{X}_r'\tilde{X}_r)^{-1} \tilde{X}_r' \tilde{y}
\]

üîî **¬°Sorpresa!** Este estimador es exactamente igual a:

\[
\hat{\beta}_r \quad \text{(el coeficiente de \( X_r \) en la regresi√≥n completa)}
\]

## ‚úÖ Interpretaci√≥n final{-}

Este resultado nos dice que:

- El efecto de \( X_r \) sobre \( y \), **controlando por \( X_s \)**, es igual al efecto de la **parte de \( X_r \) que no se relaciona con \( X_s \)** sobre la **parte de \( y \) que tampoco se relaciona con \( X_s \)**.
- En palabras simples: es una regresi√≥n entre los residuos.

## üì¶ Conclusi√≥n {-}

Este teorema tiene implicaciones profundas:

- Muestra que **controlar por variables** equivale a **quitarles su efecto tanto a la variable explicativa como a la dependiente**, y luego ver c√≥mo se relacionan esas partes "limpias".
- Es la base para entender t√©cnicas como control por regresi√≥n parcial, y tambi√©n para desarrollar intuiciones sobre variables instrumentales, efectos marginales y m√°s.


## Demostraci√≥n Formal {-}

Usando √°lgebra matricial:

\[
\hat{\beta}_r = (X_r'M_s X_r)^{-1} X_r'M_s y
\]

Esto se deduce de la forma general del estimador de MCO:

\[
\hat{\beta} = (X'X)^{-1}X'y
\]

pero aplicado al modelo reducido, en el que \( y \) y \( X_r \) han sido "limpiados" de \( X_s \).


## üß™ Ejemplo pr√°ctico del Teorema de Frisch-Waugh-Lovell en Stata, R y Python {-}

A continuaci√≥n presentamos una simulaci√≥n sencilla para ilustrar el Teorema de Frisch-Waugh-Lovell (FWL) y mostrar c√≥mo cambia el coeficiente estimado de una variable de inter√©s dependiendo de la correlaci√≥n con los controles. Tambi√©n implementamos paso a paso la construcci√≥n del estimador usando residuos.

---

### üîµ C√≥digo en Stata

```stata
clear
set seed 6981473
set obs 1000

* Variable de inter√©s Xr
gen altura = runiform()*30+150
replace altura= round(altura)
label var altura "altura"

* Crear Xs con correlaci√≥n positiva con Xr
gen ingreso_hh = rnormal() + 5*altura

* Variable dependiente Y
gen salario = 1 + 2*altura + 5*ingreso_hh + rnormal()

* CASO 1: cov(altura, ingreso_hh)>0
reg salario altura
reg salario altura ingreso_hh

* CASO 2: cov(altura, ingreso_hh)=0
gen ingreso_hh2 = rnormal()
gen salario2 = 1 + 2*altura + 5*ingreso_hh2 + rnormal()
reg salario2 altura
reg salario2 altura ingreso_hh2

* CASO 3: cov(salario, ingreso_hh)=0
gen ingreso_hh3 = rnormal() + 3*altura
gen salario3 = 1 + 2*altura + 0*ingreso_hh3 + rnormal()
reg salario3 altura
reg salario3 altura ingreso_hh3

* Teorema de FWL paso a paso
reg salario ingreso_hh
predict My, res

reg altura ingreso_hh
predict MXr, res

reg salario altura ingreso_hh
reg My MXr
* Comparar coeficientes
di _b[MXr] _b[altura]
````
### üü¢ C√≥digo en R

```r
library(tidyverse)

set.seed(6981473)
n <- 1000

# Variable de inter√©s
altura <- runif(n, 0, 30) + 150
altura <- round(altura)

# Control correlacionado con altura
ingreso_hh <- rnorm(n) + 5 * altura

# Variable dependiente
salario <- 1 + 2 * altura + 5 * ingreso_hh + rnorm(n)

df <- tibble(altura, ingreso_hh, salario)

# CASO 1: cov(altura, ingreso_hh)>0
summary(lm(salario ~ altura, data = df))
summary(lm(salario ~ altura + ingreso_hh, data = df))

# CASO 2: cov(altura, ingreso_hh)=0
df$ingreso_hh2 <- rnorm(n)
df$salario2 <- 1 + 2 * df$altura + 5 * df$ingreso_hh2 + rnorm(n)
summary(lm(salario2 ~ altura, data = df))
summary(lm(salario2 ~ altura + ingreso_hh2, data = df))

# CASO 3: cov(y, ingreso_hh)=0
df$ingreso_hh3 <- rnorm(n) + 3 * df$altura
df$salario3 <- 1 + 2 * df$altura + 0 * df$ingreso_hh3 + rnorm(n)
summary(lm(salario3 ~ altura, data = df))
summary(lm(salario3 ~ altura + ingreso_hh3, data = df))

# FWL paso a paso
modelo_y <- lm(salario ~ ingreso_hh, data = df)
df$My <- resid(modelo_y)

modelo_xr <- lm(altura ~ ingreso_hh, data = df)
df$MXr <- resid(modelo_xr)

summary(lm(My ~ MXr, data = df))  # Igual al coef. de altura

summary(lm(salario ~ altura + ingreso_hh, data = df))  # Verificaci√≥n
# Comparar coeficientes
cat("Coeficiente de altura (residuos):", coef(lm(My ~ MXr, data = df))["MXr"], "\n")
cat("Coeficiente de altura (modelo completo):", coef(lm(salario ~ altura + ingreso_hh, data = df))["altura"], "\n")
```

### üî¥ C√≥digo en Python
```python
import numpy as np
import pandas as pd
import statsmodels.api as sm
import statsmodels.formula.api as smf

np.random.seed(6981473)
n = 1000

# Variable de inter√©s
altura = np.round(np.random.uniform(0, 30, n) + 150)

# Control correlacionado
ingreso_hh = np.random.normal(0, 1, n) + 5 * altura

# Variable dependiente
salario = 1 + 2 * altura + 5 * ingreso_hh + np.random.normal(0, 1, n)

df = pd.DataFrame({
    'altura': altura,
    'ingreso_hh': ingreso_hh,
    'salario': salario
})

# CASO 1
print(sm.OLS.from_formula('salario ~ altura', data=df).fit().summary())
print(sm.OLS.from_formula('salario ~ altura + ingreso_hh', data=df).fit().summary())

# CASO 2
df['ingreso_hh2'] = np.random.normal(0, 1, n)
df['salario2'] = 1 + 2 * df['altura'] + 5 * df['ingreso_hh2'] + np.random.normal(0, 1, n)

print(sm.OLS.from_formula('salario2 ~ altura', data=df).fit().summary())
print(sm.OLS.from_formula('salario2 ~ altura + ingreso_hh2', data=df).fit().summary())

# CASO 3
df['ingreso_hh3'] = np.random.normal(0, 1, n) + 3 * df['altura']
df['salario3'] = 1 + 2 * df['altura'] + np.random.normal(0, 1, n)

print(sm.OLS.from_formula('salario3 ~ altura', data=df).fit().summary())
print(sm.OLS.from_formula('salario3 ~ altura + ingreso_hh3', data=df).fit().summary())

# FWL paso a paso
modelo_y = sm.OLS.from_formula('salario ~ ingreso_hh', data=df).fit()
df['My'] = modelo_y.resid

modelo_xr = sm.OLS.from_formula('altura ~ ingreso_hh', data=df).fit()
df['MXr'] = modelo_xr.resid

print(sm.OLS.from_formula('My ~ MXr', data=df).fit().summary())

# Verificaci√≥n
print(sm.OLS.from_formula('salario ~ altura + ingreso_hh', data=df).fit().summary())
# Comparar coeficientes
print("Coeficiente de altura (residuos):", sm.OLS.from_formula('My ~ MXr', data=df).fit().params['MXr'])
print("Coeficiente de altura (modelo completo):", sm.OLS.from_formula('salario ~ altura + ingreso_hh', data=df).fit().params['altura'])
```

<div class="box-repaso">

### üìò Preguntas de repaso {-}

1. ¬øQu√© significa que el coeficiente de una variable en una regresi√≥n m√∫ltiple sea "depurado"?

### üìå Preguntas sobre FWL y matrices de proyecci√≥n {-}

1. Sea \( y \in \mathbb{R}^{n \times 1} \), \( X \in \mathbb{R}^{n \times k} \), y \( D \in \mathbb{R}^{n \times 1} \) una variable binaria tal que \( D_i = 1 \) solo para una observaci√≥n \( i \), y \( D_j = 0 \) para \( j \neq i \).
   
   a. Use los pasos del teorema de Frisch-Waugh-Lovell para demostrar que el coeficiente estimado de \( D \) representa la diferencia entre la observaci√≥n \( i \) y la predicci√≥n para esa observaci√≥n basada en el resto de la muestra.  
   
   b. ¬øQu√© ocurre con la matriz de proyecci√≥n \( P_D \)? ¬øQu√© dimensi√≥n tiene y c√≥mo se interpreta cuando solo proyecta sobre una observaci√≥n?
   
   c. ¬øQu√© ocurre con la matriz de aniquilaci√≥n \( M_D = I - P_D \)? ¬øQu√© efecto tiene sobre el resto del vector \( y \)?
   
   d. Use esta informaci√≥n para demostrar que al incluir \( D \) en la regresi√≥n, se est√° excluyendo efectivamente la observaci√≥n \( i \) del resto del modelo. Es decir, la estimaci√≥n de los coeficientes asociados a \( X \) se hace como si se eliminara la observaci√≥n \( i \).

2. Repita el an√°lisis anterior, pero ahora asuma que la variable \( D \) es una constante. ¬øQu√© ocurre con las matrices de proyecci√≥n y aniquilaci√≥n? ¬øQu√© interpretaci√≥n tiene proyectar sobre una constante?

3. Suponga ahora que \( X \) es una dummy igual a 1 si el individuo es hombre, y que \( D \) es una dummy igual a 1 si el individuo es mujer. ¬øQu√© ocurre en este caso? ¬øC√≥mo se interpretan los coeficientes al incluir ambas dummies en la regresi√≥n?



### üìò Preguntas sobre modelos con variables binarias y constantes {-}

4. Se desea estudiar el n√∫mero de horas de lectura diaria \( Y \) como funci√≥n del nivel educativo. Se tiene una muestra de \( N \) individuos clasificados en tres grupos:  
   - Grupo 1: estudios superiores  
   - Grupo 2: estudios medios  
   - Grupo 3: estudios bajos  

   Se definen tres variables binarias \( D_1, D_2, D_3 \), donde:

   \[
   D_j = \begin{cases}
   1 & \text{si el individuo pertenece al grupo } j \\
   0 & \text{en caso contrario}
   \end{cases}
   \]

   Al estimar el siguiente modelo:

   \[
   Y = 10 D_1 + 5 D_2 + 2 D_3 + u
   \]

   a. Demuestre matem√°ticamente que las medias condicionales de horas de lectura son 10, 5 y 2 para cada grupo.

   b. ¬øQu√© sucede si se incluye una constante en este modelo? ¬øQu√© problema emp√≠rico surge? Proponga una soluci√≥n (por ejemplo, eliminar una de las dummies para evitar colinealidad perfecta).


### üßÆ Preguntas sobre regresiones simples con constantes y dummies {-}

5. Si se estima una regresi√≥n de \( Y \) contra una constante y \( D_1 \), ¬øcu√°l es el intercepto y cu√°l es el coeficiente de \( D_1 \)? Interpr√©telos.

6. Si se estima una regresi√≥n de \( Y \) contra una constante y \( D_2 \), ¬øqu√© coeficiente acompa√±a a \( D_2 \)? ¬øC√≥mo cambia la interpretaci√≥n con respecto al caso anterior?



### üìä Pregunta sobre FWL y √°lgebra matricial {-}

7. Considere el siguiente modelo lineal sin constante:

\[
Y = X \beta + u
\]

donde \( Y \in \mathbb{R}^{n \times 1} \), \( X \in \mathbb{R}^{n \times k} \), y \( u \in \mathbb{R}^{n \times 1} \). Suponga adem√°s que desea controlar por un conjunto adicional de variables \( Z \in \mathbb{R}^{n \times m} \).

   a. Defina \( \tilde{Y} = M_Z Y \) y \( \tilde{X} = M_Z X \), donde \( M_Z = I - P_Z \) y \( P_Z = Z(Z'Z)^{-1}Z' \).  
   Explique con detalle qu√© representan estas transformaciones. ¬øQu√© parte de \( Y \) y de \( X \) est√°n conservando? ¬øQu√© parte est√°n eliminando?

   b. Demuestre que el vector de coeficientes \( \hat{\beta} \) para \( X \), en la regresi√≥n de \( Y \) sobre \( X \) y \( Z \), puede obtenerse a partir de la siguiente expresi√≥n:

   \[
   \hat{\beta} = (\tilde{X}' \tilde{X})^{-1} \tilde{X}' \tilde{Y}
   \]

   c. ¬øBajo qu√© condiciones es v√°lida esta expresi√≥n? ¬øQu√© sucede si \( \tilde{X}' \tilde{X} \) no es invertible?

   d. Explique c√≥mo se interpreta este resultado en t√©rminos del Teorema de Frisch-Waugh-Lovell.

</div>
